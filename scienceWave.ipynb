{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "scienceWave.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duyqu-chan/Experiments-with-XGBoost/blob/master/scienceWave.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXoiVU_cybBQ",
        "colab_type": "text"
      },
      "source": [
        "# Science Wave Capital - Case Study\n",
        "31.08.2018\n",
        "\n",
        "by Duygu Can\n",
        "\n",
        "The aim of this study is to minimize multinominal cross-entropy, using 103 features given in trainData.csv with the help of state-of-art feature engineering, ML modelling and ensembling techniques.\n",
        "For this reason powerfull XGBoost algorithm is chosen.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgyvlaZwybBX",
        "colab_type": "code",
        "colab": {},
        "outputId": "a6fa6f88-f815-4faf-8beb-55a502b31d93"
      },
      "source": [
        "%matplotlib inline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import skew, mstats, kurtosis\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from pylab import *\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "import xgboost as xgb\n",
        "from xgboost.sklearn import XGBClassifier\n",
        "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
        "from sklearn.grid_search import GridSearchCV   #Perforing grid search\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
            "  DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8hbeG6xybBv",
        "colab_type": "code",
        "colab": {},
        "outputId": "aaddf45d-4020-40e5-b7b4-c012c3ab51ed"
      },
      "source": [
        "train = pd.read_csv(\"trainData.csv\")\n",
        "test = pd.read_csv(\"testData.csv\")\n",
        "y = train.target\n",
        "X = train.drop('target', axis=1)\n",
        "X.head()\n",
        "X_trans = BCtrans(X)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>v1</th>\n",
              "      <th>v10</th>\n",
              "      <th>v100</th>\n",
              "      <th>v101</th>\n",
              "      <th>v102</th>\n",
              "      <th>v103</th>\n",
              "      <th>v11</th>\n",
              "      <th>v12</th>\n",
              "      <th>v13</th>\n",
              "      <th>v14</th>\n",
              "      <th>...</th>\n",
              "      <th>v90</th>\n",
              "      <th>v91</th>\n",
              "      <th>v92</th>\n",
              "      <th>v93</th>\n",
              "      <th>v94</th>\n",
              "      <th>v95</th>\n",
              "      <th>v96</th>\n",
              "      <th>v97</th>\n",
              "      <th>v98</th>\n",
              "      <th>v99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.2</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.6</td>\n",
              "      <td>0.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>1.2</td>\n",
              "      <td>1.4</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.2</td>\n",
              "      <td>2.6</td>\n",
              "      <td>2.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.8</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.8</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 103 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    v1  v10  v100  v101  v102  v103  v11  v12  v13  v14 ...   v90  v91  v92  \\\n",
              "0  1.4  0.0   0.2   1.0   4.2   0.4  0.0  0.0  0.0  1.2 ...   0.2  0.6  0.2   \n",
              "1  0.0  0.0   0.0   2.8   0.0   0.8  0.0  0.2  1.2  1.4 ...   0.0  0.0  0.0   \n",
              "2  0.0  0.0   0.0   0.4   0.0   0.6  0.8  0.0  0.0  0.2 ...   0.0  0.0  0.0   \n",
              "3  0.0  0.0   0.0   0.0   0.2   0.8  0.4  0.0  0.0  0.0 ...   0.0  0.0  0.0   \n",
              "4  0.0  0.0   0.0   0.0   0.0   0.0  0.0  0.0  0.0  0.2 ...   0.0  0.0  0.2   \n",
              "\n",
              "   v93  v94  v95  v96  v97  v98  v99  \n",
              "0  0.0  3.2  1.0  0.2  0.0  1.6  0.4  \n",
              "1  1.2  0.0  1.2  0.2  0.2  2.6  2.2  \n",
              "2  0.0  0.0  0.8  0.2  0.8  1.4  0.0  \n",
              "3  0.0  0.4  0.4  0.0  0.4  0.4  0.0  \n",
              "4  0.0  0.0  0.0  0.0  0.0  0.4  0.0  \n",
              "\n",
              "[5 rows x 103 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjAd6uC_ybB8",
        "colab_type": "text"
      },
      "source": [
        "Transform categories to 0-8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJtuuIKuybCB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = y-1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejBD0DBMybCF",
        "colab_type": "text"
      },
      "source": [
        "Class Weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odjUEKM4ybCG",
        "colab_type": "code",
        "colab": {},
        "outputId": "58b0ac43-4a07-4e03-d067-73bbe3e3c89c"
      },
      "source": [
        "y.value_counts(normalize=\"TRUE\").sort_index()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.031174\n",
              "1    0.260545\n",
              "2    0.129351\n",
              "3    0.043489\n",
              "4    0.044265\n",
              "5    0.228433\n",
              "6    0.045881\n",
              "7    0.136785\n",
              "8    0.080077\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je0qFpj6ybCM",
        "colab_type": "text"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67peZktAybCT",
        "colab_type": "text"
      },
      "source": [
        "To normalize the distributions apply box-cox transformation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPexn83PybCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def BCtrans(X):\n",
        "    '''A function to columnwise box-cox transform and scale  a dataframe'''\n",
        "    Xt = pd.DataFrame(index=X.index, columns=X.columns)\n",
        "    eps = 1e-7 # a very small number to shift the range\n",
        "    scaler = MinMaxScaler(feature_range=(0+eps, 1)) #1st scaler (box-cox requires sitrictly positive range\n",
        "    scaler2 = MinMaxScaler(feature_range=(0, 1)) #2nd scaler\n",
        "    for col in X.columns: #loop over columns\n",
        "        scaled = scaler.fit_transform(X[col].values.reshape(-1, 1))\n",
        "        trans, _ = stats.boxcox(scaled) #apply box-cox transformation\n",
        "        Xt[col] = scaler2.fit_transform(trans.reshape(-1, 1))\n",
        "   \n",
        "    return Xt\n",
        "    #Xtrain_trans = pd.DataFrame(Xtrain_trans, columns=colNames)#convert array to dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cviVTnz2ybCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_trans = BCtrans(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue_mK3_MybCg",
        "colab_type": "text"
      },
      "source": [
        "### Outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGSW89q1ybCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doubleMAD(x):\n",
        "    m = median(x)\n",
        "    absDev = abs(x-m)\n",
        "    MAD_L = median(absDev[x<=m]) #left mean abs dev (MAD)\n",
        "    MAD_R = median(absDev[x>m]) #right MAD\n",
        "    return (MAD_L,MAD_R)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FytNtlW2ybC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def doubleMADfromMed(x):\n",
        "    MAD_L,MAD_R = doubleMAD(x)#find MADs for both x<=m and x>m\n",
        "    #print(MAD_L,MAD_R)\n",
        "    m = median(x)# calc median\n",
        "    distance = [0]*len(x) #set distance to 0 to avoid taggin x's w/ 0 MADs as outliers (since threshold=3 or 3.5 )\n",
        "    if MAD_L!=0 and MAD_R!=0:\n",
        "        xMAD = [MAD_L]*len(x)\n",
        "        for i in range(len(x)):\n",
        "            if x[i]>m: \n",
        "                xMAD[i] = MAD_R   \n",
        "            distance[i] =  abs(x[i]-m)/xMAD[i]#update distance value for non-zero MADs  \n",
        "            \n",
        "    return np.array(distance) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqcDUFMfybC-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def handleOutliers(x):\n",
        "    dist = doubleMADfromMed(x)\n",
        "    if len(dist)/len(x) <= 0.1: # if number of outliers found is way too MUCH\n",
        "                                # dont change anything, probably they are not outliers\n",
        "                                #otherwise 12\n",
        "        x[dist>=3.5] = median(x) #replace detected outliers w/ median values \n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-fk7E1HybDJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "7295e5d7-614a-49c7-cb3b-3ae48e100b02"
      },
      "source": [
        "tic = time.clock() #initialize timer\n",
        "X_trans2 =  X_trans\n",
        "\n",
        "for v in X_trans2.keys():\n",
        "    MAD_L,MAD_R = doubleMAD(X_trans2[v])\n",
        "    if MAD_L != 0 and MAD_R != 0: #check only the columns w/ non-zero MADs\n",
        "        #print(f\"calculating for {v}\")\n",
        "        X_trans2[v] = handleOutliers(X_trans2[v])    \n",
        "        \n",
        "toc = time.clock() \n",
        "deltaT = toc - tic #calculate time passed\n",
        "print(\"Time passed:\", deltaT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time passed: 8.96455200339551\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zRf7s8IybDU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sampleWeight(y):\n",
        "    wArr=[]\n",
        "    classWeight = y.value_counts(normalize=\"TRUE\").sort_index()\n",
        "    for i in range(len(y)):\n",
        "        cls = y[i]\n",
        "        prob = classWeight[cls]\n",
        "        wArr.append(1/prob) #sampling weights are the inverse of the likelihood of being sampled\n",
        "        \n",
        "    return wArr\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btn41hfrybDc",
        "colab_type": "text"
      },
      "source": [
        "Generate weight column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaAdZXxBybDf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X[\"Weight\"] = sampleWeight(y)\n",
        "X_trans2[\"Weight\"] = sampleWeight(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZDV6LqkybD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Raw data w/o any preprocess\n",
        "X_trainR, X_valR, y_trainR, y_valR = train_test_split(X,y,test_size = 0.20,stratify=y, random_state=42)\n",
        "# Preprocessed data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_trans2,y,test_size = 0.20,stratify=y, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHIq9vC_ybEF",
        "colab_type": "text"
      },
      "source": [
        "With stratified split, all the sets have the same class distributions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtWFNC5DybEH",
        "colab_type": "code",
        "colab": {},
        "outputId": "a5a016fd-85ff-482b-cf90-3846c59d377b"
      },
      "source": [
        "sns.countplot(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x185376d6400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x185376d6240>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oNYgNlbybEO",
        "colab_type": "code",
        "colab": {},
        "outputId": "2e6ec866-5488-4e38-be6c-796b1084260d"
      },
      "source": [
        "sns.countplot(y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x185379d0278>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x185379b9160>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlWmi3jDybEa",
        "colab_type": "code",
        "colab": {},
        "outputId": "65910f07-12cf-4691-da57-3c3edb72ee76"
      },
      "source": [
        "sns.countplot(y_val)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x18544352c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x18537629f60>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtYvcmMaybEl",
        "colab_type": "text"
      },
      "source": [
        "Convert to Dmatrices using weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHueQj1dybEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weightsTrainR = X_trainR[\"Weight\"]\n",
        "weightsValR = X_valR[\"Weight\"]\n",
        "weightsTrain = X_train[\"Weight\"]\n",
        "weightsVal = X_val[\"Weight\"]\n",
        "X_trainR=X_trainR.drop(\"Weight\", axis=1)\n",
        "X_train=X_train.drop(\"Weight\", axis=1)\n",
        "X_val=X_val.drop(\"Weight\", axis=1)\n",
        "X_valR=X_valR.drop(\"Weight\", axis=1)\n",
        "\n",
        "#Raw data\n",
        "dtrainR = xgb.DMatrix(X_trainR.values, label=y_trainR.values, weight = list(weightsTrainR))\n",
        "dvalR = xgb.DMatrix(X_valR.values, label=y_valR.values, weight = list(weightsValR))\n",
        "\n",
        "#Preprocessed data:\n",
        "dtrain = xgb.DMatrix(X_train.values, label=y_train.values, weight = list(weightsTrain)) \n",
        "dval = xgb.DMatrix(X_val.values, label=y_val.values, weight = list(weightsVal))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNyCDtiPybEx",
        "colab_type": "text"
      },
      "source": [
        "## Base Model\n",
        "\n",
        "### Finding Optimal *early_stopping_rounds* and Deciding Whether to Work w/ PP Data\n",
        "\n",
        "User default values for the parameters. Set number of boosting rounds to a large value hoping to find the optimal number of rounds before reaching it, if we haven't improved performance on our test dataset in early_stopping_round rounds. \n",
        "\n",
        "**For raw data:** \n",
        "\n",
        "*early_stopping_rounds=50*\n",
        "\n",
        "Overfits when early stopping is 50. test mlogloss is different than logloss score i printed! why?\n",
        "\n",
        "Stopping. Best iteration:\n",
        "[91]\tTrain-mlogloss:0.262372\tTest-mlogloss:0.574462\n",
        "\n",
        "Precision Score of the Training Set=  0.9050336943489816\n",
        "Precision Score of the Validation Set=  0.7686466309247741\n",
        "Recall Score of the Training Set=  0.9475262446092757\n",
        "Recall Score of the Validation Set=  0.7977945722645391\n",
        "F1 Score of the Training Set=  0.9232506310734429\n",
        "F1 Score of the Validation Set=  0.7810360100814073\n",
        "Accuracy Score the Training Set=  0.9212557068401277\n",
        "Accuracy Score of the Validation Set=  0.8074499030381384\n",
        "Logloss Score Training Set=  0.23754421251685748\n",
        "Logloss Score of the Validation Set=  0.5183675200240478\n",
        "\n",
        "*early_stopping_rounds=20*\n",
        "\n",
        "Still overfits, bur train-val err gap shrinks a little. Go with 20!\n",
        "\n",
        "Stopping. Best iteration:\n",
        "[91]\tTrain-mlogloss:0.262372\tTest-mlogloss:0.574462\n",
        "\n",
        "Precision Score of the Training Set=  0.8826887102750479\n",
        "Precision Score of the Validation Set=  0.7601492425344746\n",
        "Recall Score of the Training Set=  0.9332710752015125\n",
        "Recall Score of the Validation Set=  0.7954885367702247\n",
        "F1 Score of the Training Set=  0.9036441266119595\n",
        "F1 Score of the Validation Set=  0.7746942151975973\n",
        "Accuracy Score the Training Set=  0.9047109207708779\n",
        "Accuracy Score of the Validation Set=  0.8023594053005818\n",
        "Logloss Score Training Set=  0.2809365021279182\n",
        "Logloss Score of the Validation Set=  0.5271007130528547\n",
        "\n",
        "**For preprocessed data:** \n",
        "\n",
        "A very slight improvement when othet metrics are watched :/ \n",
        "I still choose to study with preprocessed data.\n",
        "\n",
        "*early_stopping_rounds=20*\n",
        "\n",
        "Stopping. Best iteration:\n",
        "[95]\tTrain-mlogloss:0.25647\tTest-mlogloss:0.579169\n",
        "\n",
        "Precision Score of the Training Set=  0.883825216224297\n",
        "Precision Score of the Validation Set=  0.7623610899980903\n",
        "Recall Score of the Training Set=  0.9347575557053869\n",
        "Recall Score of the Validation Set=  0.7974404086630739\n",
        "F1 Score of the Training Set=  0.9048876737998365\n",
        "F1 Score of the Validation Set=  0.7767074320735438\n",
        "Accuracy Score the Training Set=  0.9066502363540867\n",
        "Accuracy Score of the Validation Set=  0.8039754363283775\n",
        "Logloss Score Training Set=  0.2761034465086731\n",
        "Logloss Score of the Validation Set=  0.5271038199094024"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgsuDoeBybEy",
        "colab_type": "code",
        "colab": {},
        "outputId": "cb576853-85b6-43a4-baea-34bceabc0a1c"
      },
      "source": [
        "params = {\n",
        "    'eta': 0.3,  \n",
        "    'n_estimators': 1000,\n",
        "    'silent': True,  # option for logging\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass tasks\n",
        "    'num_class': 9,  # number of classes to predic\n",
        "    'max_depth': 6,  # depth of the trees in the boosting process\n",
        "    'seed': 42,\n",
        "    'eval_metric': 'mlogloss', #cross-entropy is chosen as evaluation metric\n",
        "    'min_child_weight':1,\n",
        "    'gamma':0.0,\n",
        "    'subsample':1,\n",
        "    'colsample_bytree':1,\n",
        "    'nthread': 16,\n",
        "    #'scale_pos_weight':1,\n",
        "    'reg_alpha':0\n",
        "    }\n",
        "num_round = 50\n",
        "\n",
        "#training the model\n",
        "#model = xgb.train(params, dtrainR, num_round) #change dtrainR w/ dtrain\n",
        "\n",
        "num_boost_round = 999\n",
        "model = xgb.train(\n",
        "    params,\n",
        "    dtrain,#change dtrainR w/ dtrain\n",
        "    num_boost_round=num_boost_round,\n",
        "    evals=[(dtrain, \"Train\"),(dval, \"Test\")],\n",
        "    early_stopping_rounds=20\n",
        ")\n",
        "\n",
        "probsTrain = model.predict(dtrain)\n",
        "probsVal = model.predict(dval)\n",
        "print(\"Training Class Probabilities for First 5 Instances:\\n\",probsTrain[:5])\n",
        "print(\"Validation Class Probabilities for First 5 Instances:\\n\",probsVal[:5])\n",
        "best_predsTrain = np.asarray([np.argmax(line) for line in probsTrain])\n",
        "best_predsVal = np.asarray([np.argmax(line) for line in probsVal])\n",
        "\n",
        "print(\"Best Predictions for Train:\\n\", best_predsTrain+1)\n",
        "print(\"Best Predictions for Validation:\\n\", best_predsVal+1)\n",
        "#print(min(best_preds+1),max(best_preds+1))\n",
        "\n",
        "print(\"Precision Score of the Training Set= \",precision_score(y_train, best_predsTrain, average='macro'))#change y_train(y_val) to y_trainR(y_valR)\n",
        "print(\"Precision Score of the Validation Set= \",precision_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Recall Score of the Training Set= \",recall_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"Recall Score of the Validation Set= \",recall_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"F1 Score of the Training Set= \",f1_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"F1 Score of the Validation Set= \",f1_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Accuracy Score the Training Set= \", accuracy_score(y_train, best_predsTrain))\n",
        "print(\"Accuracy Score of the Validation Set= \", accuracy_score(y_val, best_predsVal))\n",
        "scoreTrain = log_loss(y_train, probsTrain)\n",
        "scoreVal = log_loss(y_val, probsVal)\n",
        "print(\"Logloss Score Training Set= \", scoreTrain)\n",
        "print(\"Logloss Score of the Validation Set= \", scoreVal)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\tTrain-mlogloss:1.5964\tTest-mlogloss:1.6202\n",
            "Multiple eval metrics have been passed: 'Test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until Test-mlogloss hasn't improved in 20 rounds.\n",
            "[1]\tTrain-mlogloss:1.35619\tTest-mlogloss:1.39182\n",
            "[2]\tTrain-mlogloss:1.19177\tTest-mlogloss:1.23765\n",
            "[3]\tTrain-mlogloss:1.07043\tTest-mlogloss:1.12436\n",
            "[4]\tTrain-mlogloss:0.976215\tTest-mlogloss:1.04053\n",
            "[5]\tTrain-mlogloss:0.9014\tTest-mlogloss:0.972382\n",
            "[6]\tTrain-mlogloss:0.842532\tTest-mlogloss:0.919265\n",
            "[7]\tTrain-mlogloss:0.791158\tTest-mlogloss:0.874871\n",
            "[8]\tTrain-mlogloss:0.749256\tTest-mlogloss:0.839669\n",
            "[9]\tTrain-mlogloss:0.71311\tTest-mlogloss:0.809282\n",
            "[10]\tTrain-mlogloss:0.68242\tTest-mlogloss:0.785452\n",
            "[11]\tTrain-mlogloss:0.65536\tTest-mlogloss:0.763842\n",
            "[12]\tTrain-mlogloss:0.631932\tTest-mlogloss:0.744633\n",
            "[13]\tTrain-mlogloss:0.610872\tTest-mlogloss:0.728672\n",
            "[14]\tTrain-mlogloss:0.593442\tTest-mlogloss:0.715198\n",
            "[15]\tTrain-mlogloss:0.577366\tTest-mlogloss:0.703243\n",
            "[16]\tTrain-mlogloss:0.563679\tTest-mlogloss:0.693741\n",
            "[17]\tTrain-mlogloss:0.549686\tTest-mlogloss:0.684271\n",
            "[18]\tTrain-mlogloss:0.537939\tTest-mlogloss:0.676929\n",
            "[19]\tTrain-mlogloss:0.527176\tTest-mlogloss:0.669959\n",
            "[20]\tTrain-mlogloss:0.518831\tTest-mlogloss:0.663943\n",
            "[21]\tTrain-mlogloss:0.509309\tTest-mlogloss:0.657912\n",
            "[22]\tTrain-mlogloss:0.500949\tTest-mlogloss:0.652595\n",
            "[23]\tTrain-mlogloss:0.492756\tTest-mlogloss:0.647531\n",
            "[24]\tTrain-mlogloss:0.484677\tTest-mlogloss:0.64345\n",
            "[25]\tTrain-mlogloss:0.47814\tTest-mlogloss:0.639836\n",
            "[26]\tTrain-mlogloss:0.471222\tTest-mlogloss:0.636629\n",
            "[27]\tTrain-mlogloss:0.465609\tTest-mlogloss:0.633938\n",
            "[28]\tTrain-mlogloss:0.458961\tTest-mlogloss:0.629978\n",
            "[29]\tTrain-mlogloss:0.452882\tTest-mlogloss:0.626882\n",
            "[30]\tTrain-mlogloss:0.44771\tTest-mlogloss:0.624152\n",
            "[31]\tTrain-mlogloss:0.441901\tTest-mlogloss:0.622507\n",
            "[32]\tTrain-mlogloss:0.436377\tTest-mlogloss:0.619789\n",
            "[33]\tTrain-mlogloss:0.430964\tTest-mlogloss:0.616795\n",
            "[34]\tTrain-mlogloss:0.426155\tTest-mlogloss:0.614592\n",
            "[35]\tTrain-mlogloss:0.421615\tTest-mlogloss:0.612326\n",
            "[36]\tTrain-mlogloss:0.41752\tTest-mlogloss:0.610046\n",
            "[37]\tTrain-mlogloss:0.41273\tTest-mlogloss:0.608\n",
            "[38]\tTrain-mlogloss:0.408104\tTest-mlogloss:0.606323\n",
            "[39]\tTrain-mlogloss:0.404256\tTest-mlogloss:0.604912\n",
            "[40]\tTrain-mlogloss:0.399318\tTest-mlogloss:0.603699\n",
            "[41]\tTrain-mlogloss:0.395568\tTest-mlogloss:0.602221\n",
            "[42]\tTrain-mlogloss:0.39129\tTest-mlogloss:0.600697\n",
            "[43]\tTrain-mlogloss:0.38641\tTest-mlogloss:0.59866\n",
            "[44]\tTrain-mlogloss:0.381926\tTest-mlogloss:0.597136\n",
            "[45]\tTrain-mlogloss:0.377308\tTest-mlogloss:0.595443\n",
            "[46]\tTrain-mlogloss:0.3741\tTest-mlogloss:0.594668\n",
            "[47]\tTrain-mlogloss:0.371427\tTest-mlogloss:0.594124\n",
            "[48]\tTrain-mlogloss:0.36882\tTest-mlogloss:0.593623\n",
            "[49]\tTrain-mlogloss:0.364389\tTest-mlogloss:0.59244\n",
            "[50]\tTrain-mlogloss:0.361009\tTest-mlogloss:0.591232\n",
            "[51]\tTrain-mlogloss:0.357208\tTest-mlogloss:0.591168\n",
            "[52]\tTrain-mlogloss:0.354594\tTest-mlogloss:0.590129\n",
            "[53]\tTrain-mlogloss:0.351605\tTest-mlogloss:0.589746\n",
            "[54]\tTrain-mlogloss:0.348808\tTest-mlogloss:0.588938\n",
            "[55]\tTrain-mlogloss:0.346042\tTest-mlogloss:0.588202\n",
            "[56]\tTrain-mlogloss:0.343048\tTest-mlogloss:0.587412\n",
            "[57]\tTrain-mlogloss:0.339907\tTest-mlogloss:0.586697\n",
            "[58]\tTrain-mlogloss:0.337251\tTest-mlogloss:0.585913\n",
            "[59]\tTrain-mlogloss:0.335253\tTest-mlogloss:0.586158\n",
            "[60]\tTrain-mlogloss:0.332221\tTest-mlogloss:0.585328\n",
            "[61]\tTrain-mlogloss:0.329018\tTest-mlogloss:0.58466\n",
            "[62]\tTrain-mlogloss:0.32616\tTest-mlogloss:0.583946\n",
            "[63]\tTrain-mlogloss:0.323356\tTest-mlogloss:0.582725\n",
            "[64]\tTrain-mlogloss:0.320867\tTest-mlogloss:0.58267\n",
            "[65]\tTrain-mlogloss:0.318229\tTest-mlogloss:0.582653\n",
            "[66]\tTrain-mlogloss:0.315194\tTest-mlogloss:0.581915\n",
            "[67]\tTrain-mlogloss:0.312362\tTest-mlogloss:0.580854\n",
            "[68]\tTrain-mlogloss:0.309805\tTest-mlogloss:0.580155\n",
            "[69]\tTrain-mlogloss:0.308034\tTest-mlogloss:0.579734\n",
            "[70]\tTrain-mlogloss:0.305481\tTest-mlogloss:0.57989\n",
            "[71]\tTrain-mlogloss:0.303594\tTest-mlogloss:0.580101\n",
            "[72]\tTrain-mlogloss:0.301071\tTest-mlogloss:0.579951\n",
            "[73]\tTrain-mlogloss:0.299146\tTest-mlogloss:0.580143\n",
            "[74]\tTrain-mlogloss:0.297481\tTest-mlogloss:0.579797\n",
            "[75]\tTrain-mlogloss:0.295615\tTest-mlogloss:0.579359\n",
            "[76]\tTrain-mlogloss:0.293015\tTest-mlogloss:0.580086\n",
            "[77]\tTrain-mlogloss:0.291899\tTest-mlogloss:0.580073\n",
            "[78]\tTrain-mlogloss:0.289549\tTest-mlogloss:0.580065\n",
            "[79]\tTrain-mlogloss:0.287369\tTest-mlogloss:0.579673\n",
            "[80]\tTrain-mlogloss:0.285459\tTest-mlogloss:0.579522\n",
            "[81]\tTrain-mlogloss:0.283777\tTest-mlogloss:0.579289\n",
            "[82]\tTrain-mlogloss:0.281809\tTest-mlogloss:0.58006\n",
            "[83]\tTrain-mlogloss:0.280508\tTest-mlogloss:0.579994\n",
            "[84]\tTrain-mlogloss:0.278023\tTest-mlogloss:0.579704\n",
            "[85]\tTrain-mlogloss:0.275417\tTest-mlogloss:0.58007\n",
            "[86]\tTrain-mlogloss:0.273631\tTest-mlogloss:0.58\n",
            "[87]\tTrain-mlogloss:0.271701\tTest-mlogloss:0.580066\n",
            "[88]\tTrain-mlogloss:0.269435\tTest-mlogloss:0.580254\n",
            "[89]\tTrain-mlogloss:0.267254\tTest-mlogloss:0.580642\n",
            "[90]\tTrain-mlogloss:0.265525\tTest-mlogloss:0.580282\n",
            "[91]\tTrain-mlogloss:0.26374\tTest-mlogloss:0.579823\n",
            "[92]\tTrain-mlogloss:0.262042\tTest-mlogloss:0.580042\n",
            "[93]\tTrain-mlogloss:0.259783\tTest-mlogloss:0.579802\n",
            "[94]\tTrain-mlogloss:0.25787\tTest-mlogloss:0.57925\n",
            "[95]\tTrain-mlogloss:0.25647\tTest-mlogloss:0.579169\n",
            "[96]\tTrain-mlogloss:0.254288\tTest-mlogloss:0.579775\n",
            "[97]\tTrain-mlogloss:0.252721\tTest-mlogloss:0.579907\n",
            "[98]\tTrain-mlogloss:0.251437\tTest-mlogloss:0.579589\n",
            "[99]\tTrain-mlogloss:0.249754\tTest-mlogloss:0.579875\n",
            "[100]\tTrain-mlogloss:0.247899\tTest-mlogloss:0.580159\n",
            "[101]\tTrain-mlogloss:0.24542\tTest-mlogloss:0.580525\n",
            "[102]\tTrain-mlogloss:0.24355\tTest-mlogloss:0.580324\n",
            "[103]\tTrain-mlogloss:0.241778\tTest-mlogloss:0.580625\n",
            "[104]\tTrain-mlogloss:0.240048\tTest-mlogloss:0.581039\n",
            "[105]\tTrain-mlogloss:0.238503\tTest-mlogloss:0.580654\n",
            "[106]\tTrain-mlogloss:0.237086\tTest-mlogloss:0.580535\n",
            "[107]\tTrain-mlogloss:0.235203\tTest-mlogloss:0.580336\n",
            "[108]\tTrain-mlogloss:0.234041\tTest-mlogloss:0.581193\n",
            "[109]\tTrain-mlogloss:0.232517\tTest-mlogloss:0.580966\n",
            "[110]\tTrain-mlogloss:0.231718\tTest-mlogloss:0.580919\n",
            "[111]\tTrain-mlogloss:0.230244\tTest-mlogloss:0.581223\n",
            "[112]\tTrain-mlogloss:0.228617\tTest-mlogloss:0.58192\n",
            "[113]\tTrain-mlogloss:0.227214\tTest-mlogloss:0.581823\n",
            "[114]\tTrain-mlogloss:0.225306\tTest-mlogloss:0.581713\n",
            "[115]\tTrain-mlogloss:0.224103\tTest-mlogloss:0.58117\n",
            "Stopping. Best iteration:\n",
            "[95]\tTrain-mlogloss:0.25647\tTest-mlogloss:0.579169\n",
            "\n",
            "Training Class Probabilities for First 5 Instances:\n",
            " [[4.74028384e-05 1.61938611e-02 8.45693648e-01 4.19771522e-02\n",
            "  1.98198364e-07 9.42084444e-05 9.59457085e-02 3.78470868e-05\n",
            "  9.94967832e-06]\n",
            " [8.08487894e-05 8.33489776e-01 1.44172296e-01 2.20255088e-02\n",
            "  1.09651346e-07 3.58247744e-05 1.10079309e-04 7.03479673e-05\n",
            "  1.51528538e-05]\n",
            " [7.87248500e-05 5.36060870e-01 4.37779754e-01 8.62859469e-03\n",
            "  7.53155095e-04 4.57936578e-04 1.95550942e-03 1.22365430e-02\n",
            "  2.04893132e-03]\n",
            " [2.20267680e-02 4.55621909e-03 3.02629243e-03 1.23855658e-02\n",
            "  9.00464215e-07 8.92731249e-01 6.19172603e-02 2.45907414e-03\n",
            "  8.96715559e-04]\n",
            " [1.61621033e-03 3.11826943e-06 8.30345834e-06 1.11852467e-04\n",
            "  8.40166138e-07 9.94667768e-01 2.81671615e-04 8.68126925e-04\n",
            "  2.44211010e-03]]\n",
            "Validation Class Probabilities for First 5 Instances:\n",
            " [[2.42534187e-03 7.10688412e-01 1.46506261e-02 2.40177363e-01\n",
            "  3.60821468e-05 2.10648240e-03 2.21606903e-03 2.08252994e-03\n",
            "  2.56170705e-02]\n",
            " [1.85295321e-05 4.90927079e-04 1.14328788e-04 6.50963455e-04\n",
            "  9.98584867e-01 1.13003434e-05 3.75311974e-05 5.22178379e-05\n",
            "  3.93304435e-05]\n",
            " [5.33982296e-04 4.41374630e-01 3.42093676e-01 2.06937373e-01\n",
            "  2.55058512e-05 2.28629171e-04 4.75043757e-03 2.88475049e-03\n",
            "  1.17098994e-03]\n",
            " [3.31017782e-06 9.50476289e-01 3.89616638e-02 1.05138365e-02\n",
            "  9.80446430e-07 2.04145945e-05 1.55157413e-05 5.04564241e-06\n",
            "  2.93536345e-06]\n",
            " [7.69043118e-02 1.65255014e-02 5.88660920e-03 2.25672708e-03\n",
            "  2.64930859e-04 8.60474631e-03 2.94034905e-03 1.05620176e-02\n",
            "  8.76054764e-01]]\n",
            "Best Predictions for Train:\n",
            " [3 2 2 ... 2 6 6]\n",
            "Best Predictions for Validation:\n",
            " [2 5 2 ... 2 2 2]\n",
            "Precision Score of the Training Set=  0.883825216224297\n",
            "Precision Score of the Validation Set=  0.7623610899980903\n",
            "Recall Score of the Training Set=  0.9347575557053869\n",
            "Recall Score of the Validation Set=  0.7974404086630739\n",
            "F1 Score of the Training Set=  0.9048876737998365\n",
            "F1 Score of the Validation Set=  0.7767074320735438\n",
            "Accuracy Score the Training Set=  0.9066502363540867\n",
            "Accuracy Score of the Validation Set=  0.8039754363283775\n",
            "Logloss Score Training Set=  0.2761034465086731\n",
            "Logloss Score of the Validation Set=  0.5271038199094024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEOKYcR-ybE5",
        "colab_type": "text"
      },
      "source": [
        "A function to pass instance weights while CV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzfiHGa7ybE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fpreproc(dtrain, dtest, param):\n",
        "    label = dtrain.get_label()\n",
        "    #ratio = float(np.sum(label == 0)) / np.sum(label==1)\n",
        "    #param['scale_pos_weight'] = ratio\n",
        "    wtrain = dtrain.get_weight()\n",
        "    wtest = dtest.get_weight()\n",
        "    sum_weight = sum(wtrain) + sum(wtest)\n",
        "    wtrain *= sum_weight / sum(wtrain)\n",
        "    wtest *= sum_weight / sum(wtest)\n",
        "    dtrain.set_weight(wtrain)\n",
        "    dtest.set_weight(wtest)\n",
        "    return (dtrain, dtest, param)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZP5AAq6ybE_",
        "colab_type": "text"
      },
      "source": [
        "Confusion Matrices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g26iWX6rybFB",
        "colab_type": "code",
        "colab": {},
        "outputId": "c44b637a-84ec-4fd7-992f-07e6e5f53ca7"
      },
      "source": [
        "print(\"Confusion Matrix of the Training Set: \\n\")\n",
        "print(confusion_matrix(y_train, best_predsTrain))\n",
        "print(\"Confusion Matrix of the Validation Set: \\n\")\n",
        "print(confusion_matrix(y_val,best_predsVal))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix of the Training Set: \n",
            "\n",
            "[[ 1515     0     1     0     0     1    10    10     6]\n",
            " [   18 10260  1801   611    12     2   152    13    29]\n",
            " [    5   610  5414   273     0     0    96     4     1]\n",
            " [    4    62    81  1999     0     0     7     0     0]\n",
            " [    1     0     0     0  2188     0     2     0     0]\n",
            " [  102    14    10    14     2 10963    85    69    49]\n",
            " [   18    17    23    22     1     4  2184     1     1]\n",
            " [  118    14    16     1     2    15    46  6515    44]\n",
            " [   71    12     2     6     0     4    10    16  3843]]\n",
            "Confusion Matrix of the Validation Set: \n",
            "\n",
            "[[ 264    5    2    4    0   15   13   36   47]\n",
            " [   9 2259  670  192    5    9   60    8   12]\n",
            " [   1  350 1047  118    0    4   71    5    5]\n",
            " [   0   57   75  380    0    9   15    0    2]\n",
            " [   2    3    0    0  541    1    1    0    0]\n",
            " [  31   11    6    8    0 2643   42   41   45]\n",
            " [  25   23   47   10    3   15  415   22    8]\n",
            " [  59    7    5    0    0   28   27 1538   29]\n",
            " [  63    5    0    2    2   22    9   25  863]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZF5iwIKybFJ",
        "colab_type": "text"
      },
      "source": [
        "5-fold CV with the same parameters with predefined weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zOnKgYwybFL",
        "colab_type": "code",
        "colab": {},
        "outputId": "d45bab28-5f41-457c-cec7-2c1cede075bb"
      },
      "source": [
        "xgb.cv(params, dtrain, num_round, nfold=5, stratified=True, early_stopping_rounds=20,\n",
        "       metrics=\"mlogloss\", seed = 42, fpreproc = fpreproc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>test-mlogloss-mean</th>\n",
              "      <th>test-mlogloss-std</th>\n",
              "      <th>train-mlogloss-mean</th>\n",
              "      <th>train-mlogloss-std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.621867</td>\n",
              "      <td>0.004641</td>\n",
              "      <td>1.592724</td>\n",
              "      <td>0.001862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.392555</td>\n",
              "      <td>0.005024</td>\n",
              "      <td>1.348014</td>\n",
              "      <td>0.001800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.240216</td>\n",
              "      <td>0.005668</td>\n",
              "      <td>1.183735</td>\n",
              "      <td>0.001577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.129256</td>\n",
              "      <td>0.006175</td>\n",
              "      <td>1.061124</td>\n",
              "      <td>0.002383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.045067</td>\n",
              "      <td>0.006519</td>\n",
              "      <td>0.966904</td>\n",
              "      <td>0.002337</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.978236</td>\n",
              "      <td>0.006330</td>\n",
              "      <td>0.890904</td>\n",
              "      <td>0.001889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.925011</td>\n",
              "      <td>0.006669</td>\n",
              "      <td>0.829489</td>\n",
              "      <td>0.001822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.881365</td>\n",
              "      <td>0.006890</td>\n",
              "      <td>0.778292</td>\n",
              "      <td>0.002053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.845677</td>\n",
              "      <td>0.007813</td>\n",
              "      <td>0.735576</td>\n",
              "      <td>0.001418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.815931</td>\n",
              "      <td>0.008294</td>\n",
              "      <td>0.699266</td>\n",
              "      <td>0.001449</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.790865</td>\n",
              "      <td>0.008008</td>\n",
              "      <td>0.667750</td>\n",
              "      <td>0.001997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.769097</td>\n",
              "      <td>0.007785</td>\n",
              "      <td>0.640045</td>\n",
              "      <td>0.001782</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.752095</td>\n",
              "      <td>0.007737</td>\n",
              "      <td>0.617176</td>\n",
              "      <td>0.001735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.737269</td>\n",
              "      <td>0.008399</td>\n",
              "      <td>0.596386</td>\n",
              "      <td>0.001971</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.724043</td>\n",
              "      <td>0.008194</td>\n",
              "      <td>0.578130</td>\n",
              "      <td>0.001677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.712606</td>\n",
              "      <td>0.008780</td>\n",
              "      <td>0.561140</td>\n",
              "      <td>0.001983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.702468</td>\n",
              "      <td>0.008230</td>\n",
              "      <td>0.546340</td>\n",
              "      <td>0.001962</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.694024</td>\n",
              "      <td>0.008287</td>\n",
              "      <td>0.533572</td>\n",
              "      <td>0.002409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.686739</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>0.521633</td>\n",
              "      <td>0.001997</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.680279</td>\n",
              "      <td>0.008456</td>\n",
              "      <td>0.510838</td>\n",
              "      <td>0.002318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.673928</td>\n",
              "      <td>0.008296</td>\n",
              "      <td>0.500090</td>\n",
              "      <td>0.002273</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.668157</td>\n",
              "      <td>0.008181</td>\n",
              "      <td>0.490362</td>\n",
              "      <td>0.002501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.663283</td>\n",
              "      <td>0.008399</td>\n",
              "      <td>0.481399</td>\n",
              "      <td>0.002663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.658956</td>\n",
              "      <td>0.008511</td>\n",
              "      <td>0.473204</td>\n",
              "      <td>0.002894</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.654985</td>\n",
              "      <td>0.008525</td>\n",
              "      <td>0.465168</td>\n",
              "      <td>0.003099</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.651257</td>\n",
              "      <td>0.008556</td>\n",
              "      <td>0.457208</td>\n",
              "      <td>0.003289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.647921</td>\n",
              "      <td>0.008616</td>\n",
              "      <td>0.450608</td>\n",
              "      <td>0.003563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.644702</td>\n",
              "      <td>0.008415</td>\n",
              "      <td>0.443620</td>\n",
              "      <td>0.003455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.641567</td>\n",
              "      <td>0.008579</td>\n",
              "      <td>0.437057</td>\n",
              "      <td>0.003660</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.638580</td>\n",
              "      <td>0.008348</td>\n",
              "      <td>0.430624</td>\n",
              "      <td>0.004177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.636049</td>\n",
              "      <td>0.008130</td>\n",
              "      <td>0.424759</td>\n",
              "      <td>0.003602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.633750</td>\n",
              "      <td>0.007758</td>\n",
              "      <td>0.419356</td>\n",
              "      <td>0.003598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.631509</td>\n",
              "      <td>0.007691</td>\n",
              "      <td>0.414087</td>\n",
              "      <td>0.004001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.629495</td>\n",
              "      <td>0.008071</td>\n",
              "      <td>0.408081</td>\n",
              "      <td>0.004029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.627227</td>\n",
              "      <td>0.007588</td>\n",
              "      <td>0.402713</td>\n",
              "      <td>0.003514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.624968</td>\n",
              "      <td>0.007330</td>\n",
              "      <td>0.397542</td>\n",
              "      <td>0.003698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.623086</td>\n",
              "      <td>0.007356</td>\n",
              "      <td>0.392412</td>\n",
              "      <td>0.003005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.621370</td>\n",
              "      <td>0.007433</td>\n",
              "      <td>0.387291</td>\n",
              "      <td>0.002878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.619658</td>\n",
              "      <td>0.007170</td>\n",
              "      <td>0.382719</td>\n",
              "      <td>0.002554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.618171</td>\n",
              "      <td>0.007061</td>\n",
              "      <td>0.378529</td>\n",
              "      <td>0.002717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.616920</td>\n",
              "      <td>0.006785</td>\n",
              "      <td>0.374760</td>\n",
              "      <td>0.002788</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.615541</td>\n",
              "      <td>0.006900</td>\n",
              "      <td>0.370432</td>\n",
              "      <td>0.002522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.614489</td>\n",
              "      <td>0.006765</td>\n",
              "      <td>0.366706</td>\n",
              "      <td>0.002605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.613073</td>\n",
              "      <td>0.007272</td>\n",
              "      <td>0.362356</td>\n",
              "      <td>0.002641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.611840</td>\n",
              "      <td>0.006988</td>\n",
              "      <td>0.358851</td>\n",
              "      <td>0.002850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.610947</td>\n",
              "      <td>0.007368</td>\n",
              "      <td>0.354965</td>\n",
              "      <td>0.002135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.609680</td>\n",
              "      <td>0.007187</td>\n",
              "      <td>0.350531</td>\n",
              "      <td>0.002030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.608790</td>\n",
              "      <td>0.007159</td>\n",
              "      <td>0.346661</td>\n",
              "      <td>0.002227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.608223</td>\n",
              "      <td>0.007345</td>\n",
              "      <td>0.343379</td>\n",
              "      <td>0.002371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.607270</td>\n",
              "      <td>0.007438</td>\n",
              "      <td>0.339610</td>\n",
              "      <td>0.002689</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    test-mlogloss-mean  test-mlogloss-std  train-mlogloss-mean  \\\n",
              "0             1.621867           0.004641             1.592724   \n",
              "1             1.392555           0.005024             1.348014   \n",
              "2             1.240216           0.005668             1.183735   \n",
              "3             1.129256           0.006175             1.061124   \n",
              "4             1.045067           0.006519             0.966904   \n",
              "5             0.978236           0.006330             0.890904   \n",
              "6             0.925011           0.006669             0.829489   \n",
              "7             0.881365           0.006890             0.778292   \n",
              "8             0.845677           0.007813             0.735576   \n",
              "9             0.815931           0.008294             0.699266   \n",
              "10            0.790865           0.008008             0.667750   \n",
              "11            0.769097           0.007785             0.640045   \n",
              "12            0.752095           0.007737             0.617176   \n",
              "13            0.737269           0.008399             0.596386   \n",
              "14            0.724043           0.008194             0.578130   \n",
              "15            0.712606           0.008780             0.561140   \n",
              "16            0.702468           0.008230             0.546340   \n",
              "17            0.694024           0.008287             0.533572   \n",
              "18            0.686739           0.008000             0.521633   \n",
              "19            0.680279           0.008456             0.510838   \n",
              "20            0.673928           0.008296             0.500090   \n",
              "21            0.668157           0.008181             0.490362   \n",
              "22            0.663283           0.008399             0.481399   \n",
              "23            0.658956           0.008511             0.473204   \n",
              "24            0.654985           0.008525             0.465168   \n",
              "25            0.651257           0.008556             0.457208   \n",
              "26            0.647921           0.008616             0.450608   \n",
              "27            0.644702           0.008415             0.443620   \n",
              "28            0.641567           0.008579             0.437057   \n",
              "29            0.638580           0.008348             0.430624   \n",
              "30            0.636049           0.008130             0.424759   \n",
              "31            0.633750           0.007758             0.419356   \n",
              "32            0.631509           0.007691             0.414087   \n",
              "33            0.629495           0.008071             0.408081   \n",
              "34            0.627227           0.007588             0.402713   \n",
              "35            0.624968           0.007330             0.397542   \n",
              "36            0.623086           0.007356             0.392412   \n",
              "37            0.621370           0.007433             0.387291   \n",
              "38            0.619658           0.007170             0.382719   \n",
              "39            0.618171           0.007061             0.378529   \n",
              "40            0.616920           0.006785             0.374760   \n",
              "41            0.615541           0.006900             0.370432   \n",
              "42            0.614489           0.006765             0.366706   \n",
              "43            0.613073           0.007272             0.362356   \n",
              "44            0.611840           0.006988             0.358851   \n",
              "45            0.610947           0.007368             0.354965   \n",
              "46            0.609680           0.007187             0.350531   \n",
              "47            0.608790           0.007159             0.346661   \n",
              "48            0.608223           0.007345             0.343379   \n",
              "49            0.607270           0.007438             0.339610   \n",
              "\n",
              "    train-mlogloss-std  \n",
              "0             0.001862  \n",
              "1             0.001800  \n",
              "2             0.001577  \n",
              "3             0.002383  \n",
              "4             0.002337  \n",
              "5             0.001889  \n",
              "6             0.001822  \n",
              "7             0.002053  \n",
              "8             0.001418  \n",
              "9             0.001449  \n",
              "10            0.001997  \n",
              "11            0.001782  \n",
              "12            0.001735  \n",
              "13            0.001971  \n",
              "14            0.001677  \n",
              "15            0.001983  \n",
              "16            0.001962  \n",
              "17            0.002409  \n",
              "18            0.001997  \n",
              "19            0.002318  \n",
              "20            0.002273  \n",
              "21            0.002501  \n",
              "22            0.002663  \n",
              "23            0.002894  \n",
              "24            0.003099  \n",
              "25            0.003289  \n",
              "26            0.003563  \n",
              "27            0.003455  \n",
              "28            0.003660  \n",
              "29            0.004177  \n",
              "30            0.003602  \n",
              "31            0.003598  \n",
              "32            0.004001  \n",
              "33            0.004029  \n",
              "34            0.003514  \n",
              "35            0.003698  \n",
              "36            0.003005  \n",
              "37            0.002878  \n",
              "38            0.002554  \n",
              "39            0.002717  \n",
              "40            0.002788  \n",
              "41            0.002522  \n",
              "42            0.002605  \n",
              "43            0.002641  \n",
              "44            0.002850  \n",
              "45            0.002135  \n",
              "46            0.002030  \n",
              "47            0.002227  \n",
              "48            0.002371  \n",
              "49            0.002689  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7qJNbNCybFV",
        "colab_type": "text"
      },
      "source": [
        "Let's see what cross-validation score we get with our current parameters. Unfortunately, there is no nthread parameter to be set for cv method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Npd2Dma9ybFW",
        "colab_type": "text"
      },
      "source": [
        "Let's make a list containing all the combinations max_depth/min_child_weight that we want to try."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdbN_Yu-ybFX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gridsearch_params = [\n",
        "    (max_depth, min_child_weight)\n",
        "    for max_depth in range(3,9,2)\n",
        "    for min_child_weight in range(1,6,2)\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Bb91gFybFd",
        "colab_type": "text"
      },
      "source": [
        "Best params: 5, 3, mlogloss: 0.5892252 GridsearchCV de min loss u veren parametreleri best olarak seÃ§iyor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LppFAOpvybFe",
        "colab_type": "code",
        "colab": {},
        "outputId": "2b765a9d-c6a3-42b2-a81e-1148c3f2c908"
      },
      "source": [
        "# Define initial best params and mlogloss\n",
        "min_mlogloss = float(\"Inf\")\n",
        "best_params = None\n",
        "for max_depth, min_child_weight in gridsearch_params:\n",
        "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
        "                             max_depth,\n",
        "                             min_child_weight))\n",
        "\n",
        "    # Update our parameters\n",
        "    params['max_depth'] = max_depth\n",
        "    params['min_child_weight'] = min_child_weight\n",
        "\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=num_boost_round,\n",
        "        seed=42,\n",
        "        nfold=5,\n",
        "        metrics={'mlogloss'},\n",
        "        early_stopping_rounds=20,\n",
        "        fpreproc = fpreproc, #use custom fn to update weights\n",
        "        stratified =True,\n",
        "        verbose_eval =10\n",
        "    )\n",
        "\n",
        "    # Update best mlogloss\n",
        "    mean_mlogloss = cv_results['test-mlogloss-mean'].min()\n",
        "    boost_rounds = cv_results['test-mlogloss-mean'].argmin()\n",
        "    print(\"\\tmlogloss {} for {} rounds\".format(mean_mlogloss, boost_rounds))\n",
        "    if mean_mlogloss < min_mlogloss:\n",
        "        min_mlogloss = mean_mlogloss\n",
        "        best_params = (max_depth,min_child_weight)\n",
        "\n",
        "print(\"Best params: {}, {}, mlogloss: {}\".format(best_params[0], best_params[1], min_mlogloss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV with max_depth=3, min_child_weight=1\n",
            "[0]\ttrain-mlogloss:1.73069+0.001075\ttest-mlogloss:1.73607+0.00265613\n",
            "[10]\ttrain-mlogloss:0.94512+0.00248545\ttest-mlogloss:0.971084+0.00580815\n",
            "[20]\ttrain-mlogloss:0.781473+0.00253195\ttest-mlogloss:0.820804+0.00544105\n",
            "[30]\ttrain-mlogloss:0.705435+0.00261178\ttest-mlogloss:0.757391+0.00510114\n",
            "[40]\ttrain-mlogloss:0.657154+0.00195872\ttest-mlogloss:0.721251+0.00540582\n",
            "[50]\ttrain-mlogloss:0.621849+0.00190172\ttest-mlogloss:0.696674+0.00514572\n",
            "[60]\ttrain-mlogloss:0.594264+0.00170787\ttest-mlogloss:0.678219+0.00527975\n",
            "[70]\ttrain-mlogloss:0.571425+0.00168668\ttest-mlogloss:0.664258+0.00463174\n",
            "[80]\ttrain-mlogloss:0.552201+0.00216229\ttest-mlogloss:0.653509+0.00459371\n",
            "[90]\ttrain-mlogloss:0.53483+0.00206309\ttest-mlogloss:0.644261+0.00448913\n",
            "[100]\ttrain-mlogloss:0.519168+0.00182767\ttest-mlogloss:0.636488+0.00502219\n",
            "[110]\ttrain-mlogloss:0.50532+0.00186956\ttest-mlogloss:0.630407+0.00486039\n",
            "[120]\ttrain-mlogloss:0.4931+0.00150865\ttest-mlogloss:0.625323+0.00516363\n",
            "[130]\ttrain-mlogloss:0.482108+0.00127959\ttest-mlogloss:0.621279+0.0053946\n",
            "[140]\ttrain-mlogloss:0.471194+0.0018868\ttest-mlogloss:0.616725+0.00571989\n",
            "[150]\ttrain-mlogloss:0.460698+0.00138372\ttest-mlogloss:0.613135+0.00566443\n",
            "[160]\ttrain-mlogloss:0.451761+0.00129643\ttest-mlogloss:0.610217+0.00588326\n",
            "[170]\ttrain-mlogloss:0.443225+0.0011296\ttest-mlogloss:0.608303+0.00615227\n",
            "[180]\ttrain-mlogloss:0.434517+0.00150021\ttest-mlogloss:0.605435+0.00601834\n",
            "[190]\ttrain-mlogloss:0.426896+0.0016426\ttest-mlogloss:0.60345+0.0056914\n",
            "[200]\ttrain-mlogloss:0.419271+0.00143615\ttest-mlogloss:0.601482+0.00568065\n",
            "[210]\ttrain-mlogloss:0.412116+0.00187357\ttest-mlogloss:0.599913+0.00575709\n",
            "[220]\ttrain-mlogloss:0.40528+0.00186441\ttest-mlogloss:0.598278+0.00604581\n",
            "[230]\ttrain-mlogloss:0.398937+0.00191956\ttest-mlogloss:0.597323+0.00587476\n",
            "[240]\ttrain-mlogloss:0.39281+0.00200009\ttest-mlogloss:0.596779+0.00610719\n",
            "[250]\ttrain-mlogloss:0.386728+0.00211068\ttest-mlogloss:0.596115+0.00631655\n",
            "[260]\ttrain-mlogloss:0.380676+0.00215823\ttest-mlogloss:0.595632+0.00614024\n",
            "[270]\ttrain-mlogloss:0.374888+0.00219431\ttest-mlogloss:0.595457+0.00584972\n",
            "[280]\ttrain-mlogloss:0.369354+0.00242459\ttest-mlogloss:0.59438+0.00620294\n",
            "[290]\ttrain-mlogloss:0.364112+0.00296357\ttest-mlogloss:0.594138+0.00630409\n",
            "[300]\ttrain-mlogloss:0.358269+0.00284088\ttest-mlogloss:0.593704+0.00624737\n",
            "[310]\ttrain-mlogloss:0.353518+0.0028903\ttest-mlogloss:0.593706+0.00627039\n",
            "[320]\ttrain-mlogloss:0.348686+0.00314058\ttest-mlogloss:0.593662+0.00630826\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tmlogloss 0.5935242 for 307 rounds\n",
            "CV with max_depth=3, min_child_weight=3\n",
            "[0]\ttrain-mlogloss:1.73069+0.001075\ttest-mlogloss:1.73607+0.00265613\n",
            "[10]\ttrain-mlogloss:0.944985+0.00241862\ttest-mlogloss:0.970946+0.00563399\n",
            "[20]\ttrain-mlogloss:0.781622+0.00239637\ttest-mlogloss:0.820855+0.00537131\n",
            "[30]\ttrain-mlogloss:0.705836+0.00239016\ttest-mlogloss:0.758684+0.00605133\n",
            "[40]\ttrain-mlogloss:0.657522+0.00197529\ttest-mlogloss:0.721929+0.00634543\n",
            "[50]\ttrain-mlogloss:0.622429+0.00209208\ttest-mlogloss:0.697337+0.00638251\n",
            "[60]\ttrain-mlogloss:0.594504+0.00186124\ttest-mlogloss:0.678717+0.00599008\n",
            "[70]\ttrain-mlogloss:0.571256+0.00162674\ttest-mlogloss:0.664108+0.0060512\n",
            "[80]\ttrain-mlogloss:0.552024+0.00196599\ttest-mlogloss:0.653202+0.00550149\n",
            "[90]\ttrain-mlogloss:0.534866+0.00166873\ttest-mlogloss:0.643638+0.00621862\n",
            "[100]\ttrain-mlogloss:0.519823+0.00194746\ttest-mlogloss:0.636611+0.00624082\n",
            "[110]\ttrain-mlogloss:0.506916+0.00226686\ttest-mlogloss:0.630953+0.00567429\n",
            "[120]\ttrain-mlogloss:0.493835+0.00208538\ttest-mlogloss:0.625044+0.00565028\n",
            "[130]\ttrain-mlogloss:0.482513+0.00239199\ttest-mlogloss:0.620728+0.00585416\n",
            "[140]\ttrain-mlogloss:0.471979+0.00200973\ttest-mlogloss:0.616524+0.00626669\n",
            "[150]\ttrain-mlogloss:0.462571+0.00209992\ttest-mlogloss:0.613383+0.00619201\n",
            "[160]\ttrain-mlogloss:0.45308+0.00195391\ttest-mlogloss:0.610574+0.00684319\n",
            "[170]\ttrain-mlogloss:0.44346+0.00229363\ttest-mlogloss:0.60799+0.00677493\n",
            "[180]\ttrain-mlogloss:0.434996+0.00242502\ttest-mlogloss:0.605504+0.00672109\n",
            "[190]\ttrain-mlogloss:0.427036+0.00225455\ttest-mlogloss:0.603556+0.00682268\n",
            "[200]\ttrain-mlogloss:0.419737+0.00219789\ttest-mlogloss:0.601891+0.00654515\n",
            "[210]\ttrain-mlogloss:0.412514+0.00245062\ttest-mlogloss:0.600656+0.00679787\n",
            "[220]\ttrain-mlogloss:0.405534+0.00263241\ttest-mlogloss:0.599254+0.00668017\n",
            "[230]\ttrain-mlogloss:0.399125+0.0026988\ttest-mlogloss:0.598149+0.00655007\n",
            "[240]\ttrain-mlogloss:0.392866+0.00271004\ttest-mlogloss:0.59706+0.00643861\n",
            "[250]\ttrain-mlogloss:0.386861+0.0030047\ttest-mlogloss:0.596061+0.00617703\n",
            "[260]\ttrain-mlogloss:0.380908+0.00311955\ttest-mlogloss:0.595058+0.00627805\n",
            "[270]\ttrain-mlogloss:0.375001+0.00297943\ttest-mlogloss:0.594755+0.00669493\n",
            "[280]\ttrain-mlogloss:0.369414+0.00316045\ttest-mlogloss:0.594192+0.00673714\n",
            "[290]\ttrain-mlogloss:0.364519+0.00354684\ttest-mlogloss:0.593565+0.00668891\n",
            "[300]\ttrain-mlogloss:0.359376+0.00360566\ttest-mlogloss:0.593368+0.00674987\n",
            "[310]\ttrain-mlogloss:0.354357+0.00377474\ttest-mlogloss:0.593144+0.00657696\n",
            "[320]\ttrain-mlogloss:0.349358+0.00374462\ttest-mlogloss:0.593391+0.0064964\n",
            "[330]\ttrain-mlogloss:0.344629+0.00376391\ttest-mlogloss:0.593118+0.00664123\n",
            "[340]\ttrain-mlogloss:0.340075+0.00398427\ttest-mlogloss:0.593106+0.00645086\n",
            "[350]\ttrain-mlogloss:0.335462+0.00385763\ttest-mlogloss:0.593467+0.00679469\n",
            "\tmlogloss 0.5929673999999999 for 331 rounds\n",
            "CV with max_depth=3, min_child_weight=5\n",
            "[0]\ttrain-mlogloss:1.73069+0.001075\ttest-mlogloss:1.73607+0.00265613\n",
            "[10]\ttrain-mlogloss:0.944883+0.00238766\ttest-mlogloss:0.970938+0.00564326\n",
            "[20]\ttrain-mlogloss:0.781713+0.00264535\ttest-mlogloss:0.820553+0.00568861\n",
            "[30]\ttrain-mlogloss:0.705849+0.0019009\ttest-mlogloss:0.758271+0.00689953\n",
            "[40]\ttrain-mlogloss:0.657503+0.00246935\ttest-mlogloss:0.722045+0.00605183\n",
            "[50]\ttrain-mlogloss:0.622923+0.00249223\ttest-mlogloss:0.697933+0.0053347\n",
            "[60]\ttrain-mlogloss:0.59529+0.00223988\ttest-mlogloss:0.679452+0.00505675\n",
            "[70]\ttrain-mlogloss:0.572905+0.00187122\ttest-mlogloss:0.665545+0.00515875\n",
            "[80]\ttrain-mlogloss:0.552988+0.00207093\ttest-mlogloss:0.654046+0.00542023\n",
            "[90]\ttrain-mlogloss:0.536474+0.00202815\ttest-mlogloss:0.644806+0.0058809\n",
            "[100]\ttrain-mlogloss:0.521382+0.00193155\ttest-mlogloss:0.636618+0.00619246\n",
            "[110]\ttrain-mlogloss:0.507264+0.00202737\ttest-mlogloss:0.630484+0.00627223\n",
            "[120]\ttrain-mlogloss:0.494883+0.00196666\ttest-mlogloss:0.625104+0.00627821\n",
            "[130]\ttrain-mlogloss:0.483579+0.00153539\ttest-mlogloss:0.620455+0.00665636\n",
            "[140]\ttrain-mlogloss:0.472942+0.00160416\ttest-mlogloss:0.616418+0.00663332\n",
            "[150]\ttrain-mlogloss:0.463136+0.00160876\ttest-mlogloss:0.612662+0.00683955\n",
            "[160]\ttrain-mlogloss:0.453961+0.00180503\ttest-mlogloss:0.610012+0.00675163\n",
            "[170]\ttrain-mlogloss:0.445351+0.00189608\ttest-mlogloss:0.607433+0.00708984\n",
            "[180]\ttrain-mlogloss:0.437132+0.00156481\ttest-mlogloss:0.605018+0.0071113\n",
            "[190]\ttrain-mlogloss:0.429363+0.0017881\ttest-mlogloss:0.603651+0.00689616\n",
            "[200]\ttrain-mlogloss:0.421907+0.00175719\ttest-mlogloss:0.601492+0.00721403\n",
            "[210]\ttrain-mlogloss:0.414674+0.00187365\ttest-mlogloss:0.599799+0.00686029\n",
            "[220]\ttrain-mlogloss:0.407865+0.00177361\ttest-mlogloss:0.598573+0.00700504\n",
            "[230]\ttrain-mlogloss:0.401491+0.00207876\ttest-mlogloss:0.597455+0.0068627\n",
            "[240]\ttrain-mlogloss:0.395039+0.00228277\ttest-mlogloss:0.596026+0.0068735\n",
            "[250]\ttrain-mlogloss:0.389044+0.00217484\ttest-mlogloss:0.594621+0.0068974\n",
            "[260]\ttrain-mlogloss:0.383181+0.00182152\ttest-mlogloss:0.593858+0.00681419\n",
            "[270]\ttrain-mlogloss:0.377455+0.00208441\ttest-mlogloss:0.593281+0.00693428\n",
            "[280]\ttrain-mlogloss:0.371947+0.00252526\ttest-mlogloss:0.593467+0.00702624\n",
            "[290]\ttrain-mlogloss:0.366791+0.00266751\ttest-mlogloss:0.592897+0.00688884\n",
            "[300]\ttrain-mlogloss:0.361136+0.0029252\ttest-mlogloss:0.592319+0.0065508\n",
            "[310]\ttrain-mlogloss:0.356143+0.00300037\ttest-mlogloss:0.59161+0.00675168\n",
            "[320]\ttrain-mlogloss:0.351351+0.00326764\ttest-mlogloss:0.591272+0.00683832\n",
            "[330]\ttrain-mlogloss:0.346732+0.00354331\ttest-mlogloss:0.591447+0.0068308\n",
            "\tmlogloss 0.5911772 for 317 rounds\n",
            "CV with max_depth=5, min_child_weight=1\n",
            "[0]\ttrain-mlogloss:1.63309+0.00250091\ttest-mlogloss:1.65118+0.00441169\n",
            "[10]\ttrain-mlogloss:0.753366+0.00244335\ttest-mlogloss:0.831738+0.00636359\n",
            "[20]\ttrain-mlogloss:0.585829+0.00258863\ttest-mlogloss:0.704365+0.00717507\n",
            "[30]\ttrain-mlogloss:0.510623+0.00274951\ttest-mlogloss:0.658385+0.00730399\n",
            "[40]\ttrain-mlogloss:0.462096+0.00225268\ttest-mlogloss:0.634485+0.00736672\n",
            "[50]\ttrain-mlogloss:0.425788+0.00272391\ttest-mlogloss:0.619437+0.0075348\n",
            "[60]\ttrain-mlogloss:0.395342+0.0021714\ttest-mlogloss:0.609351+0.00766938\n",
            "[70]\ttrain-mlogloss:0.369869+0.00287479\ttest-mlogloss:0.602314+0.00767673\n",
            "[80]\ttrain-mlogloss:0.347451+0.00289139\ttest-mlogloss:0.597319+0.00779106\n",
            "[90]\ttrain-mlogloss:0.328908+0.00320246\ttest-mlogloss:0.594825+0.00769616\n",
            "[100]\ttrain-mlogloss:0.311555+0.00287274\ttest-mlogloss:0.593165+0.00709712\n",
            "[110]\ttrain-mlogloss:0.295817+0.00248203\ttest-mlogloss:0.592479+0.00713256\n",
            "[120]\ttrain-mlogloss:0.281112+0.00312043\ttest-mlogloss:0.591935+0.00722959\n",
            "[130]\ttrain-mlogloss:0.266431+0.00269342\ttest-mlogloss:0.593191+0.00780402\n",
            "[140]\ttrain-mlogloss:0.254574+0.00245389\ttest-mlogloss:0.594262+0.0079788\n",
            "\tmlogloss 0.5917448000000001 for 122 rounds\n",
            "CV with max_depth=5, min_child_weight=3\n",
            "[0]\ttrain-mlogloss:1.63317+0.00249639\ttest-mlogloss:1.6512+0.0044219\n",
            "[10]\ttrain-mlogloss:0.753224+0.00237597\ttest-mlogloss:0.831645+0.00579182\n",
            "[20]\ttrain-mlogloss:0.586677+0.0025156\ttest-mlogloss:0.703963+0.00615398\n",
            "[30]\ttrain-mlogloss:0.511476+0.00240812\ttest-mlogloss:0.656858+0.00646314\n",
            "[40]\ttrain-mlogloss:0.463647+0.00206658\ttest-mlogloss:0.632241+0.00629128\n",
            "[50]\ttrain-mlogloss:0.426324+0.0026746\ttest-mlogloss:0.616472+0.00668898\n",
            "[60]\ttrain-mlogloss:0.39659+0.00197683\ttest-mlogloss:0.60568+0.00773903\n",
            "[70]\ttrain-mlogloss:0.37136+0.00356637\ttest-mlogloss:0.599312+0.00791754\n",
            "[80]\ttrain-mlogloss:0.349415+0.00259919\ttest-mlogloss:0.594735+0.00773411\n",
            "[90]\ttrain-mlogloss:0.329753+0.00198226\ttest-mlogloss:0.591238+0.00678927\n",
            "[100]\ttrain-mlogloss:0.312563+0.00296997\ttest-mlogloss:0.589883+0.00575158\n",
            "[110]\ttrain-mlogloss:0.297042+0.00352344\ttest-mlogloss:0.589373+0.00575256\n",
            "[120]\ttrain-mlogloss:0.282829+0.00346163\ttest-mlogloss:0.589573+0.00572739\n",
            "\tmlogloss 0.5892252 for 105 rounds\n",
            "CV with max_depth=5, min_child_weight=5\n",
            "[0]\ttrain-mlogloss:1.63328+0.0024971\ttest-mlogloss:1.65139+0.00434458\n",
            "[10]\ttrain-mlogloss:0.753897+0.00202924\ttest-mlogloss:0.831778+0.00627872\n",
            "[20]\ttrain-mlogloss:0.587235+0.00150753\ttest-mlogloss:0.703087+0.00579084\n",
            "[30]\ttrain-mlogloss:0.513023+0.00210629\ttest-mlogloss:0.658398+0.00553365\n",
            "[40]\ttrain-mlogloss:0.465835+0.00294735\ttest-mlogloss:0.633667+0.00557987\n",
            "[50]\ttrain-mlogloss:0.430739+0.00299153\ttest-mlogloss:0.618272+0.00665884\n",
            "[60]\ttrain-mlogloss:0.40133+0.00311728\ttest-mlogloss:0.608408+0.00575556\n",
            "[70]\ttrain-mlogloss:0.375836+0.0021457\ttest-mlogloss:0.601155+0.00553222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[80]\ttrain-mlogloss:0.354399+0.00208967\ttest-mlogloss:0.596125+0.00464464\n",
            "[90]\ttrain-mlogloss:0.334947+0.00303709\ttest-mlogloss:0.592917+0.00434558\n",
            "[100]\ttrain-mlogloss:0.318151+0.00295543\ttest-mlogloss:0.591583+0.00486612\n",
            "[110]\ttrain-mlogloss:0.301732+0.00301987\ttest-mlogloss:0.589306+0.00561791\n",
            "[120]\ttrain-mlogloss:0.28734+0.00195634\ttest-mlogloss:0.589812+0.00620256\n",
            "\tmlogloss 0.5893058 for 110 rounds\n",
            "CV with max_depth=7, min_child_weight=1\n",
            "[0]\ttrain-mlogloss:1.55265+0.0015851\ttest-mlogloss:1.59736+0.00636928\n",
            "[10]\ttrain-mlogloss:0.588459+0.00230657\ttest-mlogloss:0.762751+0.00683637\n",
            "[20]\ttrain-mlogloss:0.419179+0.00287456\ttest-mlogloss:0.65596+0.00772449\n",
            "[30]\ttrain-mlogloss:0.346488+0.00192133\ttest-mlogloss:0.624973+0.00904977\n",
            "[40]\ttrain-mlogloss:0.294884+0.00168671\ttest-mlogloss:0.612977+0.0083048\n",
            "[50]\ttrain-mlogloss:0.25638+0.00202847\ttest-mlogloss:0.60873+0.00932697\n",
            "[60]\ttrain-mlogloss:0.224823+0.00238385\ttest-mlogloss:0.608399+0.00874724\n",
            "[70]\ttrain-mlogloss:0.199818+0.00234231\ttest-mlogloss:0.612418+0.00741297\n",
            "\tmlogloss 0.608008 for 58 rounds\n",
            "CV with max_depth=7, min_child_weight=3\n",
            "[0]\ttrain-mlogloss:1.55315+0.00169838\ttest-mlogloss:1.59736+0.00647255\n",
            "[10]\ttrain-mlogloss:0.589293+0.00121377\ttest-mlogloss:0.763605+0.00661008\n",
            "[20]\ttrain-mlogloss:0.42205+0.00220662\ttest-mlogloss:0.656467+0.00509918\n",
            "[30]\ttrain-mlogloss:0.35017+0.00152677\ttest-mlogloss:0.625194+0.00579476\n",
            "[40]\ttrain-mlogloss:0.299858+0.00164509\ttest-mlogloss:0.611664+0.00591149\n",
            "[50]\ttrain-mlogloss:0.260642+0.000737865\ttest-mlogloss:0.606286+0.00559812\n",
            "[60]\ttrain-mlogloss:0.229972+0.00119655\ttest-mlogloss:0.605691+0.00590111\n",
            "[70]\ttrain-mlogloss:0.203878+0.000879973\ttest-mlogloss:0.609161+0.00639813\n",
            "\tmlogloss 0.6053398000000001 for 57 rounds\n",
            "CV with max_depth=7, min_child_weight=5\n",
            "[0]\ttrain-mlogloss:1.55389+0.00162205\ttest-mlogloss:1.59748+0.00650514\n",
            "[10]\ttrain-mlogloss:0.591554+0.00141694\ttest-mlogloss:0.761709+0.00799979\n",
            "[20]\ttrain-mlogloss:0.423072+0.00106438\ttest-mlogloss:0.653784+0.00783258\n",
            "[30]\ttrain-mlogloss:0.350616+0.00235197\ttest-mlogloss:0.622279+0.00799317\n",
            "[40]\ttrain-mlogloss:0.301819+0.00261342\ttest-mlogloss:0.607952+0.00855985\n",
            "[50]\ttrain-mlogloss:0.263896+0.00291307\ttest-mlogloss:0.603061+0.00806268\n",
            "[60]\ttrain-mlogloss:0.232942+0.00258678\ttest-mlogloss:0.603363+0.00820378\n",
            "\tmlogloss 0.6030610000000001 for 50 rounds\n",
            "Best params: 5, 3, mlogloss: 0.5892252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpWiMcsvybFi",
        "colab_type": "text"
      },
      "source": [
        "Calculation with the same *so called* best parameters but  without preprocessing function!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ7Vd6afybFj",
        "colab_type": "code",
        "colab": {},
        "outputId": "cacb7711-7128-4c4f-ae32-43783638837c"
      },
      "source": [
        "gridsearch_params = [\n",
        "    (max_depth, min_child_weight)\n",
        "    for max_depth in range(5,6,2)\n",
        "    for min_child_weight in range(3,4,2)\n",
        "]\n",
        "# Define initial best params and mlogloss\n",
        "min_mlogloss = float(\"Inf\")\n",
        "best_params = None\n",
        "for max_depth, min_child_weight in gridsearch_params:\n",
        "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
        "                             max_depth,\n",
        "                             min_child_weight))\n",
        "\n",
        "    # Update our parameters\n",
        "    params['max_depth'] = max_depth\n",
        "    params['min_child_weight'] = min_child_weight\n",
        "\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=num_boost_round,\n",
        "        seed=42,\n",
        "        nfold=5,\n",
        "        metrics={'mlogloss'},\n",
        "        early_stopping_rounds=20,\n",
        "        fpreproc = fpreproc, #use custom fn to update weights\n",
        "        stratified =True,\n",
        "        verbose_eval =10\n",
        "    )\n",
        "\n",
        "    # Update best mlogloss\n",
        "    mean_mlogloss = cv_results['test-mlogloss-mean'].min()\n",
        "    boost_rounds = cv_results['test-mlogloss-mean'].argmin()\n",
        "    print(\"\\tmlogloss {} for {} rounds\".format(mean_mlogloss, boost_rounds))\n",
        "    if mean_mlogloss < min_mlogloss:\n",
        "        min_mlogloss = mean_mlogloss\n",
        "        best_params = (max_depth,min_child_weight)\n",
        "\n",
        "print(\"Best params: {}, {}, mlogloss: {}\".format(best_params[0], best_params[1], min_mlogloss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV with max_depth=5, min_child_weight=3\n",
            "[0]\ttrain-mlogloss:1.63317+0.00249639\ttest-mlogloss:1.6512+0.0044219\n",
            "[10]\ttrain-mlogloss:0.753224+0.00237597\ttest-mlogloss:0.831645+0.00579182\n",
            "[20]\ttrain-mlogloss:0.586677+0.0025156\ttest-mlogloss:0.703963+0.00615398\n",
            "[30]\ttrain-mlogloss:0.511476+0.00240812\ttest-mlogloss:0.656858+0.00646314\n",
            "[40]\ttrain-mlogloss:0.463647+0.00206658\ttest-mlogloss:0.632241+0.00629128\n",
            "[50]\ttrain-mlogloss:0.426324+0.0026746\ttest-mlogloss:0.616472+0.00668898\n",
            "[60]\ttrain-mlogloss:0.39659+0.00197683\ttest-mlogloss:0.60568+0.00773903\n",
            "[70]\ttrain-mlogloss:0.37136+0.00356637\ttest-mlogloss:0.599312+0.00791754\n",
            "[80]\ttrain-mlogloss:0.349415+0.00259919\ttest-mlogloss:0.594735+0.00773411\n",
            "[90]\ttrain-mlogloss:0.329753+0.00198226\ttest-mlogloss:0.591238+0.00678927\n",
            "[100]\ttrain-mlogloss:0.312563+0.00296997\ttest-mlogloss:0.589883+0.00575158\n",
            "[110]\ttrain-mlogloss:0.297042+0.00352344\ttest-mlogloss:0.589373+0.00575256\n",
            "[120]\ttrain-mlogloss:0.282829+0.00346163\ttest-mlogloss:0.589573+0.00572739\n",
            "\tmlogloss 0.5892252 for 105 rounds\n",
            "Best params: 5, 3, mlogloss: 0.5892252\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:34: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy5t_hxiybFp",
        "colab_type": "code",
        "colab": {},
        "outputId": "0b76c4d1-eddc-4cc6-eef4-5725ba144b8c"
      },
      "source": [
        "# Define initial best params and mlogloss\n",
        "min_mlogloss = float(\"Inf\")\n",
        "best_params = None\n",
        "for max_depth, min_child_weight in gridsearch_params:\n",
        "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
        "                             max_depth,\n",
        "                             min_child_weight))\n",
        "\n",
        "    # Update our parameters\n",
        "    params['max_depth'] = max_depth\n",
        "    params['min_child_weight'] = min_child_weight\n",
        "\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=num_boost_round,\n",
        "        seed=42,\n",
        "        nfold=5,\n",
        "        metrics={'mlogloss'},\n",
        "        early_stopping_rounds=20,\n",
        "        #fpreproc = fpreproc, #use custom fn to update weights\n",
        "        stratified =True,\n",
        "        verbose_eval =10\n",
        "    )\n",
        "\n",
        "    # Update best mlogloss\n",
        "    mean_mlogloss = cv_results['test-mlogloss-mean'].min()\n",
        "    boost_rounds = cv_results['test-mlogloss-mean'].argmin()\n",
        "    print(\"\\tmlogloss {} for {} rounds\".format(mean_mlogloss, boost_rounds))\n",
        "    if mean_mlogloss < min_mlogloss:\n",
        "        min_mlogloss = mean_mlogloss\n",
        "        best_params = (max_depth,min_child_weight)\n",
        "\n",
        "print(\"Best params: {}, {}, mlogloss: {}\".format(best_params[0], best_params[1], min_mlogloss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV with max_depth=5, min_child_weight=3\n",
            "[0]\ttrain-mlogloss:1.63337+0.00248185\ttest-mlogloss:1.65124+0.00449405\n",
            "[10]\ttrain-mlogloss:0.75451+0.00195769\ttest-mlogloss:0.831917+0.00611168\n",
            "[20]\ttrain-mlogloss:0.587336+0.00202918\ttest-mlogloss:0.702817+0.00716489\n",
            "[30]\ttrain-mlogloss:0.512829+0.00294322\ttest-mlogloss:0.657097+0.00773382\n",
            "[40]\ttrain-mlogloss:0.463559+0.00301874\ttest-mlogloss:0.631597+0.00659729\n",
            "[50]\ttrain-mlogloss:0.426814+0.00208831\ttest-mlogloss:0.616151+0.00801438\n",
            "[60]\ttrain-mlogloss:0.397646+0.00175616\ttest-mlogloss:0.606125+0.00809381\n",
            "[70]\ttrain-mlogloss:0.372035+0.0023276\ttest-mlogloss:0.599096+0.00772513\n",
            "[80]\ttrain-mlogloss:0.350781+0.00277787\ttest-mlogloss:0.594128+0.00762087\n",
            "[90]\ttrain-mlogloss:0.332365+0.0031065\ttest-mlogloss:0.591363+0.00734149\n",
            "[100]\ttrain-mlogloss:0.314877+0.00289042\ttest-mlogloss:0.590001+0.00722353\n",
            "[110]\ttrain-mlogloss:0.298692+0.00304118\ttest-mlogloss:0.588626+0.00774266\n",
            "[120]\ttrain-mlogloss:0.283691+0.00283546\ttest-mlogloss:0.588892+0.00821505\n",
            "\tmlogloss 0.5886264000000001 for 110 rounds\n",
            "Best params: 5, 3, mlogloss: 0.5886264000000001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:29: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xVdh9KrybFx",
        "colab_type": "text"
      },
      "source": [
        "Update max depth and min child weight and observe train/test performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoY8OrKCybFz",
        "colab_type": "code",
        "colab": {},
        "outputId": "344b8a2b-80f3-4d33-893e-ae33cea246c9"
      },
      "source": [
        "params = {\n",
        "    'eta': 0.3,  \n",
        "    'n_estimators': 1000,\n",
        "    'silent': True,  # option for logging\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass tasks\n",
        "    'num_class': 9,  # number of classes to predic\n",
        "    'max_depth': 5,  # depth of the trees in the boosting process\n",
        "    'seed': 42,\n",
        "    'eval_metric': 'mlogloss', #cross-entropy is chosen as evaluation metric\n",
        "    'min_child_weight':3,\n",
        "    'gamma':0.0,\n",
        "    'subsample':1,\n",
        "    'colsample_bytree':1,\n",
        "    'nthread': 16,\n",
        "    #'scale_pos_weight':1,\n",
        "    'reg_alpha':0\n",
        "    }\n",
        "num_round = 50\n",
        "\n",
        "#training the model\n",
        "#model = xgb.train(params, dtrainR, num_round) #change dtrainR w/ dtrain\n",
        "\n",
        "num_boost_round = 999\n",
        "model = xgb.train(\n",
        "    params,\n",
        "    dtrain,#change dtrainR w/ dtrain\n",
        "    num_boost_round=num_boost_round,\n",
        "    evals=[(dtrain, \"Train\"),(dval, \"Test\")],\n",
        "    early_stopping_rounds=20\n",
        ")\n",
        "\n",
        "probsTrain = model.predict(dtrain)\n",
        "probsVal = model.predict(dval)\n",
        "print(\"Training Class Probabilities for First 5 Instances:\\n\",probsTrain[:5])\n",
        "print(\"Validation Class Probabilities for First 5 Instances:\\n\",probsVal[:5])\n",
        "best_predsTrain = np.asarray([np.argmax(line) for line in probsTrain])\n",
        "best_predsVal = np.asarray([np.argmax(line) for line in probsVal])\n",
        "\n",
        "print(\"Best Predictions for Train:\\n\", best_predsTrain+1)\n",
        "print(\"Best Predictions for Validation:\\n\", best_predsVal+1)\n",
        "#print(min(best_preds+1),max(best_preds+1))\n",
        "\n",
        "print(\"Precision Score of the Training Set= \",precision_score(y_train, best_predsTrain, average='macro'))#change y_train(y_val) to y_trainR(y_valR)\n",
        "print(\"Precision Score of the Validation Set= \",precision_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Recall Score of the Training Set= \",recall_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"Recall Score of the Validation Set= \",recall_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"F1 Score of the Training Set= \",f1_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"F1 Score of the Validation Set= \",f1_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Accuracy Score the Training Set= \", accuracy_score(y_train, best_predsTrain))\n",
        "print(\"Accuracy Score of the Validation Set= \", accuracy_score(y_val, best_predsVal))\n",
        "scoreTrain = log_loss(y_train, probsTrain)\n",
        "scoreVal = log_loss(y_val, probsVal)\n",
        "print(\"Logloss Score Training Set= \", scoreTrain)\n",
        "print(\"Logloss Score of the Validation Set= \", scoreVal)\n",
        "\n",
        "print(\"Confusion Matrix of the Training Set: \\n\")\n",
        "print(confusion_matrix(y_train, best_predsTrain))\n",
        "print(\"Confusion Matrix of the Validation Set: \\n\")\n",
        "print(confusion_matrix(y_val,best_predsVal))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\tTrain-mlogloss:1.63465\tTest-mlogloss:1.64918\n",
            "Multiple eval metrics have been passed: 'Test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until Test-mlogloss hasn't improved in 20 rounds.\n",
            "[1]\tTrain-mlogloss:1.40493\tTest-mlogloss:1.42703\n",
            "[2]\tTrain-mlogloss:1.24821\tTest-mlogloss:1.27896\n",
            "[3]\tTrain-mlogloss:1.13471\tTest-mlogloss:1.16942\n",
            "[4]\tTrain-mlogloss:1.04514\tTest-mlogloss:1.08472\n",
            "[5]\tTrain-mlogloss:0.974101\tTest-mlogloss:1.01879\n",
            "[6]\tTrain-mlogloss:0.916946\tTest-mlogloss:0.966518\n",
            "[7]\tTrain-mlogloss:0.867737\tTest-mlogloss:0.921808\n",
            "[8]\tTrain-mlogloss:0.827187\tTest-mlogloss:0.886643\n",
            "[9]\tTrain-mlogloss:0.793427\tTest-mlogloss:0.85738\n",
            "[10]\tTrain-mlogloss:0.762581\tTest-mlogloss:0.83103\n",
            "[11]\tTrain-mlogloss:0.737786\tTest-mlogloss:0.808777\n",
            "[12]\tTrain-mlogloss:0.715919\tTest-mlogloss:0.79037\n",
            "[13]\tTrain-mlogloss:0.694855\tTest-mlogloss:0.773286\n",
            "[14]\tTrain-mlogloss:0.677629\tTest-mlogloss:0.759482\n",
            "[15]\tTrain-mlogloss:0.662376\tTest-mlogloss:0.747205\n",
            "[16]\tTrain-mlogloss:0.648563\tTest-mlogloss:0.736186\n",
            "[17]\tTrain-mlogloss:0.635519\tTest-mlogloss:0.726151\n",
            "[18]\tTrain-mlogloss:0.62281\tTest-mlogloss:0.716366\n",
            "[19]\tTrain-mlogloss:0.612284\tTest-mlogloss:0.707877\n",
            "[20]\tTrain-mlogloss:0.601819\tTest-mlogloss:0.70025\n",
            "[21]\tTrain-mlogloss:0.591896\tTest-mlogloss:0.693877\n",
            "[22]\tTrain-mlogloss:0.582822\tTest-mlogloss:0.687054\n",
            "[23]\tTrain-mlogloss:0.575624\tTest-mlogloss:0.68202\n",
            "[24]\tTrain-mlogloss:0.568611\tTest-mlogloss:0.676903\n",
            "[25]\tTrain-mlogloss:0.560956\tTest-mlogloss:0.672475\n",
            "[26]\tTrain-mlogloss:0.554088\tTest-mlogloss:0.667793\n",
            "[27]\tTrain-mlogloss:0.547904\tTest-mlogloss:0.663638\n",
            "[28]\tTrain-mlogloss:0.541641\tTest-mlogloss:0.658902\n",
            "[29]\tTrain-mlogloss:0.535675\tTest-mlogloss:0.655096\n",
            "[30]\tTrain-mlogloss:0.530795\tTest-mlogloss:0.652849\n",
            "[31]\tTrain-mlogloss:0.525657\tTest-mlogloss:0.650244\n",
            "[32]\tTrain-mlogloss:0.520435\tTest-mlogloss:0.646778\n",
            "[33]\tTrain-mlogloss:0.515793\tTest-mlogloss:0.644549\n",
            "[34]\tTrain-mlogloss:0.50968\tTest-mlogloss:0.641225\n",
            "[35]\tTrain-mlogloss:0.504656\tTest-mlogloss:0.638343\n",
            "[36]\tTrain-mlogloss:0.500348\tTest-mlogloss:0.635636\n",
            "[37]\tTrain-mlogloss:0.495556\tTest-mlogloss:0.633538\n",
            "[38]\tTrain-mlogloss:0.491658\tTest-mlogloss:0.631743\n",
            "[39]\tTrain-mlogloss:0.487735\tTest-mlogloss:0.630148\n",
            "[40]\tTrain-mlogloss:0.483907\tTest-mlogloss:0.627747\n",
            "[41]\tTrain-mlogloss:0.479754\tTest-mlogloss:0.625861\n",
            "[42]\tTrain-mlogloss:0.475787\tTest-mlogloss:0.623914\n",
            "[43]\tTrain-mlogloss:0.471738\tTest-mlogloss:0.622608\n",
            "[44]\tTrain-mlogloss:0.46705\tTest-mlogloss:0.620163\n",
            "[45]\tTrain-mlogloss:0.464312\tTest-mlogloss:0.618301\n",
            "[46]\tTrain-mlogloss:0.460437\tTest-mlogloss:0.616594\n",
            "[47]\tTrain-mlogloss:0.456434\tTest-mlogloss:0.614142\n",
            "[48]\tTrain-mlogloss:0.452555\tTest-mlogloss:0.612737\n",
            "[49]\tTrain-mlogloss:0.449523\tTest-mlogloss:0.611262\n",
            "[50]\tTrain-mlogloss:0.444628\tTest-mlogloss:0.609409\n",
            "[51]\tTrain-mlogloss:0.44104\tTest-mlogloss:0.607933\n",
            "[52]\tTrain-mlogloss:0.438261\tTest-mlogloss:0.606956\n",
            "[53]\tTrain-mlogloss:0.435049\tTest-mlogloss:0.605639\n",
            "[54]\tTrain-mlogloss:0.432363\tTest-mlogloss:0.604634\n",
            "[55]\tTrain-mlogloss:0.429513\tTest-mlogloss:0.603909\n",
            "[56]\tTrain-mlogloss:0.427105\tTest-mlogloss:0.602862\n",
            "[57]\tTrain-mlogloss:0.423883\tTest-mlogloss:0.601501\n",
            "[58]\tTrain-mlogloss:0.420553\tTest-mlogloss:0.600233\n",
            "[59]\tTrain-mlogloss:0.417603\tTest-mlogloss:0.59941\n",
            "[60]\tTrain-mlogloss:0.41449\tTest-mlogloss:0.597688\n",
            "[61]\tTrain-mlogloss:0.412294\tTest-mlogloss:0.59735\n",
            "[62]\tTrain-mlogloss:0.410202\tTest-mlogloss:0.596849\n",
            "[63]\tTrain-mlogloss:0.407938\tTest-mlogloss:0.596015\n",
            "[64]\tTrain-mlogloss:0.406159\tTest-mlogloss:0.595348\n",
            "[65]\tTrain-mlogloss:0.403877\tTest-mlogloss:0.594496\n",
            "[66]\tTrain-mlogloss:0.400749\tTest-mlogloss:0.594078\n",
            "[67]\tTrain-mlogloss:0.398612\tTest-mlogloss:0.593003\n",
            "[68]\tTrain-mlogloss:0.395766\tTest-mlogloss:0.592097\n",
            "[69]\tTrain-mlogloss:0.394124\tTest-mlogloss:0.591765\n",
            "[70]\tTrain-mlogloss:0.391467\tTest-mlogloss:0.590984\n",
            "[71]\tTrain-mlogloss:0.388952\tTest-mlogloss:0.590674\n",
            "[72]\tTrain-mlogloss:0.3872\tTest-mlogloss:0.590495\n",
            "[73]\tTrain-mlogloss:0.38466\tTest-mlogloss:0.59008\n",
            "[74]\tTrain-mlogloss:0.381781\tTest-mlogloss:0.589137\n",
            "[75]\tTrain-mlogloss:0.380621\tTest-mlogloss:0.588894\n",
            "[76]\tTrain-mlogloss:0.378106\tTest-mlogloss:0.588442\n",
            "[77]\tTrain-mlogloss:0.376718\tTest-mlogloss:0.587814\n",
            "[78]\tTrain-mlogloss:0.374004\tTest-mlogloss:0.58723\n",
            "[79]\tTrain-mlogloss:0.37237\tTest-mlogloss:0.5867\n",
            "[80]\tTrain-mlogloss:0.371203\tTest-mlogloss:0.586503\n",
            "[81]\tTrain-mlogloss:0.369364\tTest-mlogloss:0.585592\n",
            "[82]\tTrain-mlogloss:0.367833\tTest-mlogloss:0.585087\n",
            "[83]\tTrain-mlogloss:0.366365\tTest-mlogloss:0.585039\n",
            "[84]\tTrain-mlogloss:0.364768\tTest-mlogloss:0.584507\n",
            "[85]\tTrain-mlogloss:0.363349\tTest-mlogloss:0.583987\n",
            "[86]\tTrain-mlogloss:0.361287\tTest-mlogloss:0.583954\n",
            "[87]\tTrain-mlogloss:0.35944\tTest-mlogloss:0.58351\n",
            "[88]\tTrain-mlogloss:0.357201\tTest-mlogloss:0.582446\n",
            "[89]\tTrain-mlogloss:0.355096\tTest-mlogloss:0.581936\n",
            "[90]\tTrain-mlogloss:0.353271\tTest-mlogloss:0.581801\n",
            "[91]\tTrain-mlogloss:0.351258\tTest-mlogloss:0.58121\n",
            "[92]\tTrain-mlogloss:0.349496\tTest-mlogloss:0.581053\n",
            "[93]\tTrain-mlogloss:0.347636\tTest-mlogloss:0.580267\n",
            "[94]\tTrain-mlogloss:0.346012\tTest-mlogloss:0.579876\n",
            "[95]\tTrain-mlogloss:0.343918\tTest-mlogloss:0.580023\n",
            "[96]\tTrain-mlogloss:0.342115\tTest-mlogloss:0.579292\n",
            "[97]\tTrain-mlogloss:0.34045\tTest-mlogloss:0.578837\n",
            "[98]\tTrain-mlogloss:0.338778\tTest-mlogloss:0.578955\n",
            "[99]\tTrain-mlogloss:0.336733\tTest-mlogloss:0.578604\n",
            "[100]\tTrain-mlogloss:0.334952\tTest-mlogloss:0.578277\n",
            "[101]\tTrain-mlogloss:0.333466\tTest-mlogloss:0.578223\n",
            "[102]\tTrain-mlogloss:0.332373\tTest-mlogloss:0.577862\n",
            "[103]\tTrain-mlogloss:0.331097\tTest-mlogloss:0.577295\n",
            "[104]\tTrain-mlogloss:0.329725\tTest-mlogloss:0.57725\n",
            "[105]\tTrain-mlogloss:0.327488\tTest-mlogloss:0.576958\n",
            "[106]\tTrain-mlogloss:0.325562\tTest-mlogloss:0.576395\n",
            "[107]\tTrain-mlogloss:0.323869\tTest-mlogloss:0.576639\n",
            "[108]\tTrain-mlogloss:0.321853\tTest-mlogloss:0.576071\n",
            "[109]\tTrain-mlogloss:0.320735\tTest-mlogloss:0.575557\n",
            "[110]\tTrain-mlogloss:0.319093\tTest-mlogloss:0.575005\n",
            "[111]\tTrain-mlogloss:0.317294\tTest-mlogloss:0.574815\n",
            "[112]\tTrain-mlogloss:0.315799\tTest-mlogloss:0.574696\n",
            "[113]\tTrain-mlogloss:0.314455\tTest-mlogloss:0.574457\n",
            "[114]\tTrain-mlogloss:0.313147\tTest-mlogloss:0.574541\n",
            "[115]\tTrain-mlogloss:0.311963\tTest-mlogloss:0.573791\n",
            "[116]\tTrain-mlogloss:0.310432\tTest-mlogloss:0.574032\n",
            "[117]\tTrain-mlogloss:0.308786\tTest-mlogloss:0.574047\n",
            "[118]\tTrain-mlogloss:0.307153\tTest-mlogloss:0.574108\n",
            "[119]\tTrain-mlogloss:0.305175\tTest-mlogloss:0.573733\n",
            "[120]\tTrain-mlogloss:0.304076\tTest-mlogloss:0.573861\n",
            "[121]\tTrain-mlogloss:0.302725\tTest-mlogloss:0.573634\n",
            "[122]\tTrain-mlogloss:0.301376\tTest-mlogloss:0.573342\n",
            "[123]\tTrain-mlogloss:0.299821\tTest-mlogloss:0.573267\n",
            "[124]\tTrain-mlogloss:0.298859\tTest-mlogloss:0.573242\n",
            "[125]\tTrain-mlogloss:0.297901\tTest-mlogloss:0.573169\n",
            "[126]\tTrain-mlogloss:0.296457\tTest-mlogloss:0.573399\n",
            "[127]\tTrain-mlogloss:0.295003\tTest-mlogloss:0.572951\n",
            "[128]\tTrain-mlogloss:0.293621\tTest-mlogloss:0.572794\n",
            "[129]\tTrain-mlogloss:0.292717\tTest-mlogloss:0.572757\n",
            "[130]\tTrain-mlogloss:0.291531\tTest-mlogloss:0.572679\n",
            "[131]\tTrain-mlogloss:0.290072\tTest-mlogloss:0.572962\n",
            "[132]\tTrain-mlogloss:0.288846\tTest-mlogloss:0.573112\n",
            "[133]\tTrain-mlogloss:0.287488\tTest-mlogloss:0.573049\n",
            "[134]\tTrain-mlogloss:0.286331\tTest-mlogloss:0.572925\n",
            "[135]\tTrain-mlogloss:0.284789\tTest-mlogloss:0.572663\n",
            "[136]\tTrain-mlogloss:0.283351\tTest-mlogloss:0.572564\n",
            "[137]\tTrain-mlogloss:0.28235\tTest-mlogloss:0.572687\n",
            "[138]\tTrain-mlogloss:0.281174\tTest-mlogloss:0.572883\n",
            "[139]\tTrain-mlogloss:0.280181\tTest-mlogloss:0.572713\n",
            "[140]\tTrain-mlogloss:0.279244\tTest-mlogloss:0.57252\n",
            "[141]\tTrain-mlogloss:0.277477\tTest-mlogloss:0.572626\n",
            "[142]\tTrain-mlogloss:0.276322\tTest-mlogloss:0.572618\n",
            "[143]\tTrain-mlogloss:0.275366\tTest-mlogloss:0.572364\n",
            "[144]\tTrain-mlogloss:0.274482\tTest-mlogloss:0.572083\n",
            "[145]\tTrain-mlogloss:0.272915\tTest-mlogloss:0.571786\n",
            "[146]\tTrain-mlogloss:0.271684\tTest-mlogloss:0.572126\n",
            "[147]\tTrain-mlogloss:0.270894\tTest-mlogloss:0.572211\n",
            "[148]\tTrain-mlogloss:0.269949\tTest-mlogloss:0.572004\n",
            "[149]\tTrain-mlogloss:0.269483\tTest-mlogloss:0.572057\n",
            "[150]\tTrain-mlogloss:0.26846\tTest-mlogloss:0.572006\n",
            "[151]\tTrain-mlogloss:0.267358\tTest-mlogloss:0.571807\n",
            "[152]\tTrain-mlogloss:0.266019\tTest-mlogloss:0.572288\n",
            "[153]\tTrain-mlogloss:0.264959\tTest-mlogloss:0.572106\n",
            "[154]\tTrain-mlogloss:0.263739\tTest-mlogloss:0.572115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[155]\tTrain-mlogloss:0.262497\tTest-mlogloss:0.572639\n",
            "[156]\tTrain-mlogloss:0.261655\tTest-mlogloss:0.57259\n",
            "[157]\tTrain-mlogloss:0.260527\tTest-mlogloss:0.572484\n",
            "[158]\tTrain-mlogloss:0.259656\tTest-mlogloss:0.572504\n",
            "[159]\tTrain-mlogloss:0.258464\tTest-mlogloss:0.572603\n",
            "[160]\tTrain-mlogloss:0.257512\tTest-mlogloss:0.572404\n",
            "[161]\tTrain-mlogloss:0.256937\tTest-mlogloss:0.57225\n",
            "[162]\tTrain-mlogloss:0.255976\tTest-mlogloss:0.572159\n",
            "[163]\tTrain-mlogloss:0.255101\tTest-mlogloss:0.572308\n",
            "[164]\tTrain-mlogloss:0.254084\tTest-mlogloss:0.572556\n",
            "[165]\tTrain-mlogloss:0.253238\tTest-mlogloss:0.572694\n",
            "Stopping. Best iteration:\n",
            "[145]\tTrain-mlogloss:0.272915\tTest-mlogloss:0.571786\n",
            "\n",
            "Training Class Probabilities for First 5 Instances:\n",
            " [[3.7886475e-05 1.4681351e-02 8.5946023e-01 5.1839426e-02 1.9558225e-08\n",
            "  7.0614115e-05 7.3880687e-02 1.9167897e-05 1.0578938e-05]\n",
            " [4.1077998e-05 7.7888596e-01 2.1253510e-01 8.1778914e-03 3.2592464e-08\n",
            "  2.2145954e-05 1.8717280e-04 1.3181753e-04 1.8763523e-05]\n",
            " [1.8530812e-04 4.8500535e-01 4.8824495e-01 9.5267855e-03 2.6087302e-03\n",
            "  2.0877097e-03 1.2356213e-03 8.8238623e-03 2.2817354e-03]\n",
            " [3.6005110e-02 9.4269356e-03 3.5882031e-03 2.7469696e-02 4.0011378e-06\n",
            "  7.6123047e-01 1.5595239e-01 3.5114628e-03 2.8117166e-03]\n",
            " [9.5551746e-04 8.3841149e-07 1.9758982e-06 1.6009691e-04 2.3289163e-08\n",
            "  9.9713123e-01 1.0131892e-04 9.5544138e-04 6.9349073e-04]]\n",
            "Validation Class Probabilities for First 5 Instances:\n",
            " [[1.77985895e-03 7.41764307e-01 1.07413838e-02 2.17852309e-01\n",
            "  2.03982738e-04 2.25873594e-03 1.88060955e-03 1.41073158e-03\n",
            "  2.21081153e-02]\n",
            " [2.37006443e-05 1.07245555e-03 1.17242875e-04 7.95550179e-04\n",
            "  9.97748911e-01 1.68896640e-05 8.75058904e-05 9.38692901e-05\n",
            "  4.38362877e-05]\n",
            " [5.96838305e-04 3.35491508e-01 4.36812878e-01 1.96874321e-01\n",
            "  1.35410737e-05 8.14135186e-04 2.61439420e-02 2.01648776e-03\n",
            "  1.23629917e-03]\n",
            " [7.41499025e-06 9.05267835e-01 8.50746483e-02 9.58387647e-03\n",
            "  5.04925708e-07 3.94307935e-05 7.02108809e-06 1.58768635e-05\n",
            "  3.37408574e-06]\n",
            " [9.47220325e-02 2.18752716e-02 6.55315351e-03 3.70580121e-03\n",
            "  2.33834289e-04 6.02648640e-03 1.70913839e-03 7.16043264e-03\n",
            "  8.58013809e-01]]\n",
            "Best Predictions for Train:\n",
            " [3 2 3 ... 2 6 6]\n",
            "Best Predictions for Validation:\n",
            " [2 5 3 ... 2 2 2]\n",
            "Precision Score of the Training Set=  0.8667363246265614\n",
            "Precision Score of the Validation Set=  0.7594834494752255\n",
            "Recall Score of the Training Set=  0.9231173793355626\n",
            "Recall Score of the Validation Set=  0.7992566494023049\n",
            "F1 Score of the Training Set=  0.889340160512366\n",
            "F1 Score of the Validation Set=  0.7753302884306561\n",
            "Accuracy Score the Training Set=  0.8923073815199386\n",
            "Accuracy Score of the Validation Set=  0.8012281835811248\n",
            "Logloss Score Training Set=  0.3086723900813987\n",
            "Logloss Score of the Validation Set=  0.5315601851905287\n",
            "Confusion Matrix of the Training Set: \n",
            "\n",
            "[[ 1504     0     2     0     0     2    10    12    13]\n",
            " [   25  9936  1976   713    12     4   176    17    39]\n",
            " [    7   680  5253   335     0     0   121     5     2]\n",
            " [    5    79    93  1962     0     0    14     0     0]\n",
            " [    1     0     0     0  2188     0     2     0     0]\n",
            " [  115    17    11    22     1 10895   107    76    64]\n",
            " [   21    23    31    17     2     5  2170     1     1]\n",
            " [  144    16    15     0     1    28    51  6458    58]\n",
            " [   91    14     3     7     0     7    14    23  3805]]\n",
            "Confusion Matrix of the Validation Set: \n",
            "\n",
            "[[ 267    6    2    3    1   10   16   29   52]\n",
            " [   9 2228  675  206    3   11   71    8   13]\n",
            " [   1  347 1041  124    0    3   76    5    4]\n",
            " [   0   66   65  385    0    8   12    0    2]\n",
            " [   1    4    0    0  541    1    1    0    0]\n",
            " [  35   11    5   10    0 2629   47   43   47]\n",
            " [  24   20   43   12    2   14  428   20    5]\n",
            " [  58    7    6    0    0   27   27 1541   27]\n",
            " [  66    5    1    3    2   19    9   30  856]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rGgaNXcybF4",
        "colab_type": "text"
      },
      "source": [
        "Fine Tuning:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4Vi89cgybF5",
        "colab_type": "code",
        "colab": {},
        "outputId": "604daddb-a84c-4ea5-fa2f-359536ae2866"
      },
      "source": [
        "gridsearch_params = [\n",
        "    (max_depth, min_child_weight)\n",
        "    for max_depth in range(4,7)\n",
        "    for min_child_weight in range(2,5)\n",
        "]\n",
        "params = {\n",
        "    'eta': 0.3,  \n",
        "    'n_estimators': 1000,\n",
        "    'silent': True,  # option for logging\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass tasks\n",
        "    'num_class': 9,  # number of classes to predic\n",
        "    'max_depth': 6,  # depth of the trees in the boosting process\n",
        "    'seed': 42,\n",
        "    'eval_metric': 'mlogloss', #cross-entropy is chosen as evaluation metric\n",
        "    'min_child_weight':1,\n",
        "    'gamma':0.0,\n",
        "    'subsample':1,\n",
        "    'colsample_bytree':1,\n",
        "    'nthread': 16,\n",
        "    #'scale_pos_weight':1,\n",
        "    'reg_alpha':0,\n",
        "    'max_delta_step':1\n",
        "    }\n",
        "# Define initial best params and mlogloss\n",
        "min_mlogloss = float(\"Inf\")\n",
        "best_params = None\n",
        "for max_depth, min_child_weight in gridsearch_params:\n",
        "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
        "                             max_depth,\n",
        "                             min_child_weight))\n",
        "\n",
        "    # Update our parameters\n",
        "    params['max_depth'] = max_depth\n",
        "    params['min_child_weight'] = min_child_weight\n",
        "\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=num_boost_round,\n",
        "        seed=42,\n",
        "        nfold=5,\n",
        "        metrics={'mlogloss'},\n",
        "        early_stopping_rounds=20,\n",
        "        fpreproc = fpreproc, #use custom fn to update weights\n",
        "        stratified =True,\n",
        "        verbose_eval =10\n",
        "    )\n",
        "\n",
        "    # Update best mlogloss\n",
        "    mean_mlogloss = cv_results['test-mlogloss-mean'].min()\n",
        "    boost_rounds = cv_results['test-mlogloss-mean'].argmin()\n",
        "    print(\"\\tmlogloss {} for {} rounds\".format(mean_mlogloss, boost_rounds))\n",
        "    if mean_mlogloss < min_mlogloss:\n",
        "        min_mlogloss = mean_mlogloss\n",
        "        best_params = (max_depth,min_child_weight)\n",
        "\n",
        "print(\"Best params: {}, {}, mlogloss: {}\".format(best_params[0], best_params[1], min_mlogloss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV with max_depth=4, min_child_weight=2\n",
            "[0]\ttrain-mlogloss:1.95491+0.000307546\ttest-mlogloss:1.95921+0.00151919\n",
            "[10]\ttrain-mlogloss:0.897871+0.00148306\ttest-mlogloss:0.936455+0.00491392\n",
            "[20]\ttrain-mlogloss:0.702651+0.00132786\ttest-mlogloss:0.76494+0.00587637\n",
            "[30]\ttrain-mlogloss:0.622092+0.00209129\ttest-mlogloss:0.703951+0.00573356\n",
            "[40]\ttrain-mlogloss:0.573516+0.00143937\ttest-mlogloss:0.67123+0.00613833\n",
            "[50]\ttrain-mlogloss:0.537283+0.00155839\ttest-mlogloss:0.650495+0.00555617\n",
            "[60]\ttrain-mlogloss:0.508781+0.00237427\ttest-mlogloss:0.63568+0.00552383\n",
            "[70]\ttrain-mlogloss:0.485797+0.00302174\ttest-mlogloss:0.624528+0.00495475\n",
            "[80]\ttrain-mlogloss:0.465561+0.00302657\ttest-mlogloss:0.616227+0.00468347\n",
            "[90]\ttrain-mlogloss:0.44715+0.00336764\ttest-mlogloss:0.609618+0.00513494\n",
            "[100]\ttrain-mlogloss:0.430386+0.0036509\ttest-mlogloss:0.60404+0.00513329\n",
            "[110]\ttrain-mlogloss:0.415281+0.00355746\ttest-mlogloss:0.600024+0.00515898\n",
            "[120]\ttrain-mlogloss:0.401367+0.00305566\ttest-mlogloss:0.596762+0.00561055\n",
            "[130]\ttrain-mlogloss:0.388256+0.00254693\ttest-mlogloss:0.593839+0.0062534\n",
            "[140]\ttrain-mlogloss:0.376349+0.00287766\ttest-mlogloss:0.591919+0.00559249\n",
            "[150]\ttrain-mlogloss:0.365179+0.00303379\ttest-mlogloss:0.590587+0.00537409\n",
            "[160]\ttrain-mlogloss:0.354919+0.0032839\ttest-mlogloss:0.589352+0.00518551\n",
            "[170]\ttrain-mlogloss:0.345053+0.00391329\ttest-mlogloss:0.588675+0.00510213\n",
            "[180]\ttrain-mlogloss:0.335274+0.0040227\ttest-mlogloss:0.588056+0.00556672\n",
            "[190]\ttrain-mlogloss:0.326689+0.00375278\ttest-mlogloss:0.588179+0.00480169\n",
            "[200]\ttrain-mlogloss:0.31817+0.00347508\ttest-mlogloss:0.588056+0.00443053\n",
            "[210]\ttrain-mlogloss:0.309358+0.00372515\ttest-mlogloss:0.588254+0.00423464\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tmlogloss 0.5877764 for 196 rounds\n",
            "CV with max_depth=4, min_child_weight=3\n",
            "[0]\ttrain-mlogloss:1.95491+0.000306631\ttest-mlogloss:1.95921+0.00152026\n",
            "[10]\ttrain-mlogloss:0.897842+0.00144456\ttest-mlogloss:0.936416+0.00486273\n",
            "[20]\ttrain-mlogloss:0.702495+0.00162139\ttest-mlogloss:0.764393+0.00581936\n",
            "[30]\ttrain-mlogloss:0.622761+0.002545\ttest-mlogloss:0.70397+0.0059185\n",
            "[40]\ttrain-mlogloss:0.573487+0.00267889\ttest-mlogloss:0.67093+0.006079\n",
            "[50]\ttrain-mlogloss:0.538059+0.00319189\ttest-mlogloss:0.64942+0.00504359\n",
            "[60]\ttrain-mlogloss:0.509486+0.00301056\ttest-mlogloss:0.634629+0.00474971\n",
            "[70]\ttrain-mlogloss:0.48588+0.00336424\ttest-mlogloss:0.623565+0.00541788\n",
            "[80]\ttrain-mlogloss:0.465341+0.00313233\ttest-mlogloss:0.614744+0.00562404\n",
            "[90]\ttrain-mlogloss:0.447459+0.00308978\ttest-mlogloss:0.608475+0.00554476\n",
            "[100]\ttrain-mlogloss:0.430433+0.00308212\ttest-mlogloss:0.603562+0.00615983\n",
            "[110]\ttrain-mlogloss:0.415267+0.0027493\ttest-mlogloss:0.599302+0.00589264\n",
            "[120]\ttrain-mlogloss:0.401675+0.00263769\ttest-mlogloss:0.59585+0.00545421\n",
            "[130]\ttrain-mlogloss:0.388617+0.00262002\ttest-mlogloss:0.593159+0.00572701\n",
            "[140]\ttrain-mlogloss:0.376959+0.00313513\ttest-mlogloss:0.591336+0.00540306\n",
            "[150]\ttrain-mlogloss:0.366437+0.00273785\ttest-mlogloss:0.589987+0.00554743\n",
            "[160]\ttrain-mlogloss:0.355171+0.00276305\ttest-mlogloss:0.58894+0.00576583\n",
            "[170]\ttrain-mlogloss:0.344636+0.00286846\ttest-mlogloss:0.587911+0.00515389\n",
            "[180]\ttrain-mlogloss:0.335288+0.0032261\ttest-mlogloss:0.587643+0.00537601\n",
            "[190]\ttrain-mlogloss:0.326202+0.00319558\ttest-mlogloss:0.58719+0.00482837\n",
            "[200]\ttrain-mlogloss:0.317342+0.00308315\ttest-mlogloss:0.587832+0.004576\n",
            "\tmlogloss 0.5871902 for 190 rounds\n",
            "CV with max_depth=4, min_child_weight=4\n",
            "[0]\ttrain-mlogloss:1.95491+0.000307163\ttest-mlogloss:1.9592+0.00153128\n",
            "[10]\ttrain-mlogloss:0.897796+0.00148827\ttest-mlogloss:0.936506+0.0047581\n",
            "[20]\ttrain-mlogloss:0.702775+0.00238697\ttest-mlogloss:0.764926+0.0041904\n",
            "[30]\ttrain-mlogloss:0.622223+0.00293172\ttest-mlogloss:0.704049+0.0039031\n",
            "[40]\ttrain-mlogloss:0.573046+0.00226766\ttest-mlogloss:0.671317+0.00443474\n",
            "[50]\ttrain-mlogloss:0.537591+0.00235325\ttest-mlogloss:0.650811+0.00414217\n",
            "[60]\ttrain-mlogloss:0.50925+0.00272498\ttest-mlogloss:0.63603+0.00473402\n",
            "[70]\ttrain-mlogloss:0.485948+0.00272577\ttest-mlogloss:0.624967+0.0039122\n",
            "[80]\ttrain-mlogloss:0.464949+0.00286752\ttest-mlogloss:0.61573+0.00400247\n",
            "[90]\ttrain-mlogloss:0.447141+0.00260394\ttest-mlogloss:0.60927+0.00340998\n",
            "[100]\ttrain-mlogloss:0.431098+0.00284094\ttest-mlogloss:0.603918+0.00386118\n",
            "[110]\ttrain-mlogloss:0.415732+0.00369025\ttest-mlogloss:0.599779+0.00348264\n",
            "[120]\ttrain-mlogloss:0.401811+0.00364574\ttest-mlogloss:0.596226+0.00396964\n",
            "[130]\ttrain-mlogloss:0.388973+0.00357332\ttest-mlogloss:0.593758+0.00386627\n",
            "[140]\ttrain-mlogloss:0.376972+0.00352394\ttest-mlogloss:0.591697+0.00398408\n",
            "[150]\ttrain-mlogloss:0.366185+0.00319572\ttest-mlogloss:0.59008+0.00427571\n",
            "[160]\ttrain-mlogloss:0.355384+0.0034918\ttest-mlogloss:0.588685+0.00469005\n",
            "[170]\ttrain-mlogloss:0.345102+0.00347715\ttest-mlogloss:0.58782+0.00428885\n",
            "[180]\ttrain-mlogloss:0.335848+0.00337238\ttest-mlogloss:0.58817+0.00458938\n",
            "[190]\ttrain-mlogloss:0.326225+0.00276281\ttest-mlogloss:0.588058+0.00414679\n",
            "\tmlogloss 0.5878082 for 175 rounds\n",
            "CV with max_depth=5, min_child_weight=2\n",
            "[0]\ttrain-mlogloss:1.941+0.000431197\ttest-mlogloss:1.94826+0.00189641\n",
            "[10]\ttrain-mlogloss:0.816427+0.00196264\ttest-mlogloss:0.878647+0.00613967\n",
            "[20]\ttrain-mlogloss:0.616772+0.00191\ttest-mlogloss:0.716996+0.005825\n",
            "[30]\ttrain-mlogloss:0.536284+0.0026797\ttest-mlogloss:0.663392+0.00583176\n",
            "[40]\ttrain-mlogloss:0.48585+0.00242913\ttest-mlogloss:0.636739+0.00587459\n",
            "[50]\ttrain-mlogloss:0.44888+0.00303834\ttest-mlogloss:0.620155+0.00527186\n",
            "[60]\ttrain-mlogloss:0.419502+0.0039057\ttest-mlogloss:0.608279+0.00541459\n",
            "[70]\ttrain-mlogloss:0.393695+0.00347835\ttest-mlogloss:0.600481+0.00598804\n",
            "[80]\ttrain-mlogloss:0.372498+0.00310815\ttest-mlogloss:0.595136+0.00585333\n",
            "[90]\ttrain-mlogloss:0.352023+0.00291258\ttest-mlogloss:0.591925+0.0060815\n",
            "[100]\ttrain-mlogloss:0.334142+0.00241715\ttest-mlogloss:0.589709+0.00661177\n",
            "[110]\ttrain-mlogloss:0.317489+0.00323215\ttest-mlogloss:0.588437+0.00630046\n",
            "[120]\ttrain-mlogloss:0.302924+0.0029154\ttest-mlogloss:0.587911+0.00674547\n",
            "[130]\ttrain-mlogloss:0.288699+0.00295288\ttest-mlogloss:0.587614+0.00752415\n",
            "[140]\ttrain-mlogloss:0.275977+0.0039374\ttest-mlogloss:0.588387+0.00704745\n",
            "\tmlogloss 0.587614 for 130 rounds\n",
            "CV with max_depth=5, min_child_weight=3\n",
            "[0]\ttrain-mlogloss:1.941+0.00043193\ttest-mlogloss:1.94826+0.00188358\n",
            "[10]\ttrain-mlogloss:0.816097+0.00172408\ttest-mlogloss:0.87836+0.00676489\n",
            "[20]\ttrain-mlogloss:0.615849+0.00204802\ttest-mlogloss:0.716518+0.00612211\n",
            "[30]\ttrain-mlogloss:0.536183+0.00163241\ttest-mlogloss:0.663653+0.00560005\n",
            "[40]\ttrain-mlogloss:0.484623+0.0020344\ttest-mlogloss:0.636421+0.00599157\n",
            "[50]\ttrain-mlogloss:0.448391+0.00280178\ttest-mlogloss:0.620053+0.00556146\n",
            "[60]\ttrain-mlogloss:0.419505+0.00279126\ttest-mlogloss:0.609406+0.00528281\n",
            "[70]\ttrain-mlogloss:0.395021+0.00244183\ttest-mlogloss:0.602274+0.00561854\n",
            "[80]\ttrain-mlogloss:0.370693+0.00281748\ttest-mlogloss:0.595725+0.00593381\n",
            "[90]\ttrain-mlogloss:0.350896+0.00303127\ttest-mlogloss:0.592403+0.00561023\n",
            "[100]\ttrain-mlogloss:0.332761+0.00251789\ttest-mlogloss:0.589942+0.0059176\n",
            "[110]\ttrain-mlogloss:0.316691+0.00298093\ttest-mlogloss:0.58916+0.00516458\n",
            "[120]\ttrain-mlogloss:0.301331+0.00312262\ttest-mlogloss:0.588427+0.00502819\n",
            "[130]\ttrain-mlogloss:0.288248+0.00322131\ttest-mlogloss:0.589093+0.00514524\n",
            "\tmlogloss 0.5883586 for 118 rounds\n",
            "CV with max_depth=5, min_child_weight=4\n",
            "[0]\ttrain-mlogloss:1.94101+0.000431428\ttest-mlogloss:1.94828+0.00189091\n",
            "[10]\ttrain-mlogloss:0.815957+0.00220266\ttest-mlogloss:0.878813+0.00606293\n",
            "[20]\ttrain-mlogloss:0.6162+0.00302908\ttest-mlogloss:0.7155+0.0053933\n",
            "[30]\ttrain-mlogloss:0.535799+0.00178368\ttest-mlogloss:0.663428+0.00599407\n",
            "[40]\ttrain-mlogloss:0.485082+0.00142107\ttest-mlogloss:0.63487+0.00619383\n",
            "[50]\ttrain-mlogloss:0.449232+0.00203971\ttest-mlogloss:0.618895+0.00617434\n",
            "[60]\ttrain-mlogloss:0.41888+0.00214714\ttest-mlogloss:0.607085+0.00583003\n",
            "[70]\ttrain-mlogloss:0.394087+0.00194211\ttest-mlogloss:0.599951+0.00586678\n",
            "[80]\ttrain-mlogloss:0.371726+0.00158942\ttest-mlogloss:0.594302+0.00549573\n",
            "[90]\ttrain-mlogloss:0.3528+0.00198099\ttest-mlogloss:0.591265+0.00529665\n",
            "[100]\ttrain-mlogloss:0.335354+0.00159484\ttest-mlogloss:0.589409+0.00553453\n",
            "[110]\ttrain-mlogloss:0.317512+0.00146277\ttest-mlogloss:0.588149+0.00592284\n",
            "[120]\ttrain-mlogloss:0.302302+0.00134948\ttest-mlogloss:0.587171+0.00571533\n",
            "[130]\ttrain-mlogloss:0.288044+0.00196018\ttest-mlogloss:0.587311+0.00638613\n",
            "[140]\ttrain-mlogloss:0.275366+0.00251478\ttest-mlogloss:0.587883+0.00539485\n",
            "\tmlogloss 0.5869902 for 124 rounds\n",
            "CV with max_depth=6, min_child_weight=2\n",
            "[0]\ttrain-mlogloss:1.92856+0.000612652\ttest-mlogloss:1.93897+0.00110873\n",
            "[10]\ttrain-mlogloss:0.739644+0.00130427\ttest-mlogloss:0.836935+0.0067411\n",
            "[20]\ttrain-mlogloss:0.535266+0.00193815\ttest-mlogloss:0.6843+0.00601006\n",
            "[30]\ttrain-mlogloss:0.454823+0.00154437\ttest-mlogloss:0.639158+0.00523717\n",
            "[40]\ttrain-mlogloss:0.403502+0.00215977\ttest-mlogloss:0.616555+0.00559796\n",
            "[50]\ttrain-mlogloss:0.365711+0.00349445\ttest-mlogloss:0.604888+0.00518747\n",
            "[60]\ttrain-mlogloss:0.332721+0.00345574\ttest-mlogloss:0.598146+0.00507855\n",
            "[70]\ttrain-mlogloss:0.30508+0.00351814\ttest-mlogloss:0.594833+0.00464861\n",
            "[80]\ttrain-mlogloss:0.282636+0.00345861\ttest-mlogloss:0.593196+0.00461932\n",
            "[90]\ttrain-mlogloss:0.261725+0.00201336\ttest-mlogloss:0.593152+0.0045717\n",
            "[100]\ttrain-mlogloss:0.242224+0.00229815\ttest-mlogloss:0.594434+0.00444548\n",
            "\tmlogloss 0.592935 for 88 rounds\n",
            "CV with max_depth=6, min_child_weight=3\n",
            "[0]\ttrain-mlogloss:1.92857+0.000616527\ttest-mlogloss:1.93899+0.00111563\n",
            "[10]\ttrain-mlogloss:0.739663+0.000816503\ttest-mlogloss:0.836872+0.00578344\n",
            "[20]\ttrain-mlogloss:0.53439+0.00200857\ttest-mlogloss:0.683323+0.00673875\n",
            "[30]\ttrain-mlogloss:0.454127+0.00193917\ttest-mlogloss:0.638352+0.00605605\n",
            "[40]\ttrain-mlogloss:0.403498+0.00298628\ttest-mlogloss:0.616418+0.00610141\n",
            "[50]\ttrain-mlogloss:0.364928+0.00334005\ttest-mlogloss:0.604138+0.00634163\n",
            "[60]\ttrain-mlogloss:0.333299+0.00367207\ttest-mlogloss:0.597642+0.0059956\n",
            "[70]\ttrain-mlogloss:0.306118+0.00301172\ttest-mlogloss:0.594127+0.00633494\n",
            "[80]\ttrain-mlogloss:0.282733+0.00333374\ttest-mlogloss:0.592231+0.00668498\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[90]\ttrain-mlogloss:0.261891+0.00266729\ttest-mlogloss:0.592319+0.00704685\n",
            "[100]\ttrain-mlogloss:0.242225+0.00215475\ttest-mlogloss:0.59301+0.00706845\n",
            "\tmlogloss 0.5921426000000001 for 82 rounds\n",
            "CV with max_depth=6, min_child_weight=4\n",
            "[0]\ttrain-mlogloss:1.92859+0.000617876\ttest-mlogloss:1.93897+0.00108723\n",
            "[10]\ttrain-mlogloss:0.739691+0.000871383\ttest-mlogloss:0.836731+0.00572381\n",
            "[20]\ttrain-mlogloss:0.535887+0.00146663\ttest-mlogloss:0.684543+0.00557907\n",
            "[30]\ttrain-mlogloss:0.454236+0.00330517\ttest-mlogloss:0.637901+0.00488516\n",
            "[40]\ttrain-mlogloss:0.402311+0.00229687\ttest-mlogloss:0.616324+0.00482277\n",
            "[50]\ttrain-mlogloss:0.364924+0.00367925\ttest-mlogloss:0.604264+0.00492304\n",
            "[60]\ttrain-mlogloss:0.332227+0.00433836\ttest-mlogloss:0.597968+0.00572364\n",
            "[70]\ttrain-mlogloss:0.305928+0.00336997\ttest-mlogloss:0.595004+0.00488698\n",
            "[80]\ttrain-mlogloss:0.281906+0.00350085\ttest-mlogloss:0.592906+0.00546481\n",
            "[90]\ttrain-mlogloss:0.261437+0.0038167\ttest-mlogloss:0.592855+0.00578552\n",
            "[100]\ttrain-mlogloss:0.242959+0.0033523\ttest-mlogloss:0.593761+0.00657152\n",
            "[110]\ttrain-mlogloss:0.225015+0.00307497\ttest-mlogloss:0.595878+0.0072207\n",
            "\tmlogloss 0.5927743999999999 for 91 rounds\n",
            "Best params: 5, 4, mlogloss: 0.5869902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kxtUCghybF_",
        "colab_type": "text"
      },
      "source": [
        "Update parameters to max_depth=5, min_child_weight=4.\n",
        "CV degil de normal train yapinca overfit oldu!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se5RON3zybGA",
        "colab_type": "code",
        "colab": {},
        "outputId": "9bd59d98-b6f9-4833-8620-1b639973722c"
      },
      "source": [
        "params = {\n",
        "    'eta': 0.3,  \n",
        "    'n_estimators': 1000,\n",
        "    'silent': True,  # option for logging\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass tasks\n",
        "    'num_class': 9,  # number of classes to predic\n",
        "    'max_depth': 5,  # depth of the trees in the boosting process\n",
        "    'seed': 42,\n",
        "    'eval_metric': 'mlogloss', #cross-entropy is chosen as evaluation metric\n",
        "    'min_child_weight':4,\n",
        "    'gamma':0.0,\n",
        "    'subsample':1,\n",
        "    'colsample_bytree':1,\n",
        "    'nthread': 16,\n",
        "    #'scale_pos_weight':1,\n",
        "    'reg_alpha':0\n",
        "    }\n",
        "num_round = 50\n",
        "\n",
        "#training the model\n",
        "#model = xgb.train(params, dtrainR, num_round) #change dtrainR w/ dtrain\n",
        "\n",
        "num_boost_round = 999\n",
        "model = xgb.train(\n",
        "    params,\n",
        "    dtrain,#change dtrainR w/ dtrain\n",
        "    num_boost_round=num_boost_round,\n",
        "    evals=[(dtrain, \"Train\"),(dval, \"Test\")],\n",
        "    early_stopping_rounds=20\n",
        ")\n",
        "\n",
        "probsTrain = model.predict(dtrain)\n",
        "probsVal = model.predict(dval)\n",
        "print(\"Training Class Probabilities for First 5 Instances:\\n\",probsTrain[:5])\n",
        "print(\"Validation Class Probabilities for First 5 Instances:\\n\",probsVal[:5])\n",
        "best_predsTrain = np.asarray([np.argmax(line) for line in probsTrain])\n",
        "best_predsVal = np.asarray([np.argmax(line) for line in probsVal])\n",
        "\n",
        "print(\"Best Predictions for Train:\\n\", best_predsTrain+1)\n",
        "print(\"Best Predictions for Validation:\\n\", best_predsVal+1)\n",
        "#print(min(best_preds+1),max(best_preds+1))\n",
        "\n",
        "print(\"Precision Score of the Training Set= \",precision_score(y_train, best_predsTrain, average='macro'))#change y_train(y_val) to y_trainR(y_valR)\n",
        "print(\"Precision Score of the Validation Set= \",precision_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Recall Score of the Training Set= \",recall_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"Recall Score of the Validation Set= \",recall_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"F1 Score of the Training Set= \",f1_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"F1 Score of the Validation Set= \",f1_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Accuracy Score the Training Set= \", accuracy_score(y_train, best_predsTrain))\n",
        "print(\"Accuracy Score of the Validation Set= \", accuracy_score(y_val, best_predsVal))\n",
        "scoreTrain = log_loss(y_train, probsTrain)\n",
        "scoreVal = log_loss(y_val, probsVal)\n",
        "print(\"Logloss Score Training Set= \", scoreTrain)\n",
        "print(\"Logloss Score of the Validation Set= \", scoreVal)\n",
        "\n",
        "print(\"Confusion Matrix of the Training Set: \\n\")\n",
        "print(confusion_matrix(y_train, best_predsTrain))\n",
        "print(\"Confusion Matrix of the Validation Set: \\n\")\n",
        "print(confusion_matrix(y_val,best_predsVal))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\tTrain-mlogloss:1.6347\tTest-mlogloss:1.64912\n",
            "Multiple eval metrics have been passed: 'Test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until Test-mlogloss hasn't improved in 20 rounds.\n",
            "[1]\tTrain-mlogloss:1.40499\tTest-mlogloss:1.42688\n",
            "[2]\tTrain-mlogloss:1.24857\tTest-mlogloss:1.27871\n",
            "[3]\tTrain-mlogloss:1.13596\tTest-mlogloss:1.16991\n",
            "[4]\tTrain-mlogloss:1.04602\tTest-mlogloss:1.08526\n",
            "[5]\tTrain-mlogloss:0.9749\tTest-mlogloss:1.01815\n",
            "[6]\tTrain-mlogloss:0.917054\tTest-mlogloss:0.965155\n",
            "[7]\tTrain-mlogloss:0.869046\tTest-mlogloss:0.921649\n",
            "[8]\tTrain-mlogloss:0.828674\tTest-mlogloss:0.886441\n",
            "[9]\tTrain-mlogloss:0.794013\tTest-mlogloss:0.857076\n",
            "[10]\tTrain-mlogloss:0.763669\tTest-mlogloss:0.82944\n",
            "[11]\tTrain-mlogloss:0.738946\tTest-mlogloss:0.809058\n",
            "[12]\tTrain-mlogloss:0.716818\tTest-mlogloss:0.790915\n",
            "[13]\tTrain-mlogloss:0.696225\tTest-mlogloss:0.774748\n",
            "[14]\tTrain-mlogloss:0.6789\tTest-mlogloss:0.760372\n",
            "[15]\tTrain-mlogloss:0.663725\tTest-mlogloss:0.747251\n",
            "[16]\tTrain-mlogloss:0.648591\tTest-mlogloss:0.735448\n",
            "[17]\tTrain-mlogloss:0.63537\tTest-mlogloss:0.725198\n",
            "[18]\tTrain-mlogloss:0.623361\tTest-mlogloss:0.716702\n",
            "[19]\tTrain-mlogloss:0.612291\tTest-mlogloss:0.708298\n",
            "[20]\tTrain-mlogloss:0.602711\tTest-mlogloss:0.70176\n",
            "[21]\tTrain-mlogloss:0.593217\tTest-mlogloss:0.694978\n",
            "[22]\tTrain-mlogloss:0.584923\tTest-mlogloss:0.688583\n",
            "[23]\tTrain-mlogloss:0.57597\tTest-mlogloss:0.682956\n",
            "[24]\tTrain-mlogloss:0.56846\tTest-mlogloss:0.678157\n",
            "[25]\tTrain-mlogloss:0.561191\tTest-mlogloss:0.67401\n",
            "[26]\tTrain-mlogloss:0.554842\tTest-mlogloss:0.670125\n",
            "[27]\tTrain-mlogloss:0.548505\tTest-mlogloss:0.66641\n",
            "[28]\tTrain-mlogloss:0.541896\tTest-mlogloss:0.662675\n",
            "[29]\tTrain-mlogloss:0.537118\tTest-mlogloss:0.659105\n",
            "[30]\tTrain-mlogloss:0.531194\tTest-mlogloss:0.654826\n",
            "[31]\tTrain-mlogloss:0.52453\tTest-mlogloss:0.650602\n",
            "[32]\tTrain-mlogloss:0.518606\tTest-mlogloss:0.647561\n",
            "[33]\tTrain-mlogloss:0.514076\tTest-mlogloss:0.64509\n",
            "[34]\tTrain-mlogloss:0.508896\tTest-mlogloss:0.642125\n",
            "[35]\tTrain-mlogloss:0.504084\tTest-mlogloss:0.6396\n",
            "[36]\tTrain-mlogloss:0.499984\tTest-mlogloss:0.636985\n",
            "[37]\tTrain-mlogloss:0.495329\tTest-mlogloss:0.634298\n",
            "[38]\tTrain-mlogloss:0.490509\tTest-mlogloss:0.632359\n",
            "[39]\tTrain-mlogloss:0.486627\tTest-mlogloss:0.630374\n",
            "[40]\tTrain-mlogloss:0.481839\tTest-mlogloss:0.628477\n",
            "[41]\tTrain-mlogloss:0.477922\tTest-mlogloss:0.626552\n",
            "[42]\tTrain-mlogloss:0.473429\tTest-mlogloss:0.623525\n",
            "[43]\tTrain-mlogloss:0.470104\tTest-mlogloss:0.621842\n",
            "[44]\tTrain-mlogloss:0.466544\tTest-mlogloss:0.619988\n",
            "[45]\tTrain-mlogloss:0.46282\tTest-mlogloss:0.618466\n",
            "[46]\tTrain-mlogloss:0.459661\tTest-mlogloss:0.617512\n",
            "[47]\tTrain-mlogloss:0.45637\tTest-mlogloss:0.61528\n",
            "[48]\tTrain-mlogloss:0.453122\tTest-mlogloss:0.613743\n",
            "[49]\tTrain-mlogloss:0.449345\tTest-mlogloss:0.611981\n",
            "[50]\tTrain-mlogloss:0.44653\tTest-mlogloss:0.610935\n",
            "[51]\tTrain-mlogloss:0.443462\tTest-mlogloss:0.610022\n",
            "[52]\tTrain-mlogloss:0.440009\tTest-mlogloss:0.608241\n",
            "[53]\tTrain-mlogloss:0.437427\tTest-mlogloss:0.607871\n",
            "[54]\tTrain-mlogloss:0.433968\tTest-mlogloss:0.606362\n",
            "[55]\tTrain-mlogloss:0.430693\tTest-mlogloss:0.605124\n",
            "[56]\tTrain-mlogloss:0.428142\tTest-mlogloss:0.603995\n",
            "[57]\tTrain-mlogloss:0.425707\tTest-mlogloss:0.603372\n",
            "[58]\tTrain-mlogloss:0.422563\tTest-mlogloss:0.602635\n",
            "[59]\tTrain-mlogloss:0.41908\tTest-mlogloss:0.601211\n",
            "[60]\tTrain-mlogloss:0.41564\tTest-mlogloss:0.600603\n",
            "[61]\tTrain-mlogloss:0.413785\tTest-mlogloss:0.600312\n",
            "[62]\tTrain-mlogloss:0.411429\tTest-mlogloss:0.598794\n",
            "[63]\tTrain-mlogloss:0.409362\tTest-mlogloss:0.597595\n",
            "[64]\tTrain-mlogloss:0.40659\tTest-mlogloss:0.596297\n",
            "[65]\tTrain-mlogloss:0.40429\tTest-mlogloss:0.59556\n",
            "[66]\tTrain-mlogloss:0.401893\tTest-mlogloss:0.594766\n",
            "[67]\tTrain-mlogloss:0.398868\tTest-mlogloss:0.594031\n",
            "[68]\tTrain-mlogloss:0.3965\tTest-mlogloss:0.592828\n",
            "[69]\tTrain-mlogloss:0.39498\tTest-mlogloss:0.592285\n",
            "[70]\tTrain-mlogloss:0.393414\tTest-mlogloss:0.591562\n",
            "[71]\tTrain-mlogloss:0.391418\tTest-mlogloss:0.591264\n",
            "[72]\tTrain-mlogloss:0.389931\tTest-mlogloss:0.590795\n",
            "[73]\tTrain-mlogloss:0.38789\tTest-mlogloss:0.59014\n",
            "[74]\tTrain-mlogloss:0.385747\tTest-mlogloss:0.588959\n",
            "[75]\tTrain-mlogloss:0.383257\tTest-mlogloss:0.588918\n",
            "[76]\tTrain-mlogloss:0.381403\tTest-mlogloss:0.588402\n",
            "[77]\tTrain-mlogloss:0.37953\tTest-mlogloss:0.588022\n",
            "[78]\tTrain-mlogloss:0.377335\tTest-mlogloss:0.58759\n",
            "[79]\tTrain-mlogloss:0.375273\tTest-mlogloss:0.587268\n",
            "[80]\tTrain-mlogloss:0.373482\tTest-mlogloss:0.587171\n",
            "[81]\tTrain-mlogloss:0.371662\tTest-mlogloss:0.586339\n",
            "[82]\tTrain-mlogloss:0.369723\tTest-mlogloss:0.585575\n",
            "[83]\tTrain-mlogloss:0.367954\tTest-mlogloss:0.585443\n",
            "[84]\tTrain-mlogloss:0.365992\tTest-mlogloss:0.584385\n",
            "[85]\tTrain-mlogloss:0.364125\tTest-mlogloss:0.583893\n",
            "[86]\tTrain-mlogloss:0.361967\tTest-mlogloss:0.583024\n",
            "[87]\tTrain-mlogloss:0.360089\tTest-mlogloss:0.582833\n",
            "[88]\tTrain-mlogloss:0.357475\tTest-mlogloss:0.582157\n",
            "[89]\tTrain-mlogloss:0.35542\tTest-mlogloss:0.582314\n",
            "[90]\tTrain-mlogloss:0.353848\tTest-mlogloss:0.581968\n",
            "[91]\tTrain-mlogloss:0.352394\tTest-mlogloss:0.582138\n",
            "[92]\tTrain-mlogloss:0.350524\tTest-mlogloss:0.58137\n",
            "[93]\tTrain-mlogloss:0.348932\tTest-mlogloss:0.581191\n",
            "[94]\tTrain-mlogloss:0.347724\tTest-mlogloss:0.580378\n",
            "[95]\tTrain-mlogloss:0.346142\tTest-mlogloss:0.580185\n",
            "[96]\tTrain-mlogloss:0.344457\tTest-mlogloss:0.579705\n",
            "[97]\tTrain-mlogloss:0.342804\tTest-mlogloss:0.579533\n",
            "[98]\tTrain-mlogloss:0.34176\tTest-mlogloss:0.579424\n",
            "[99]\tTrain-mlogloss:0.340829\tTest-mlogloss:0.579666\n",
            "[100]\tTrain-mlogloss:0.339044\tTest-mlogloss:0.580015\n",
            "[101]\tTrain-mlogloss:0.336846\tTest-mlogloss:0.579649\n",
            "[102]\tTrain-mlogloss:0.335222\tTest-mlogloss:0.579248\n",
            "[103]\tTrain-mlogloss:0.333685\tTest-mlogloss:0.578791\n",
            "[104]\tTrain-mlogloss:0.331617\tTest-mlogloss:0.578368\n",
            "[105]\tTrain-mlogloss:0.32915\tTest-mlogloss:0.578242\n",
            "[106]\tTrain-mlogloss:0.327627\tTest-mlogloss:0.577971\n",
            "[107]\tTrain-mlogloss:0.326513\tTest-mlogloss:0.577909\n",
            "[108]\tTrain-mlogloss:0.324735\tTest-mlogloss:0.57739\n",
            "[109]\tTrain-mlogloss:0.323318\tTest-mlogloss:0.577093\n",
            "[110]\tTrain-mlogloss:0.32241\tTest-mlogloss:0.577133\n",
            "[111]\tTrain-mlogloss:0.320779\tTest-mlogloss:0.57729\n",
            "[112]\tTrain-mlogloss:0.319159\tTest-mlogloss:0.576815\n",
            "[113]\tTrain-mlogloss:0.317715\tTest-mlogloss:0.576632\n",
            "[114]\tTrain-mlogloss:0.316118\tTest-mlogloss:0.576602\n",
            "[115]\tTrain-mlogloss:0.314918\tTest-mlogloss:0.576555\n",
            "[116]\tTrain-mlogloss:0.314117\tTest-mlogloss:0.576856\n",
            "[117]\tTrain-mlogloss:0.312753\tTest-mlogloss:0.576902\n",
            "[118]\tTrain-mlogloss:0.311547\tTest-mlogloss:0.5772\n",
            "[119]\tTrain-mlogloss:0.310345\tTest-mlogloss:0.577105\n",
            "[120]\tTrain-mlogloss:0.308995\tTest-mlogloss:0.576725\n",
            "[121]\tTrain-mlogloss:0.307563\tTest-mlogloss:0.576713\n",
            "[122]\tTrain-mlogloss:0.306449\tTest-mlogloss:0.576399\n",
            "[123]\tTrain-mlogloss:0.304706\tTest-mlogloss:0.576303\n",
            "[124]\tTrain-mlogloss:0.303299\tTest-mlogloss:0.576396\n",
            "[125]\tTrain-mlogloss:0.301415\tTest-mlogloss:0.576417\n",
            "[126]\tTrain-mlogloss:0.30008\tTest-mlogloss:0.576506\n",
            "[127]\tTrain-mlogloss:0.299374\tTest-mlogloss:0.576335\n",
            "[128]\tTrain-mlogloss:0.298265\tTest-mlogloss:0.576144\n",
            "[129]\tTrain-mlogloss:0.297454\tTest-mlogloss:0.575827\n",
            "[130]\tTrain-mlogloss:0.296332\tTest-mlogloss:0.575595\n",
            "[131]\tTrain-mlogloss:0.295175\tTest-mlogloss:0.575171\n",
            "[132]\tTrain-mlogloss:0.293695\tTest-mlogloss:0.574873\n",
            "[133]\tTrain-mlogloss:0.292038\tTest-mlogloss:0.574883\n",
            "[134]\tTrain-mlogloss:0.290548\tTest-mlogloss:0.574393\n",
            "[135]\tTrain-mlogloss:0.289326\tTest-mlogloss:0.574399\n",
            "[136]\tTrain-mlogloss:0.288179\tTest-mlogloss:0.574176\n",
            "[137]\tTrain-mlogloss:0.287118\tTest-mlogloss:0.574085\n",
            "[138]\tTrain-mlogloss:0.286274\tTest-mlogloss:0.574411\n",
            "[139]\tTrain-mlogloss:0.285068\tTest-mlogloss:0.574101\n",
            "[140]\tTrain-mlogloss:0.283983\tTest-mlogloss:0.574144\n",
            "[141]\tTrain-mlogloss:0.28236\tTest-mlogloss:0.574204\n",
            "[142]\tTrain-mlogloss:0.281375\tTest-mlogloss:0.574262\n",
            "[143]\tTrain-mlogloss:0.280048\tTest-mlogloss:0.574092\n",
            "[144]\tTrain-mlogloss:0.278584\tTest-mlogloss:0.574013\n",
            "[145]\tTrain-mlogloss:0.277077\tTest-mlogloss:0.574345\n",
            "[146]\tTrain-mlogloss:0.276339\tTest-mlogloss:0.574152\n",
            "[147]\tTrain-mlogloss:0.275056\tTest-mlogloss:0.57418\n",
            "[148]\tTrain-mlogloss:0.27402\tTest-mlogloss:0.573863\n",
            "[149]\tTrain-mlogloss:0.273049\tTest-mlogloss:0.574008\n",
            "[150]\tTrain-mlogloss:0.271905\tTest-mlogloss:0.574222\n",
            "[151]\tTrain-mlogloss:0.271075\tTest-mlogloss:0.574406\n",
            "[152]\tTrain-mlogloss:0.270196\tTest-mlogloss:0.574439\n",
            "[153]\tTrain-mlogloss:0.268872\tTest-mlogloss:0.574623\n",
            "[154]\tTrain-mlogloss:0.267937\tTest-mlogloss:0.574726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[155]\tTrain-mlogloss:0.266972\tTest-mlogloss:0.574777\n",
            "[156]\tTrain-mlogloss:0.266192\tTest-mlogloss:0.574656\n",
            "[157]\tTrain-mlogloss:0.264915\tTest-mlogloss:0.574818\n",
            "[158]\tTrain-mlogloss:0.263875\tTest-mlogloss:0.574554\n",
            "[159]\tTrain-mlogloss:0.262881\tTest-mlogloss:0.574659\n",
            "[160]\tTrain-mlogloss:0.261669\tTest-mlogloss:0.574532\n",
            "[161]\tTrain-mlogloss:0.260646\tTest-mlogloss:0.574741\n",
            "[162]\tTrain-mlogloss:0.259337\tTest-mlogloss:0.574732\n",
            "[163]\tTrain-mlogloss:0.258502\tTest-mlogloss:0.575197\n",
            "[164]\tTrain-mlogloss:0.257505\tTest-mlogloss:0.575425\n",
            "[165]\tTrain-mlogloss:0.256706\tTest-mlogloss:0.575279\n",
            "[166]\tTrain-mlogloss:0.255687\tTest-mlogloss:0.575483\n",
            "[167]\tTrain-mlogloss:0.254379\tTest-mlogloss:0.575036\n",
            "[168]\tTrain-mlogloss:0.253298\tTest-mlogloss:0.575035\n",
            "Stopping. Best iteration:\n",
            "[148]\tTrain-mlogloss:0.27402\tTest-mlogloss:0.573863\n",
            "\n",
            "Training Class Probabilities for First 5 Instances:\n",
            " [[4.28382882e-05 1.42005365e-02 8.68714452e-01 5.55121526e-02\n",
            "  8.67227534e-09 2.51045014e-04 6.12630360e-02 1.21489566e-05\n",
            "  3.83733277e-06]\n",
            " [2.89158597e-05 6.35809660e-01 3.60844374e-01 2.37396243e-03\n",
            "  3.99724946e-08 2.42062306e-05 7.82200543e-04 1.22039986e-04\n",
            "  1.46003340e-05]\n",
            " [2.42931827e-04 4.33948964e-01 5.36764503e-01 1.07833669e-02\n",
            "  3.55823291e-03 1.27841986e-03 1.02649687e-03 1.11016193e-02\n",
            "  1.29546539e-03]\n",
            " [3.05809621e-02 6.16453728e-03 2.98106787e-03 1.29144089e-02\n",
            "  2.58331988e-06 8.58887851e-01 7.94712454e-02 5.67393005e-03\n",
            "  3.32339900e-03]\n",
            " [2.27970839e-03 1.21739617e-06 1.43530269e-06 9.38566736e-05\n",
            "  5.89002873e-08 9.96395051e-01 1.51872388e-04 5.66370029e-04\n",
            "  5.10458136e-04]]\n",
            "Validation Class Probabilities for First 5 Instances:\n",
            " [[2.32111779e-03 7.58712947e-01 1.16359163e-02 2.00418860e-01\n",
            "  1.41869823e-04 1.76425965e-03 2.39051064e-03 1.81047991e-03\n",
            "  2.08040494e-02]\n",
            " [2.34086274e-05 7.62368145e-04 1.48352003e-04 1.75614783e-03\n",
            "  9.97107923e-01 1.14428640e-05 5.07107397e-05 1.00025070e-04\n",
            "  3.96052092e-05]\n",
            " [7.35440757e-04 2.27123216e-01 5.03830791e-01 2.52879083e-01\n",
            "  2.31691938e-05 3.11648357e-04 9.88071691e-03 3.67600285e-03\n",
            "  1.53998286e-03]\n",
            " [4.73677574e-06 9.17354822e-01 6.64820224e-02 1.60658434e-02\n",
            "  6.94396533e-07 5.20911417e-05 2.37456206e-05 1.43321295e-05\n",
            "  1.74234958e-06]\n",
            " [8.08202997e-02 2.19494551e-02 6.53649261e-03 4.12333990e-03\n",
            "  3.07248702e-04 7.64499931e-03 2.09044595e-03 9.00676474e-03\n",
            "  8.67520928e-01]]\n",
            "Best Predictions for Train:\n",
            " [3 2 3 ... 2 6 6]\n",
            "Best Predictions for Validation:\n",
            " [2 5 3 ... 2 2 2]\n",
            "Precision Score of the Training Set=  0.8665971293064322\n",
            "Precision Score of the Validation Set=  0.753891210223281\n",
            "Recall Score of the Training Set=  0.9226969510064775\n",
            "Recall Score of the Validation Set=  0.7914117365242528\n",
            "F1 Score of the Training Set=  0.8891426643148943\n",
            "F1 Score of the Validation Set=  0.7690599181786091\n",
            "Accuracy Score the Training Set=  0.8916407417882106\n",
            "Accuracy Score of the Validation Set=  0.7965416936005171\n",
            "Logloss Score Training Set=  0.30976197329551414\n",
            "Logloss Score of the Validation Set=  0.5324694778916462\n",
            "Confusion Matrix of the Training Set: \n",
            "\n",
            "[[ 1503     0     1     0     0     3    10    10    16]\n",
            " [   26  9913  1971   728    14    10   181    18    37]\n",
            " [    9   704  5253   321     0     0   110     5     1]\n",
            " [    2    84    93  1959     1     1    13     0     0]\n",
            " [    1     0     0     0  2188     0     2     0     0]\n",
            " [  119    21    12    21     1 10879   104    70    81]\n",
            " [   18    20    29    21     2     6  2171     3     1]\n",
            " [  134    21    15     0     1    28    48  6470    54]\n",
            " [   92    11     4    10     0    10    13    22  3802]]\n",
            "Confusion Matrix of the Validation Set: \n",
            "\n",
            "[[ 262    7    0    3    0   13   14   34   53]\n",
            " [   9 2223  683  210    5   11   61    8   14]\n",
            " [   1  369 1019  128    1    6   70    4    3]\n",
            " [   0   71   69  371    1    9   15    1    1]\n",
            " [   1    3    1    0  541    1    1    0    0]\n",
            " [  39   11    5    8    0 2625   44   44   51]\n",
            " [  24   21   46   13    3   16  420   19    6]\n",
            " [  63    8    7    0    0   23   27 1543   22]\n",
            " [  67    5    1    2    2   19   10   31  854]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXVco7CMybGK",
        "colab_type": "text"
      },
      "source": [
        "## Tune subsample and colsample rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0aUpY2RybGL",
        "colab_type": "code",
        "colab": {},
        "outputId": "5ea25d13-2930-4dd1-d558-3f840337a281"
      },
      "source": [
        "gridsearch_params = [\n",
        "    (subsample, colsample)\n",
        "    for subsample in [i/10. for i in range(7,11)]\n",
        "    for colsample in [i/10. for i in range(7,11)]\n",
        "]\n",
        "\n",
        "params = {\n",
        "    'eta': 0.3,  \n",
        "    'n_estimators': 1000,\n",
        "    'silent': True,  # option for logging\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass tasks\n",
        "    'num_class': 9,  # number of classes to predic\n",
        "    'max_depth': 5,  # depth of the trees in the boosting process\n",
        "    'seed': 42,\n",
        "    'eval_metric': 'mlogloss', #cross-entropy is chosen as evaluation metric\n",
        "    'min_child_weight':4,\n",
        "    'gamma':0.0,\n",
        "    'subsample':1,\n",
        "    'colsample_bytree':1,\n",
        "    'nthread': 16,\n",
        "    #'scale_pos_weight':1,\n",
        "    'reg_alpha':0,\n",
        "    'max_delta_step':1\n",
        "    }\n",
        "# Define initial best params and mlogloss\n",
        "min_mlogloss = float(\"Inf\")\n",
        "best_params = None\n",
        "# We start by the largest values and go down to the smallest\n",
        "for subsample, colsample in reversed(gridsearch_params):\n",
        "    print(\"CV with subsample={}, colsample={}\".format(\n",
        "                             subsample,\n",
        "                             colsample))\n",
        "\n",
        "    # We update our parameters\n",
        "    params['subsample'] = subsample\n",
        "    params['colsample_bytree'] = colsample\n",
        "\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=num_boost_round,\n",
        "        seed=42,\n",
        "        nfold=5,\n",
        "        metrics={'mlogloss'},\n",
        "        early_stopping_rounds=20,\n",
        "        fpreproc = fpreproc, #use custom fn to update weights\n",
        "        stratified =True,\n",
        "        verbose_eval =10\n",
        "    )\n",
        "\n",
        "    # Update best mlogloss\n",
        "    mean_mlogloss = cv_results['test-mlogloss-mean'].min()\n",
        "    boost_rounds = cv_results['test-mlogloss-mean'].argmin()\n",
        "    print(\"\\tmlogloss {} for {} rounds\".format(mean_mlogloss, boost_rounds))\n",
        "    if mean_mlogloss < min_mlogloss:\n",
        "        min_mlogloss = mean_mlogloss\n",
        "        best_params = (subsample,colsample)\n",
        "\n",
        "print(\"Best params: {}, {}, mlogloss: {}\".format(best_params[0], best_params[1], min_mlogloss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV with subsample=1.0, colsample=1.0\n",
            "[0]\ttrain-mlogloss:1.94101+0.000431428\ttest-mlogloss:1.94828+0.00189091\n",
            "[10]\ttrain-mlogloss:0.815957+0.00220266\ttest-mlogloss:0.878813+0.00606293\n",
            "[20]\ttrain-mlogloss:0.6162+0.00302908\ttest-mlogloss:0.7155+0.0053933\n",
            "[30]\ttrain-mlogloss:0.535799+0.00178368\ttest-mlogloss:0.663428+0.00599407\n",
            "[40]\ttrain-mlogloss:0.485082+0.00142107\ttest-mlogloss:0.63487+0.00619383\n",
            "[50]\ttrain-mlogloss:0.449232+0.00203971\ttest-mlogloss:0.618895+0.00617434\n",
            "[60]\ttrain-mlogloss:0.41888+0.00214714\ttest-mlogloss:0.607085+0.00583003\n",
            "[70]\ttrain-mlogloss:0.394087+0.00194211\ttest-mlogloss:0.599951+0.00586678\n",
            "[80]\ttrain-mlogloss:0.371726+0.00158942\ttest-mlogloss:0.594302+0.00549573\n",
            "[90]\ttrain-mlogloss:0.3528+0.00198099\ttest-mlogloss:0.591265+0.00529665\n",
            "[100]\ttrain-mlogloss:0.335354+0.00159484\ttest-mlogloss:0.589409+0.00553453\n",
            "[110]\ttrain-mlogloss:0.317512+0.00146277\ttest-mlogloss:0.588149+0.00592284\n",
            "[120]\ttrain-mlogloss:0.302302+0.00134948\ttest-mlogloss:0.587171+0.00571533\n",
            "[130]\ttrain-mlogloss:0.288044+0.00196018\ttest-mlogloss:0.587311+0.00638613\n",
            "[140]\ttrain-mlogloss:0.275366+0.00251478\ttest-mlogloss:0.587883+0.00539485\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tmlogloss 0.5869902 for 124 rounds\n",
            "CV with subsample=1.0, colsample=0.9\n",
            "[0]\ttrain-mlogloss:1.94368+0.00107265\ttest-mlogloss:1.95136+0.00203709\n",
            "[10]\ttrain-mlogloss:0.821493+0.00223931\ttest-mlogloss:0.883896+0.0057113\n",
            "[20]\ttrain-mlogloss:0.618356+0.00221113\ttest-mlogloss:0.716938+0.00600404\n",
            "[30]\ttrain-mlogloss:0.538224+0.00070589\ttest-mlogloss:0.664036+0.00717173\n",
            "[40]\ttrain-mlogloss:0.48895+0.000921683\ttest-mlogloss:0.6376+0.00630425\n",
            "[50]\ttrain-mlogloss:0.452222+0.000906889\ttest-mlogloss:0.620797+0.00655815\n",
            "[60]\ttrain-mlogloss:0.422306+0.00180121\ttest-mlogloss:0.609806+0.00614158\n",
            "[70]\ttrain-mlogloss:0.396533+0.00226778\ttest-mlogloss:0.601652+0.00687885\n",
            "[80]\ttrain-mlogloss:0.374801+0.00279141\ttest-mlogloss:0.596192+0.00703902\n",
            "[90]\ttrain-mlogloss:0.355264+0.00197972\ttest-mlogloss:0.592971+0.00739396\n",
            "[100]\ttrain-mlogloss:0.336664+0.00171227\ttest-mlogloss:0.59053+0.00765159\n",
            "[110]\ttrain-mlogloss:0.32037+0.00177281\ttest-mlogloss:0.588764+0.00757249\n",
            "[120]\ttrain-mlogloss:0.30483+0.00170324\ttest-mlogloss:0.588403+0.00679322\n",
            "[130]\ttrain-mlogloss:0.290614+0.00180394\ttest-mlogloss:0.588112+0.00627228\n",
            "[140]\ttrain-mlogloss:0.277096+0.0029803\ttest-mlogloss:0.588815+0.00705731\n",
            "[150]\ttrain-mlogloss:0.265058+0.00279757\ttest-mlogloss:0.589781+0.00621621\n",
            "\tmlogloss 0.588058 for 131 rounds\n",
            "CV with subsample=1.0, colsample=0.8\n",
            "[0]\ttrain-mlogloss:1.94623+0.00050792\ttest-mlogloss:1.95295+0.00208533\n",
            "[10]\ttrain-mlogloss:0.829263+0.00312277\ttest-mlogloss:0.891605+0.00885591\n",
            "[20]\ttrain-mlogloss:0.623144+0.00208703\ttest-mlogloss:0.719735+0.0067556\n",
            "[30]\ttrain-mlogloss:0.541091+0.0018008\ttest-mlogloss:0.665297+0.00528841\n",
            "[40]\ttrain-mlogloss:0.49337+0.00230289\ttest-mlogloss:0.63854+0.00460342\n",
            "[50]\ttrain-mlogloss:0.454978+0.00227773\ttest-mlogloss:0.621186+0.00448017\n",
            "[60]\ttrain-mlogloss:0.425442+0.00280314\ttest-mlogloss:0.610259+0.00514919\n",
            "[70]\ttrain-mlogloss:0.399918+0.00204424\ttest-mlogloss:0.602022+0.00515743\n",
            "[80]\ttrain-mlogloss:0.377694+0.00309721\ttest-mlogloss:0.59595+0.00547832\n",
            "[90]\ttrain-mlogloss:0.358756+0.0028545\ttest-mlogloss:0.592409+0.00561154\n",
            "[100]\ttrain-mlogloss:0.34061+0.00281382\ttest-mlogloss:0.589725+0.00569284\n",
            "[110]\ttrain-mlogloss:0.323545+0.00343055\ttest-mlogloss:0.587791+0.00561011\n",
            "[120]\ttrain-mlogloss:0.308687+0.00322088\ttest-mlogloss:0.58842+0.00551444\n",
            "[130]\ttrain-mlogloss:0.294469+0.00308441\ttest-mlogloss:0.58802+0.00460653\n",
            "\tmlogloss 0.5874828 for 113 rounds\n",
            "CV with subsample=1.0, colsample=0.7\n",
            "[0]\ttrain-mlogloss:1.95038+0.000263954\ttest-mlogloss:1.95694+0.00197632\n",
            "[10]\ttrain-mlogloss:0.842593+0.00541644\ttest-mlogloss:0.903311+0.0106057\n",
            "[20]\ttrain-mlogloss:0.629963+0.00199345\ttest-mlogloss:0.725096+0.00723365\n",
            "[30]\ttrain-mlogloss:0.546425+0.00269671\ttest-mlogloss:0.667738+0.00589898\n",
            "[40]\ttrain-mlogloss:0.494831+0.00261863\ttest-mlogloss:0.638699+0.00522423\n",
            "[50]\ttrain-mlogloss:0.457878+0.00156412\ttest-mlogloss:0.621909+0.0046391\n",
            "[60]\ttrain-mlogloss:0.427452+0.00229652\ttest-mlogloss:0.610058+0.0052191\n",
            "[70]\ttrain-mlogloss:0.402368+0.00240124\ttest-mlogloss:0.602875+0.00500197\n",
            "[80]\ttrain-mlogloss:0.380769+0.00325256\ttest-mlogloss:0.59743+0.00489215\n",
            "[90]\ttrain-mlogloss:0.361081+0.00263051\ttest-mlogloss:0.593483+0.00455108\n",
            "[100]\ttrain-mlogloss:0.342932+0.00199904\ttest-mlogloss:0.591382+0.00455766\n",
            "[110]\ttrain-mlogloss:0.326259+0.00216921\ttest-mlogloss:0.589303+0.00418257\n",
            "[120]\ttrain-mlogloss:0.311088+0.00260039\ttest-mlogloss:0.587453+0.00400741\n",
            "[130]\ttrain-mlogloss:0.296974+0.00314288\ttest-mlogloss:0.586596+0.00398345\n",
            "[140]\ttrain-mlogloss:0.284243+0.00282807\ttest-mlogloss:0.586958+0.00408318\n",
            "[150]\ttrain-mlogloss:0.272203+0.00317013\ttest-mlogloss:0.587961+0.00341422\n",
            "\tmlogloss 0.5862668 for 134 rounds\n",
            "CV with subsample=0.9, colsample=1.0\n",
            "[0]\ttrain-mlogloss:1.94185+0.000709863\ttest-mlogloss:1.94912+0.00193474\n",
            "[10]\ttrain-mlogloss:0.815348+0.0014915\ttest-mlogloss:0.879893+0.00536968\n",
            "[20]\ttrain-mlogloss:0.613798+0.00190697\ttest-mlogloss:0.714823+0.00601973\n",
            "[30]\ttrain-mlogloss:0.530577+0.0018781\ttest-mlogloss:0.660913+0.00662299\n",
            "[40]\ttrain-mlogloss:0.476935+0.00246034\ttest-mlogloss:0.631848+0.00616635\n",
            "[50]\ttrain-mlogloss:0.437151+0.00314597\ttest-mlogloss:0.61477+0.00618332\n",
            "[60]\ttrain-mlogloss:0.404889+0.00305695\ttest-mlogloss:0.602767+0.00712546\n",
            "[70]\ttrain-mlogloss:0.376645+0.00285929\ttest-mlogloss:0.594411+0.00738642\n",
            "[80]\ttrain-mlogloss:0.351724+0.00304438\ttest-mlogloss:0.589077+0.00704595\n",
            "[90]\ttrain-mlogloss:0.329625+0.00240203\ttest-mlogloss:0.585922+0.00735772\n",
            "[100]\ttrain-mlogloss:0.310566+0.00226891\ttest-mlogloss:0.583651+0.00752647\n",
            "[110]\ttrain-mlogloss:0.292989+0.00292732\ttest-mlogloss:0.582528+0.00835132\n",
            "[120]\ttrain-mlogloss:0.276553+0.00284045\ttest-mlogloss:0.582862+0.00848726\n",
            "\tmlogloss 0.5825278 for 110 rounds\n",
            "CV with subsample=0.9, colsample=0.9\n",
            "[0]\ttrain-mlogloss:1.94354+0.0013829\ttest-mlogloss:1.95108+0.00120787\n",
            "[10]\ttrain-mlogloss:0.821714+0.00366598\ttest-mlogloss:0.883731+0.00641934\n",
            "[20]\ttrain-mlogloss:0.616516+0.0022396\ttest-mlogloss:0.71423+0.00770981\n",
            "[30]\ttrain-mlogloss:0.533109+0.00158655\ttest-mlogloss:0.659071+0.00692158\n",
            "[40]\ttrain-mlogloss:0.47994+0.0020095\ttest-mlogloss:0.631607+0.00681981\n",
            "[50]\ttrain-mlogloss:0.440551+0.00233773\ttest-mlogloss:0.61462+0.00670532\n",
            "[60]\ttrain-mlogloss:0.408768+0.00240403\ttest-mlogloss:0.603329+0.00696457\n",
            "[70]\ttrain-mlogloss:0.381187+0.00140205\ttest-mlogloss:0.594328+0.00656643\n",
            "[80]\ttrain-mlogloss:0.356786+0.000912691\ttest-mlogloss:0.588342+0.00635042\n",
            "[90]\ttrain-mlogloss:0.335034+0.000567536\ttest-mlogloss:0.585888+0.00638399\n",
            "[100]\ttrain-mlogloss:0.315597+0.00111076\ttest-mlogloss:0.583829+0.00628283\n",
            "[110]\ttrain-mlogloss:0.297448+0.00180183\ttest-mlogloss:0.582565+0.00638818\n",
            "[120]\ttrain-mlogloss:0.280909+0.00169964\ttest-mlogloss:0.583005+0.00658697\n",
            "[130]\ttrain-mlogloss:0.266003+0.0015523\ttest-mlogloss:0.584148+0.00677506\n",
            "\tmlogloss 0.5823780000000001 for 113 rounds\n",
            "CV with subsample=0.9, colsample=0.8\n",
            "[0]\ttrain-mlogloss:1.94794+0.00262723\ttest-mlogloss:1.95538+0.00257747\n",
            "[10]\ttrain-mlogloss:0.82936+0.00277919\ttest-mlogloss:0.891218+0.00443345\n",
            "[20]\ttrain-mlogloss:0.622461+0.00259627\ttest-mlogloss:0.71903+0.00462436\n",
            "[30]\ttrain-mlogloss:0.538042+0.00283904\ttest-mlogloss:0.662538+0.00448166\n",
            "[40]\ttrain-mlogloss:0.484913+0.00265099\ttest-mlogloss:0.634081+0.00534012\n",
            "[50]\ttrain-mlogloss:0.444755+0.00288571\ttest-mlogloss:0.616957+0.00497551\n",
            "[60]\ttrain-mlogloss:0.412848+0.00254578\ttest-mlogloss:0.605122+0.00534171\n",
            "[70]\ttrain-mlogloss:0.385459+0.00258819\ttest-mlogloss:0.597331+0.00527584\n",
            "[80]\ttrain-mlogloss:0.361326+0.00254351\ttest-mlogloss:0.591653+0.00529335\n",
            "[90]\ttrain-mlogloss:0.338849+0.00182913\ttest-mlogloss:0.587857+0.00603677\n",
            "[100]\ttrain-mlogloss:0.319711+0.00204266\ttest-mlogloss:0.584668+0.00593615\n",
            "[110]\ttrain-mlogloss:0.302528+0.00263955\ttest-mlogloss:0.583093+0.00624481\n",
            "[120]\ttrain-mlogloss:0.285986+0.00279766\ttest-mlogloss:0.582487+0.00635661\n",
            "[130]\ttrain-mlogloss:0.270481+0.00252148\ttest-mlogloss:0.582985+0.00621439\n",
            "\tmlogloss 0.5822644 for 119 rounds\n",
            "CV with subsample=0.9, colsample=0.7\n",
            "[0]\ttrain-mlogloss:1.95367+0.00103657\ttest-mlogloss:1.96123+0.00157724\n",
            "[10]\ttrain-mlogloss:0.843047+0.00440015\ttest-mlogloss:0.902777+0.00588072\n",
            "[20]\ttrain-mlogloss:0.629965+0.00225109\ttest-mlogloss:0.724655+0.00571231\n",
            "[30]\ttrain-mlogloss:0.543661+0.00102171\ttest-mlogloss:0.665404+0.00562006\n",
            "[40]\ttrain-mlogloss:0.489901+0.000414117\ttest-mlogloss:0.636223+0.00583965\n",
            "[50]\ttrain-mlogloss:0.450355+0.0010062\ttest-mlogloss:0.618581+0.00509949\n",
            "[60]\ttrain-mlogloss:0.417939+0.000995926\ttest-mlogloss:0.606254+0.00591198\n",
            "[70]\ttrain-mlogloss:0.390838+0.000997781\ttest-mlogloss:0.597734+0.00516843\n",
            "[80]\ttrain-mlogloss:0.366281+0.00184769\ttest-mlogloss:0.591726+0.00473939\n",
            "[90]\ttrain-mlogloss:0.34419+0.00096222\ttest-mlogloss:0.587906+0.00571683\n",
            "[100]\ttrain-mlogloss:0.32474+0.0010166\ttest-mlogloss:0.585573+0.00548821\n",
            "[110]\ttrain-mlogloss:0.3069+0.00140045\ttest-mlogloss:0.584218+0.00542137\n",
            "[120]\ttrain-mlogloss:0.290603+0.00101801\ttest-mlogloss:0.58386+0.00560339\n",
            "[130]\ttrain-mlogloss:0.275498+0.00141855\ttest-mlogloss:0.585188+0.00524238\n",
            "[140]\ttrain-mlogloss:0.261213+0.00180827\ttest-mlogloss:0.586566+0.00459166\n",
            "\tmlogloss 0.5838363999999999 for 121 rounds\n",
            "CV with subsample=0.8, colsample=1.0\n",
            "[0]\ttrain-mlogloss:1.94187+0.000412579\ttest-mlogloss:1.94844+0.00201629\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[10]\ttrain-mlogloss:0.816055+0.00119059\ttest-mlogloss:0.879381+0.00697944\n",
            "[20]\ttrain-mlogloss:0.61531+0.00248174\ttest-mlogloss:0.715804+0.00650357\n",
            "[30]\ttrain-mlogloss:0.530182+0.0024524\ttest-mlogloss:0.661702+0.00490166\n",
            "[40]\ttrain-mlogloss:0.476556+0.00266484\ttest-mlogloss:0.631207+0.00520485\n",
            "[50]\ttrain-mlogloss:0.434525+0.00298266\ttest-mlogloss:0.614466+0.00552877\n",
            "[60]\ttrain-mlogloss:0.401205+0.00255689\ttest-mlogloss:0.601826+0.00536996\n",
            "[70]\ttrain-mlogloss:0.372861+0.00206132\ttest-mlogloss:0.593153+0.00465531\n",
            "[80]\ttrain-mlogloss:0.348856+0.00262021\ttest-mlogloss:0.588755+0.00466944\n",
            "[90]\ttrain-mlogloss:0.325545+0.00301558\ttest-mlogloss:0.586294+0.00583265\n",
            "[100]\ttrain-mlogloss:0.305807+0.00269538\ttest-mlogloss:0.585603+0.00582941\n",
            "[110]\ttrain-mlogloss:0.286752+0.00253771\ttest-mlogloss:0.585099+0.00546355\n",
            "[120]\ttrain-mlogloss:0.26974+0.00261856\ttest-mlogloss:0.585901+0.00516133\n",
            "[130]\ttrain-mlogloss:0.253727+0.0034679\ttest-mlogloss:0.586542+0.00479851\n",
            "\tmlogloss 0.5847222000000001 for 112 rounds\n",
            "CV with subsample=0.8, colsample=0.9\n",
            "[0]\ttrain-mlogloss:1.94351+0.00163557\ttest-mlogloss:1.9506+0.0020427\n",
            "[10]\ttrain-mlogloss:0.822281+0.00385476\ttest-mlogloss:0.884012+0.00675592\n",
            "[20]\ttrain-mlogloss:0.617704+0.00346651\ttest-mlogloss:0.716332+0.00699506\n",
            "[30]\ttrain-mlogloss:0.533366+0.00297673\ttest-mlogloss:0.659875+0.00632346\n",
            "[40]\ttrain-mlogloss:0.479394+0.0030765\ttest-mlogloss:0.63102+0.00637058\n",
            "[50]\ttrain-mlogloss:0.43815+0.0033148\ttest-mlogloss:0.614855+0.0050282\n",
            "[60]\ttrain-mlogloss:0.405126+0.00311168\ttest-mlogloss:0.602671+0.0043618\n",
            "[70]\ttrain-mlogloss:0.376542+0.00280638\ttest-mlogloss:0.595311+0.00486171\n",
            "[80]\ttrain-mlogloss:0.352348+0.00263947\ttest-mlogloss:0.590409+0.00501941\n",
            "[90]\ttrain-mlogloss:0.329829+0.00334103\ttest-mlogloss:0.587355+0.00496355\n",
            "[100]\ttrain-mlogloss:0.309558+0.00360782\ttest-mlogloss:0.584903+0.00501057\n",
            "[110]\ttrain-mlogloss:0.290137+0.00390628\ttest-mlogloss:0.584654+0.00570961\n",
            "[120]\ttrain-mlogloss:0.273572+0.00398108\ttest-mlogloss:0.584513+0.00569962\n",
            "[130]\ttrain-mlogloss:0.257761+0.00426405\ttest-mlogloss:0.585504+0.00530688\n",
            "\tmlogloss 0.5843638 for 111 rounds\n",
            "CV with subsample=0.8, colsample=0.8\n",
            "[0]\ttrain-mlogloss:1.94832+0.0035276\ttest-mlogloss:1.95544+0.0039809\n",
            "[10]\ttrain-mlogloss:0.831778+0.00213143\ttest-mlogloss:0.892424+0.00606665\n",
            "[20]\ttrain-mlogloss:0.62476+0.00212296\ttest-mlogloss:0.719965+0.0065507\n",
            "[30]\ttrain-mlogloss:0.538904+0.00131905\ttest-mlogloss:0.663303+0.00599668\n",
            "[40]\ttrain-mlogloss:0.48435+0.00206118\ttest-mlogloss:0.633917+0.00592949\n",
            "[50]\ttrain-mlogloss:0.443202+0.00184893\ttest-mlogloss:0.616056+0.00548323\n",
            "[60]\ttrain-mlogloss:0.409684+0.00169875\ttest-mlogloss:0.602615+0.00558452\n",
            "[70]\ttrain-mlogloss:0.382018+0.00208483\ttest-mlogloss:0.5946+0.00636426\n",
            "[80]\ttrain-mlogloss:0.356081+0.00165009\ttest-mlogloss:0.588461+0.00621588\n",
            "[90]\ttrain-mlogloss:0.33344+0.00147496\ttest-mlogloss:0.585723+0.0069083\n",
            "[100]\ttrain-mlogloss:0.313604+0.00213328\ttest-mlogloss:0.583869+0.00676749\n",
            "[110]\ttrain-mlogloss:0.294163+0.00215576\ttest-mlogloss:0.582764+0.0065083\n",
            "[120]\ttrain-mlogloss:0.27856+0.00250192\ttest-mlogloss:0.583671+0.0075891\n",
            "\tmlogloss 0.582656 for 109 rounds\n",
            "CV with subsample=0.8, colsample=0.7\n",
            "[0]\ttrain-mlogloss:1.95393+0.00198209\ttest-mlogloss:1.96108+0.00241013\n",
            "[10]\ttrain-mlogloss:0.843303+0.0058016\ttest-mlogloss:0.902656+0.00680975\n",
            "[20]\ttrain-mlogloss:0.630697+0.00183179\ttest-mlogloss:0.723216+0.0066001\n",
            "[30]\ttrain-mlogloss:0.543374+0.000902251\ttest-mlogloss:0.664102+0.00586481\n",
            "[40]\ttrain-mlogloss:0.488097+0.00156274\ttest-mlogloss:0.634159+0.00556188\n",
            "[50]\ttrain-mlogloss:0.447447+0.000946798\ttest-mlogloss:0.615665+0.00554937\n",
            "[60]\ttrain-mlogloss:0.413401+0.00118202\ttest-mlogloss:0.603856+0.00535085\n",
            "[70]\ttrain-mlogloss:0.386161+0.00103619\ttest-mlogloss:0.595496+0.00502793\n",
            "[80]\ttrain-mlogloss:0.361228+0.00135911\ttest-mlogloss:0.590128+0.00558937\n",
            "[90]\ttrain-mlogloss:0.339002+0.00162064\ttest-mlogloss:0.587059+0.0064247\n",
            "[100]\ttrain-mlogloss:0.318963+0.0012173\ttest-mlogloss:0.584362+0.00697379\n",
            "[110]\ttrain-mlogloss:0.300029+0.00218266\ttest-mlogloss:0.582784+0.00701884\n",
            "[120]\ttrain-mlogloss:0.283439+0.00228118\ttest-mlogloss:0.583819+0.00768077\n",
            "\tmlogloss 0.5827840000000001 for 110 rounds\n",
            "CV with subsample=0.7, colsample=1.0\n",
            "[0]\ttrain-mlogloss:1.94334+0.000978053\ttest-mlogloss:1.94972+0.00211478\n",
            "[10]\ttrain-mlogloss:0.816539+0.00145278\ttest-mlogloss:0.879609+0.00545851\n",
            "[20]\ttrain-mlogloss:0.614327+0.00182618\ttest-mlogloss:0.713644+0.00709257\n",
            "[30]\ttrain-mlogloss:0.529481+0.00183962\ttest-mlogloss:0.658024+0.00684303\n",
            "[40]\ttrain-mlogloss:0.476235+0.00188469\ttest-mlogloss:0.631106+0.00541774\n",
            "[50]\ttrain-mlogloss:0.434029+0.00304416\ttest-mlogloss:0.612744+0.00611633\n",
            "[60]\ttrain-mlogloss:0.399995+0.00212881\ttest-mlogloss:0.602237+0.00660552\n",
            "[70]\ttrain-mlogloss:0.370328+0.00205495\ttest-mlogloss:0.592978+0.00613118\n",
            "[80]\ttrain-mlogloss:0.345244+0.00165407\ttest-mlogloss:0.588919+0.0052937\n",
            "[90]\ttrain-mlogloss:0.322118+0.00236072\ttest-mlogloss:0.586617+0.0049744\n",
            "[100]\ttrain-mlogloss:0.301565+0.00174763\ttest-mlogloss:0.585416+0.00529664\n",
            "[110]\ttrain-mlogloss:0.282763+0.002373\ttest-mlogloss:0.585156+0.00508074\n",
            "\tmlogloss 0.584848 for 97 rounds\n",
            "CV with subsample=0.7, colsample=0.9\n",
            "[0]\ttrain-mlogloss:1.94525+0.00111704\ttest-mlogloss:1.95218+0.00114309\n",
            "[10]\ttrain-mlogloss:0.82345+0.00496065\ttest-mlogloss:0.885939+0.00681162\n",
            "[20]\ttrain-mlogloss:0.618639+0.00228725\ttest-mlogloss:0.715605+0.00788864\n",
            "[30]\ttrain-mlogloss:0.533673+0.00228979\ttest-mlogloss:0.660348+0.00556573\n",
            "[40]\ttrain-mlogloss:0.478543+0.00299305\ttest-mlogloss:0.630859+0.00623734\n",
            "[50]\ttrain-mlogloss:0.437807+0.00380787\ttest-mlogloss:0.613742+0.00584293\n",
            "[60]\ttrain-mlogloss:0.403576+0.00359421\ttest-mlogloss:0.60251+0.00624378\n",
            "[70]\ttrain-mlogloss:0.374882+0.00323761\ttest-mlogloss:0.594648+0.00626785\n",
            "[80]\ttrain-mlogloss:0.349938+0.00255909\ttest-mlogloss:0.589743+0.00649816\n",
            "[90]\ttrain-mlogloss:0.327628+0.00245757\ttest-mlogloss:0.586773+0.00668551\n",
            "[100]\ttrain-mlogloss:0.307472+0.00181658\ttest-mlogloss:0.585203+0.00710655\n",
            "[110]\ttrain-mlogloss:0.288356+0.00155114\ttest-mlogloss:0.584947+0.00572161\n",
            "[120]\ttrain-mlogloss:0.27115+0.00138272\ttest-mlogloss:0.585111+0.00562898\n",
            "\tmlogloss 0.5846587999999999 for 109 rounds\n",
            "CV with subsample=0.7, colsample=0.8\n",
            "[0]\ttrain-mlogloss:1.94956+0.00230811\ttest-mlogloss:1.95614+0.00223846\n",
            "[10]\ttrain-mlogloss:0.832763+0.00405775\ttest-mlogloss:0.893791+0.0064134\n",
            "[20]\ttrain-mlogloss:0.6257+0.00206452\ttest-mlogloss:0.721281+0.00732903\n",
            "[30]\ttrain-mlogloss:0.538854+0.00120198\ttest-mlogloss:0.663978+0.00746618\n",
            "[40]\ttrain-mlogloss:0.483652+0.000790012\ttest-mlogloss:0.63441+0.00740973\n",
            "[50]\ttrain-mlogloss:0.442325+0.00139696\ttest-mlogloss:0.617447+0.00640648\n",
            "[60]\ttrain-mlogloss:0.408514+0.00187166\ttest-mlogloss:0.605856+0.00621281\n",
            "[70]\ttrain-mlogloss:0.379539+0.00183034\ttest-mlogloss:0.596745+0.00632917\n",
            "[80]\ttrain-mlogloss:0.354071+0.00112527\ttest-mlogloss:0.591683+0.00614098\n",
            "[90]\ttrain-mlogloss:0.331368+0.00192669\ttest-mlogloss:0.588456+0.00582107\n",
            "[100]\ttrain-mlogloss:0.31098+0.00173186\ttest-mlogloss:0.587254+0.00585252\n",
            "[110]\ttrain-mlogloss:0.292497+0.00231123\ttest-mlogloss:0.586463+0.00632224\n",
            "[120]\ttrain-mlogloss:0.275692+0.00242399\ttest-mlogloss:0.586997+0.00619212\n",
            "[130]\ttrain-mlogloss:0.259457+0.00230987\ttest-mlogloss:0.588686+0.00595787\n",
            "\tmlogloss 0.5863204000000001 for 114 rounds\n",
            "CV with subsample=0.7, colsample=0.7\n",
            "[0]\ttrain-mlogloss:1.95485+0.00167063\ttest-mlogloss:1.96175+0.00198164\n",
            "[10]\ttrain-mlogloss:0.846792+0.0043439\ttest-mlogloss:0.904622+0.00587549\n",
            "[20]\ttrain-mlogloss:0.631739+0.00096911\ttest-mlogloss:0.723181+0.00540063\n",
            "[30]\ttrain-mlogloss:0.543724+0.0012734\ttest-mlogloss:0.664755+0.00471332\n",
            "[40]\ttrain-mlogloss:0.488774+0.00169691\ttest-mlogloss:0.634839+0.00419803\n",
            "[50]\ttrain-mlogloss:0.448308+0.0022245\ttest-mlogloss:0.617429+0.00401209\n",
            "[60]\ttrain-mlogloss:0.414791+0.00265246\ttest-mlogloss:0.606156+0.00423092\n",
            "[70]\ttrain-mlogloss:0.386091+0.00204426\ttest-mlogloss:0.598536+0.00493353\n",
            "[80]\ttrain-mlogloss:0.361281+0.0026252\ttest-mlogloss:0.592995+0.00592717\n",
            "[90]\ttrain-mlogloss:0.338365+0.00316419\ttest-mlogloss:0.589787+0.00635828\n",
            "[100]\ttrain-mlogloss:0.317688+0.00306474\ttest-mlogloss:0.587687+0.00578538\n",
            "[110]\ttrain-mlogloss:0.298985+0.00284314\ttest-mlogloss:0.586873+0.00514713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[120]\ttrain-mlogloss:0.281764+0.00271477\ttest-mlogloss:0.586269+0.00527078\n",
            "[130]\ttrain-mlogloss:0.265248+0.00276361\ttest-mlogloss:0.58713+0.00509235\n",
            "\tmlogloss 0.5859716 for 119 rounds\n",
            "Best params: 0.9, 0.8, mlogloss: 0.5822644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nyipm6gLybGO",
        "colab_type": "text"
      },
      "source": [
        "Update parameters subsample = 0.9 an colsample =0.8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0hsbulPybGP",
        "colab_type": "code",
        "colab": {},
        "outputId": "5572202d-1353-45f9-f193-29509f1068f0"
      },
      "source": [
        "params = {\n",
        "    'eta': 0.3,  \n",
        "    'n_estimators': 1000,\n",
        "    'silent': True,  # option for logging\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass tasks\n",
        "    'num_class': 9,  # number of classes to predic\n",
        "    'max_depth': 5,  # depth of the trees in the boosting process\n",
        "    'seed': 42,\n",
        "    'eval_metric': 'mlogloss', #cross-entropy is chosen as evaluation metric\n",
        "    'min_child_weight':4,\n",
        "    'gamma':0.0,\n",
        "    'subsample':0.9,\n",
        "    'colsample_bytree':0.8,\n",
        "    'nthread': 16,\n",
        "    #'scale_pos_weight':1,\n",
        "    'reg_alpha':0,\n",
        "    'max_delta_step':1\n",
        "    \n",
        "    }\n",
        "num_round = 50\n",
        "\n",
        "#training the model\n",
        "#model = xgb.train(params, dtrainR, num_round) #change dtrainR w/ dtrain\n",
        "\n",
        "num_boost_round = 999\n",
        "model = xgb.train(\n",
        "    params,\n",
        "    dtrain,#change dtrainR w/ dtrain\n",
        "    num_boost_round=num_boost_round,\n",
        "    evals=[(dtrain, \"Train\"),(dval, \"Test\")],\n",
        "    early_stopping_rounds=20\n",
        ")\n",
        "\n",
        "probsTrain = model.predict(dtrain)\n",
        "probsVal = model.predict(dval)\n",
        "print(\"Training Class Probabilities for First 5 Instances:\\n\",probsTrain[:5])\n",
        "print(\"Validation Class Probabilities for First 5 Instances:\\n\",probsVal[:5])\n",
        "best_predsTrain = np.asarray([np.argmax(line) for line in probsTrain])\n",
        "best_predsVal = np.asarray([np.argmax(line) for line in probsVal])\n",
        "\n",
        "print(\"Best Predictions for Train:\\n\", best_predsTrain+1)\n",
        "print(\"Best Predictions for Validation:\\n\", best_predsVal+1)\n",
        "#print(min(best_preds+1),max(best_preds+1))\n",
        "\n",
        "print(\"Precision Score of the Training Set= \",precision_score(y_train, best_predsTrain, average='macro'))#change y_train(y_val) to y_trainR(y_valR)\n",
        "print(\"Precision Score of the Validation Set= \",precision_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Recall Score of the Training Set= \",recall_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"Recall Score of the Validation Set= \",recall_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"F1 Score of the Training Set= \",f1_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"F1 Score of the Validation Set= \",f1_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Accuracy Score the Training Set= \", accuracy_score(y_train, best_predsTrain))\n",
        "print(\"Accuracy Score of the Validation Set= \", accuracy_score(y_val, best_predsVal))\n",
        "scoreTrain = log_loss(y_train, probsTrain)\n",
        "scoreVal = log_loss(y_val, probsVal)\n",
        "print(\"Logloss Score Training Set= \", scoreTrain)\n",
        "print(\"Logloss Score of the Validation Set= \", scoreVal)\n",
        "\n",
        "print(\"Confusion Matrix of the Training Set: \\n\")\n",
        "print(confusion_matrix(y_train, best_predsTrain))\n",
        "print(\"Confusion Matrix of the Validation Set: \\n\")\n",
        "print(confusion_matrix(y_val,best_predsVal))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\tTrain-mlogloss:1.94364\tTest-mlogloss:1.9495\n",
            "Multiple eval metrics have been passed: 'Test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until Test-mlogloss hasn't improved in 20 rounds.\n",
            "[1]\tTrain-mlogloss:1.73388\tTest-mlogloss:1.74192\n",
            "[2]\tTrain-mlogloss:1.54484\tTest-mlogloss:1.55897\n",
            "[3]\tTrain-mlogloss:1.38436\tTest-mlogloss:1.40377\n",
            "[4]\tTrain-mlogloss:1.24434\tTest-mlogloss:1.27028\n",
            "[5]\tTrain-mlogloss:1.13893\tTest-mlogloss:1.16976\n",
            "[6]\tTrain-mlogloss:1.04973\tTest-mlogloss:1.0855\n",
            "[7]\tTrain-mlogloss:0.980226\tTest-mlogloss:1.02111\n",
            "[8]\tTrain-mlogloss:0.921315\tTest-mlogloss:0.965533\n",
            "[9]\tTrain-mlogloss:0.872764\tTest-mlogloss:0.920872\n",
            "[10]\tTrain-mlogloss:0.833088\tTest-mlogloss:0.884585\n",
            "[11]\tTrain-mlogloss:0.798614\tTest-mlogloss:0.853064\n",
            "[12]\tTrain-mlogloss:0.771385\tTest-mlogloss:0.829299\n",
            "[13]\tTrain-mlogloss:0.746492\tTest-mlogloss:0.807316\n",
            "[14]\tTrain-mlogloss:0.725975\tTest-mlogloss:0.789952\n",
            "[15]\tTrain-mlogloss:0.706246\tTest-mlogloss:0.773041\n",
            "[16]\tTrain-mlogloss:0.688031\tTest-mlogloss:0.757503\n",
            "[17]\tTrain-mlogloss:0.672182\tTest-mlogloss:0.743997\n",
            "[18]\tTrain-mlogloss:0.658737\tTest-mlogloss:0.734113\n",
            "[19]\tTrain-mlogloss:0.645909\tTest-mlogloss:0.723944\n",
            "[20]\tTrain-mlogloss:0.634379\tTest-mlogloss:0.71463\n",
            "[21]\tTrain-mlogloss:0.623535\tTest-mlogloss:0.706315\n",
            "[22]\tTrain-mlogloss:0.612564\tTest-mlogloss:0.697636\n",
            "[23]\tTrain-mlogloss:0.603074\tTest-mlogloss:0.691132\n",
            "[24]\tTrain-mlogloss:0.594847\tTest-mlogloss:0.684861\n",
            "[25]\tTrain-mlogloss:0.586235\tTest-mlogloss:0.678919\n",
            "[26]\tTrain-mlogloss:0.578554\tTest-mlogloss:0.674117\n",
            "[27]\tTrain-mlogloss:0.571543\tTest-mlogloss:0.669424\n",
            "[28]\tTrain-mlogloss:0.56451\tTest-mlogloss:0.663878\n",
            "[29]\tTrain-mlogloss:0.558638\tTest-mlogloss:0.659656\n",
            "[30]\tTrain-mlogloss:0.551443\tTest-mlogloss:0.654946\n",
            "[31]\tTrain-mlogloss:0.545526\tTest-mlogloss:0.650888\n",
            "[32]\tTrain-mlogloss:0.539698\tTest-mlogloss:0.646886\n",
            "[33]\tTrain-mlogloss:0.533973\tTest-mlogloss:0.643467\n",
            "[34]\tTrain-mlogloss:0.528144\tTest-mlogloss:0.640505\n",
            "[35]\tTrain-mlogloss:0.522556\tTest-mlogloss:0.637439\n",
            "[36]\tTrain-mlogloss:0.51686\tTest-mlogloss:0.633845\n",
            "[37]\tTrain-mlogloss:0.51219\tTest-mlogloss:0.631498\n",
            "[38]\tTrain-mlogloss:0.507888\tTest-mlogloss:0.629143\n",
            "[39]\tTrain-mlogloss:0.502934\tTest-mlogloss:0.626107\n",
            "[40]\tTrain-mlogloss:0.498431\tTest-mlogloss:0.623934\n",
            "[41]\tTrain-mlogloss:0.493882\tTest-mlogloss:0.622182\n",
            "[42]\tTrain-mlogloss:0.489674\tTest-mlogloss:0.620213\n",
            "[43]\tTrain-mlogloss:0.486038\tTest-mlogloss:0.617669\n",
            "[44]\tTrain-mlogloss:0.48187\tTest-mlogloss:0.615539\n",
            "[45]\tTrain-mlogloss:0.477599\tTest-mlogloss:0.612944\n",
            "[46]\tTrain-mlogloss:0.474239\tTest-mlogloss:0.611668\n",
            "[47]\tTrain-mlogloss:0.470575\tTest-mlogloss:0.609966\n",
            "[48]\tTrain-mlogloss:0.466853\tTest-mlogloss:0.60823\n",
            "[49]\tTrain-mlogloss:0.463512\tTest-mlogloss:0.606445\n",
            "[50]\tTrain-mlogloss:0.460225\tTest-mlogloss:0.604945\n",
            "[51]\tTrain-mlogloss:0.456754\tTest-mlogloss:0.60358\n",
            "[52]\tTrain-mlogloss:0.453473\tTest-mlogloss:0.60283\n",
            "[53]\tTrain-mlogloss:0.449586\tTest-mlogloss:0.600624\n",
            "[54]\tTrain-mlogloss:0.446949\tTest-mlogloss:0.599739\n",
            "[55]\tTrain-mlogloss:0.444005\tTest-mlogloss:0.598871\n",
            "[56]\tTrain-mlogloss:0.440521\tTest-mlogloss:0.597954\n",
            "[57]\tTrain-mlogloss:0.437724\tTest-mlogloss:0.596896\n",
            "[58]\tTrain-mlogloss:0.43514\tTest-mlogloss:0.596099\n",
            "[59]\tTrain-mlogloss:0.432596\tTest-mlogloss:0.595039\n",
            "[60]\tTrain-mlogloss:0.429673\tTest-mlogloss:0.593703\n",
            "[61]\tTrain-mlogloss:0.426667\tTest-mlogloss:0.59301\n",
            "[62]\tTrain-mlogloss:0.423658\tTest-mlogloss:0.591986\n",
            "[63]\tTrain-mlogloss:0.420908\tTest-mlogloss:0.591115\n",
            "[64]\tTrain-mlogloss:0.418515\tTest-mlogloss:0.591144\n",
            "[65]\tTrain-mlogloss:0.41571\tTest-mlogloss:0.589766\n",
            "[66]\tTrain-mlogloss:0.413345\tTest-mlogloss:0.589264\n",
            "[67]\tTrain-mlogloss:0.410528\tTest-mlogloss:0.588617\n",
            "[68]\tTrain-mlogloss:0.408671\tTest-mlogloss:0.588125\n",
            "[69]\tTrain-mlogloss:0.40603\tTest-mlogloss:0.586755\n",
            "[70]\tTrain-mlogloss:0.40425\tTest-mlogloss:0.586726\n",
            "[71]\tTrain-mlogloss:0.401827\tTest-mlogloss:0.585691\n",
            "[72]\tTrain-mlogloss:0.399474\tTest-mlogloss:0.584895\n",
            "[73]\tTrain-mlogloss:0.397227\tTest-mlogloss:0.584564\n",
            "[74]\tTrain-mlogloss:0.395075\tTest-mlogloss:0.584121\n",
            "[75]\tTrain-mlogloss:0.393328\tTest-mlogloss:0.583337\n",
            "[76]\tTrain-mlogloss:0.391251\tTest-mlogloss:0.582777\n",
            "[77]\tTrain-mlogloss:0.389118\tTest-mlogloss:0.581913\n",
            "[78]\tTrain-mlogloss:0.387157\tTest-mlogloss:0.58097\n",
            "[79]\tTrain-mlogloss:0.384551\tTest-mlogloss:0.580424\n",
            "[80]\tTrain-mlogloss:0.38263\tTest-mlogloss:0.580067\n",
            "[81]\tTrain-mlogloss:0.380083\tTest-mlogloss:0.579276\n",
            "[82]\tTrain-mlogloss:0.377896\tTest-mlogloss:0.578492\n",
            "[83]\tTrain-mlogloss:0.376553\tTest-mlogloss:0.578024\n",
            "[84]\tTrain-mlogloss:0.373823\tTest-mlogloss:0.577316\n",
            "[85]\tTrain-mlogloss:0.371979\tTest-mlogloss:0.577288\n",
            "[86]\tTrain-mlogloss:0.369371\tTest-mlogloss:0.576449\n",
            "[87]\tTrain-mlogloss:0.367001\tTest-mlogloss:0.575342\n",
            "[88]\tTrain-mlogloss:0.364631\tTest-mlogloss:0.574243\n",
            "[89]\tTrain-mlogloss:0.36274\tTest-mlogloss:0.573951\n",
            "[90]\tTrain-mlogloss:0.360323\tTest-mlogloss:0.573553\n",
            "[91]\tTrain-mlogloss:0.358507\tTest-mlogloss:0.573131\n",
            "[92]\tTrain-mlogloss:0.356568\tTest-mlogloss:0.572788\n",
            "[93]\tTrain-mlogloss:0.354677\tTest-mlogloss:0.572823\n",
            "[94]\tTrain-mlogloss:0.353111\tTest-mlogloss:0.572518\n",
            "[95]\tTrain-mlogloss:0.351355\tTest-mlogloss:0.572057\n",
            "[96]\tTrain-mlogloss:0.349547\tTest-mlogloss:0.571657\n",
            "[97]\tTrain-mlogloss:0.347959\tTest-mlogloss:0.571247\n",
            "[98]\tTrain-mlogloss:0.345732\tTest-mlogloss:0.571385\n",
            "[99]\tTrain-mlogloss:0.344274\tTest-mlogloss:0.571088\n",
            "[100]\tTrain-mlogloss:0.342965\tTest-mlogloss:0.571165\n",
            "[101]\tTrain-mlogloss:0.340973\tTest-mlogloss:0.570786\n",
            "[102]\tTrain-mlogloss:0.339162\tTest-mlogloss:0.570934\n",
            "[103]\tTrain-mlogloss:0.337383\tTest-mlogloss:0.570743\n",
            "[104]\tTrain-mlogloss:0.335973\tTest-mlogloss:0.570517\n",
            "[105]\tTrain-mlogloss:0.334863\tTest-mlogloss:0.570648\n",
            "[106]\tTrain-mlogloss:0.332886\tTest-mlogloss:0.570056\n",
            "[107]\tTrain-mlogloss:0.331347\tTest-mlogloss:0.569894\n",
            "[108]\tTrain-mlogloss:0.329974\tTest-mlogloss:0.569952\n",
            "[109]\tTrain-mlogloss:0.327989\tTest-mlogloss:0.569634\n",
            "[110]\tTrain-mlogloss:0.326404\tTest-mlogloss:0.569272\n",
            "[111]\tTrain-mlogloss:0.324903\tTest-mlogloss:0.569285\n",
            "[112]\tTrain-mlogloss:0.322787\tTest-mlogloss:0.568867\n",
            "[113]\tTrain-mlogloss:0.321169\tTest-mlogloss:0.568264\n",
            "[114]\tTrain-mlogloss:0.319754\tTest-mlogloss:0.568179\n",
            "[115]\tTrain-mlogloss:0.318427\tTest-mlogloss:0.567583\n",
            "[116]\tTrain-mlogloss:0.316342\tTest-mlogloss:0.567041\n",
            "[117]\tTrain-mlogloss:0.31496\tTest-mlogloss:0.566713\n",
            "[118]\tTrain-mlogloss:0.313313\tTest-mlogloss:0.567146\n",
            "[119]\tTrain-mlogloss:0.311805\tTest-mlogloss:0.566476\n",
            "[120]\tTrain-mlogloss:0.310024\tTest-mlogloss:0.565971\n",
            "[121]\tTrain-mlogloss:0.308347\tTest-mlogloss:0.56624\n",
            "[122]\tTrain-mlogloss:0.307283\tTest-mlogloss:0.566302\n",
            "[123]\tTrain-mlogloss:0.305868\tTest-mlogloss:0.566604\n",
            "[124]\tTrain-mlogloss:0.304415\tTest-mlogloss:0.567021\n",
            "[125]\tTrain-mlogloss:0.303063\tTest-mlogloss:0.567054\n",
            "[126]\tTrain-mlogloss:0.30153\tTest-mlogloss:0.566355\n",
            "[127]\tTrain-mlogloss:0.299888\tTest-mlogloss:0.566347\n",
            "[128]\tTrain-mlogloss:0.298353\tTest-mlogloss:0.566185\n",
            "[129]\tTrain-mlogloss:0.296559\tTest-mlogloss:0.565881\n",
            "[130]\tTrain-mlogloss:0.295695\tTest-mlogloss:0.56559\n",
            "[131]\tTrain-mlogloss:0.294632\tTest-mlogloss:0.565541\n",
            "[132]\tTrain-mlogloss:0.292928\tTest-mlogloss:0.565261\n",
            "[133]\tTrain-mlogloss:0.291803\tTest-mlogloss:0.565425\n",
            "[134]\tTrain-mlogloss:0.2902\tTest-mlogloss:0.565408\n",
            "[135]\tTrain-mlogloss:0.289173\tTest-mlogloss:0.565687\n",
            "[136]\tTrain-mlogloss:0.287703\tTest-mlogloss:0.56557\n",
            "[137]\tTrain-mlogloss:0.286645\tTest-mlogloss:0.565201\n",
            "[138]\tTrain-mlogloss:0.285204\tTest-mlogloss:0.565265\n",
            "[139]\tTrain-mlogloss:0.283854\tTest-mlogloss:0.564765\n",
            "[140]\tTrain-mlogloss:0.282392\tTest-mlogloss:0.564511\n",
            "[141]\tTrain-mlogloss:0.281465\tTest-mlogloss:0.564784\n",
            "[142]\tTrain-mlogloss:0.280153\tTest-mlogloss:0.56505\n",
            "[143]\tTrain-mlogloss:0.278951\tTest-mlogloss:0.56558\n",
            "[144]\tTrain-mlogloss:0.277669\tTest-mlogloss:0.565857\n",
            "[145]\tTrain-mlogloss:0.276185\tTest-mlogloss:0.56577\n",
            "[146]\tTrain-mlogloss:0.274764\tTest-mlogloss:0.565414\n",
            "[147]\tTrain-mlogloss:0.27403\tTest-mlogloss:0.565504\n",
            "[148]\tTrain-mlogloss:0.273092\tTest-mlogloss:0.565409\n",
            "[149]\tTrain-mlogloss:0.272219\tTest-mlogloss:0.565148\n",
            "[150]\tTrain-mlogloss:0.271262\tTest-mlogloss:0.565553\n",
            "[151]\tTrain-mlogloss:0.270144\tTest-mlogloss:0.565563\n",
            "[152]\tTrain-mlogloss:0.268792\tTest-mlogloss:0.565902\n",
            "[153]\tTrain-mlogloss:0.267732\tTest-mlogloss:0.565952\n",
            "[154]\tTrain-mlogloss:0.266592\tTest-mlogloss:0.565713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[155]\tTrain-mlogloss:0.265791\tTest-mlogloss:0.565639\n",
            "[156]\tTrain-mlogloss:0.264675\tTest-mlogloss:0.565762\n",
            "[157]\tTrain-mlogloss:0.263552\tTest-mlogloss:0.565418\n",
            "[158]\tTrain-mlogloss:0.262139\tTest-mlogloss:0.565438\n",
            "[159]\tTrain-mlogloss:0.260902\tTest-mlogloss:0.565942\n",
            "[160]\tTrain-mlogloss:0.259597\tTest-mlogloss:0.566167\n",
            "Stopping. Best iteration:\n",
            "[140]\tTrain-mlogloss:0.282392\tTest-mlogloss:0.564511\n",
            "\n",
            "Training Class Probabilities for First 5 Instances:\n",
            " [[4.32121997e-05 1.35701196e-02 8.86947811e-01 2.32539605e-02\n",
            "  1.16076286e-08 1.55559552e-04 7.59910196e-02 2.56481544e-05\n",
            "  1.26700252e-05]\n",
            " [1.27578314e-04 5.80920219e-01 3.89635056e-01 2.54892856e-02\n",
            "  2.62578624e-07 2.16188128e-04 3.29053099e-03 2.75950850e-04\n",
            "  4.49456747e-05]\n",
            " [2.35912201e-04 5.36339045e-01 4.41821277e-01 7.51442928e-03\n",
            "  3.79903917e-03 1.32194301e-03 1.58136222e-03 6.28292235e-03\n",
            "  1.10407197e-03]\n",
            " [4.80750725e-02 6.01877738e-03 3.08276620e-03 3.05130724e-02\n",
            "  4.14092719e-05 7.76751399e-01 1.30463809e-01 2.59907288e-03\n",
            "  2.45468458e-03]\n",
            " [2.41160346e-03 8.48164973e-07 2.81211237e-06 2.14618194e-04\n",
            "  1.01379655e-07 9.95933831e-01 4.00851772e-04 4.85525641e-04\n",
            "  5.49872580e-04]]\n",
            "Validation Class Probabilities for First 5 Instances:\n",
            " [[3.3552994e-03 8.1201857e-01 2.2348939e-02 1.3750863e-01 5.6627989e-05\n",
            "  5.3017866e-03 3.9054707e-03 1.4800237e-03 1.4024669e-02]\n",
            " [1.3939937e-05 4.6128739e-04 5.6389112e-05 1.7129657e-03 9.9765021e-01\n",
            "  2.9525247e-06 2.3471310e-05 4.0961091e-05 3.7793612e-05]\n",
            " [5.0118100e-04 2.1712053e-01 1.5362555e-01 6.1451828e-01 4.5436072e-06\n",
            "  3.5145134e-04 8.3836792e-03 2.8387515e-03 2.6559937e-03]\n",
            " [5.9285267e-06 9.6637505e-01 1.1869550e-02 2.1648621e-02 3.1701038e-07\n",
            "  3.0121269e-05 5.1103998e-05 1.3057924e-05 6.3054213e-06]\n",
            " [7.8229904e-02 2.1762224e-02 4.2218887e-03 1.2070239e-03 3.3502007e-04\n",
            "  7.0011956e-03 1.2664844e-03 7.0830612e-03 8.7889314e-01]]\n",
            "Best Predictions for Train:\n",
            " [3 2 2 ... 2 6 6]\n",
            "Best Predictions for Validation:\n",
            " [2 5 4 ... 2 4 2]\n",
            "Precision Score of the Training Set=  0.8615217835548912\n",
            "Precision Score of the Validation Set=  0.7577130668158965\n",
            "Recall Score of the Training Set=  0.9189450841395134\n",
            "Recall Score of the Validation Set=  0.7978232590153667\n",
            "F1 Score of the Training Set=  0.8843724931887361\n",
            "F1 Score of the Validation Set=  0.7736948569713695\n",
            "Accuracy Score the Training Set=  0.8879237202537271\n",
            "Accuracy Score of the Validation Set=  0.8001777634130576\n",
            "Logloss Score Training Set=  0.3167077553926924\n",
            "Logloss Score of the Validation Set=  0.5257120492871312\n",
            "Confusion Matrix of the Training Set: \n",
            "\n",
            "[[ 1500     2     1     0     0     4    13    12    11]\n",
            " [   28  9875  1999   737    10     4   191    20    34]\n",
            " [   11   694  5220   341     0     1   129     6     1]\n",
            " [    2    84   103  1944     1     3    14     0     2]\n",
            " [    0     0     0     0  2189     0     2     0     0]\n",
            " [  132    25    15    21     0 10830   124    79    82]\n",
            " [   22    24    38    21     1     5  2155     4     1]\n",
            " [  139    25    21     2     1    27    56  6451    49]\n",
            " [   97    14     4     8     0    13    16    22  3790]]\n",
            "Confusion Matrix of the Validation Set: \n",
            "\n",
            "[[ 260    6    2    4    1   17   17   29   50]\n",
            " [  14 2225  678  207    5   10   63    5   17]\n",
            " [   1  351 1034  134    0    4   66    5    6]\n",
            " [   0   61   72  381    1    6   15    0    2]\n",
            " [   1    4    0    0  541    1    1    0    0]\n",
            " [  35   11    4   13    0 2626   46   43   49]\n",
            " [  18   18   44    8    2   12  438   19    9]\n",
            " [  61    6    7    1    0   23   29 1540   26]\n",
            " [  67    4    1    2    2   19    8   30  858]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lj6jYQ2lybGT",
        "colab_type": "text"
      },
      "source": [
        "## Tune Eta\n",
        "\n",
        "If decreased num_round must be increased while training. best eto is 0.05 but it takes too much time and doesnt outperform 0.1 too. So, choose 0.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MN9-AILybGU",
        "colab_type": "code",
        "colab": {},
        "outputId": "d0805f57-5cee-4db2-e17c-238c4758a19f"
      },
      "source": [
        "%time\n",
        "\n",
        "params = {\n",
        "    'eta': 0.3,  \n",
        "    'n_estimators': 1000,\n",
        "    'silent': True,  # option for logging\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass tasks\n",
        "    'num_class': 9,  # number of classes to predict\n",
        "    'max_depth': 5,  # depth of the trees in the boosting process\n",
        "    'seed': 42,\n",
        "    'eval_metric': 'mlogloss', #cross-entropy is chosen as evaluation metric\n",
        "    'min_child_weight':4,\n",
        "    'gamma':0.0,\n",
        "    'subsample':0.9,\n",
        "    'colsample_bytree':0.8,\n",
        "    'nthread': 16,\n",
        "    #'scale_pos_weight':1,\n",
        "    'reg_alpha':0,\n",
        "    'max_delta_step':1\n",
        "    } \n",
        "\n",
        "min_mlogloss = float(\"Inf\")\n",
        "best_params = None\n",
        "\n",
        "for eta in [.3, .2, .1, .05, .01, .005]:\n",
        "    print(\"CV with eta={}\".format(eta))\n",
        "\n",
        "    # We update our parameters\n",
        "    params['eta'] = eta\n",
        "\n",
        "    # Run and time CV\n",
        "    %time cv_results = xgb.cv(params,dtrain,num_boost_round=num_boost_round,seed=42,nfold=5,metrics='mlogloss',early_stopping_rounds=20,fpreproc = fpreproc,stratified =True,verbose_eval =10)\n",
        "          \n",
        "\n",
        "    # Update best score\n",
        "    mean_mlogloss = cv_results['test-mlogloss-mean'].min()\n",
        "    boost_rounds = cv_results['test-mlogloss-mean'].argmin()\n",
        "    print(\"\\tmlogloss {} for {} rounds\\n\".format(mean_mlogloss, boost_rounds))\n",
        "    if mean_mlogloss < min_mlogloss:\n",
        "        min_mlogloss = mean_mlogloss\n",
        "        best_params = eta\n",
        "\n",
        "print(\"Best params: {}, mlogloss: {}\".format(best_params, min_mlogloss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wall time: 0 ns\n",
            "CV with eta=0.3\n",
            "[0]\ttrain-mlogloss:1.94794+0.00262723\ttest-mlogloss:1.95538+0.00257747\n",
            "[10]\ttrain-mlogloss:0.82936+0.00277919\ttest-mlogloss:0.891218+0.00443345\n",
            "[20]\ttrain-mlogloss:0.622461+0.00259627\ttest-mlogloss:0.71903+0.00462436\n",
            "[30]\ttrain-mlogloss:0.538042+0.00283904\ttest-mlogloss:0.662538+0.00448166\n",
            "[40]\ttrain-mlogloss:0.484913+0.00265099\ttest-mlogloss:0.634081+0.00534012\n",
            "[50]\ttrain-mlogloss:0.444755+0.00288571\ttest-mlogloss:0.616957+0.00497551\n",
            "[60]\ttrain-mlogloss:0.412848+0.00254578\ttest-mlogloss:0.605122+0.00534171\n",
            "[70]\ttrain-mlogloss:0.385459+0.00258819\ttest-mlogloss:0.597331+0.00527584\n",
            "[80]\ttrain-mlogloss:0.361326+0.00254351\ttest-mlogloss:0.591653+0.00529335\n",
            "[90]\ttrain-mlogloss:0.338849+0.00182913\ttest-mlogloss:0.587857+0.00603677\n",
            "[100]\ttrain-mlogloss:0.319711+0.00204266\ttest-mlogloss:0.584668+0.00593615\n",
            "[110]\ttrain-mlogloss:0.302528+0.00263955\ttest-mlogloss:0.583093+0.00624481\n",
            "[120]\ttrain-mlogloss:0.285986+0.00279766\ttest-mlogloss:0.582487+0.00635661\n",
            "[130]\ttrain-mlogloss:0.270481+0.00252148\ttest-mlogloss:0.582985+0.00621439\n",
            "Wall time: 2min 40s\n",
            "\tmlogloss 0.5822644 for 119 rounds\n",
            "\n",
            "CV with eta=0.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\ttrain-mlogloss:2.02798+0.00176979\ttest-mlogloss:2.03294+0.00173213\n",
            "[10]\ttrain-mlogloss:1.02043+0.00340252\ttest-mlogloss:1.06671+0.00273778\n",
            "[20]\ttrain-mlogloss:0.739934+0.00263917\ttest-mlogloss:0.813295+0.00485221\n",
            "[30]\ttrain-mlogloss:0.625297+0.00187535\ttest-mlogloss:0.720292+0.00566353\n",
            "[40]\ttrain-mlogloss:0.561677+0.0017557\ttest-mlogloss:0.67641+0.00585179\n",
            "[50]\ttrain-mlogloss:0.51808+0.00150695\ttest-mlogloss:0.649213+0.00590989\n",
            "[60]\ttrain-mlogloss:0.484598+0.00180012\ttest-mlogloss:0.631103+0.00562825\n",
            "[70]\ttrain-mlogloss:0.456485+0.00245465\ttest-mlogloss:0.618115+0.00604968\n",
            "[80]\ttrain-mlogloss:0.431963+0.0023803\ttest-mlogloss:0.60812+0.00580495\n",
            "[90]\ttrain-mlogloss:0.411642+0.00202451\ttest-mlogloss:0.600148+0.00639029\n",
            "[100]\ttrain-mlogloss:0.392717+0.00207496\ttest-mlogloss:0.593725+0.0063765\n",
            "[110]\ttrain-mlogloss:0.375595+0.00187112\ttest-mlogloss:0.588849+0.00622427\n",
            "[120]\ttrain-mlogloss:0.360039+0.00221487\ttest-mlogloss:0.585252+0.00619612\n",
            "[130]\ttrain-mlogloss:0.345684+0.0021707\ttest-mlogloss:0.58205+0.00660483\n",
            "[140]\ttrain-mlogloss:0.332074+0.00206303\ttest-mlogloss:0.579389+0.0061955\n",
            "[150]\ttrain-mlogloss:0.319408+0.00181365\ttest-mlogloss:0.577948+0.00591477\n",
            "[160]\ttrain-mlogloss:0.307521+0.00199865\ttest-mlogloss:0.577348+0.00597756\n",
            "[170]\ttrain-mlogloss:0.296253+0.00203936\ttest-mlogloss:0.576109+0.00609856\n",
            "[180]\ttrain-mlogloss:0.285324+0.00168134\ttest-mlogloss:0.576351+0.00550893\n",
            "[190]\ttrain-mlogloss:0.275083+0.00173809\ttest-mlogloss:0.576127+0.00551751\n",
            "[200]\ttrain-mlogloss:0.265138+0.00232899\ttest-mlogloss:0.576167+0.00567125\n",
            "[210]\ttrain-mlogloss:0.256122+0.00236022\ttest-mlogloss:0.576386+0.00588236\n",
            "[220]\ttrain-mlogloss:0.247398+0.00234007\ttest-mlogloss:0.576837+0.00640931\n",
            "Wall time: 4min 19s\n",
            "\tmlogloss 0.5759532 for 202 rounds\n",
            "\n",
            "CV with eta=0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\ttrain-mlogloss:2.11114+0.000894384\ttest-mlogloss:2.11362+0.000873664\n",
            "[10]\ttrain-mlogloss:1.43157+0.00247873\ttest-mlogloss:1.45595+0.0034606\n",
            "[20]\ttrain-mlogloss:1.05012+0.00474347\ttest-mlogloss:1.09344+0.00367331\n",
            "[30]\ttrain-mlogloss:0.858248+0.00352296\ttest-mlogloss:0.916164+0.00394313\n",
            "[40]\ttrain-mlogloss:0.749239+0.00286761\ttest-mlogloss:0.820082+0.00428562\n",
            "[50]\ttrain-mlogloss:0.677865+0.00173907\ttest-mlogloss:0.76018+0.0049505\n",
            "[60]\ttrain-mlogloss:0.62899+0.00173048\ttest-mlogloss:0.72163+0.00484994\n",
            "[70]\ttrain-mlogloss:0.592473+0.00140268\ttest-mlogloss:0.694693+0.00475542\n",
            "[80]\ttrain-mlogloss:0.564018+0.00158558\ttest-mlogloss:0.674896+0.00468934\n",
            "[90]\ttrain-mlogloss:0.540205+0.00133984\ttest-mlogloss:0.659211+0.00482925\n",
            "[100]\ttrain-mlogloss:0.520102+0.00130494\ttest-mlogloss:0.646888+0.00495995\n",
            "[110]\ttrain-mlogloss:0.501881+0.00163272\ttest-mlogloss:0.636799+0.00482189\n",
            "[120]\ttrain-mlogloss:0.485946+0.00174344\ttest-mlogloss:0.628277+0.00437968\n",
            "[130]\ttrain-mlogloss:0.471648+0.00177326\ttest-mlogloss:0.620791+0.00451412\n",
            "[140]\ttrain-mlogloss:0.457896+0.00195494\ttest-mlogloss:0.61441+0.00456382\n",
            "[150]\ttrain-mlogloss:0.445246+0.00212431\ttest-mlogloss:0.608839+0.0043463\n",
            "[160]\ttrain-mlogloss:0.433679+0.00238784\ttest-mlogloss:0.603785+0.00435126\n",
            "[170]\ttrain-mlogloss:0.422775+0.00242039\ttest-mlogloss:0.59932+0.00433356\n",
            "[180]\ttrain-mlogloss:0.412611+0.00211843\ttest-mlogloss:0.595398+0.00437736\n",
            "[190]\ttrain-mlogloss:0.403068+0.00194103\ttest-mlogloss:0.592032+0.00420017\n",
            "[200]\ttrain-mlogloss:0.393651+0.00214843\ttest-mlogloss:0.588844+0.00401939\n",
            "[210]\ttrain-mlogloss:0.385142+0.00195153\ttest-mlogloss:0.586147+0.00400239\n",
            "[220]\ttrain-mlogloss:0.376442+0.00216851\ttest-mlogloss:0.583732+0.00390753\n",
            "[230]\ttrain-mlogloss:0.368409+0.00209243\ttest-mlogloss:0.581664+0.00393817\n",
            "[240]\ttrain-mlogloss:0.360404+0.00224017\ttest-mlogloss:0.579885+0.0041459\n",
            "[250]\ttrain-mlogloss:0.353065+0.00248899\ttest-mlogloss:0.578166+0.00390863\n",
            "[260]\ttrain-mlogloss:0.345703+0.00233092\ttest-mlogloss:0.576687+0.00403557\n",
            "[270]\ttrain-mlogloss:0.338914+0.00222916\ttest-mlogloss:0.57524+0.00404814\n",
            "[280]\ttrain-mlogloss:0.332157+0.0024512\ttest-mlogloss:0.574325+0.00414484\n",
            "[290]\ttrain-mlogloss:0.325914+0.00237055\ttest-mlogloss:0.573163+0.00402817\n",
            "[300]\ttrain-mlogloss:0.319675+0.0022014\ttest-mlogloss:0.572559+0.00441649\n",
            "[310]\ttrain-mlogloss:0.313402+0.00232321\ttest-mlogloss:0.571585+0.00421968\n",
            "[320]\ttrain-mlogloss:0.307552+0.00217736\ttest-mlogloss:0.571057+0.0042835\n",
            "[330]\ttrain-mlogloss:0.301909+0.00232964\ttest-mlogloss:0.570577+0.00442029\n",
            "[340]\ttrain-mlogloss:0.296002+0.00235714\ttest-mlogloss:0.570082+0.00466173\n",
            "[350]\ttrain-mlogloss:0.290627+0.00237579\ttest-mlogloss:0.569538+0.00477681\n",
            "[360]\ttrain-mlogloss:0.285348+0.00234619\ttest-mlogloss:0.568864+0.00486694\n",
            "[370]\ttrain-mlogloss:0.280028+0.00243093\ttest-mlogloss:0.568706+0.00484953\n",
            "[380]\ttrain-mlogloss:0.275168+0.00252002\ttest-mlogloss:0.568574+0.00472139\n",
            "[390]\ttrain-mlogloss:0.270231+0.00249385\ttest-mlogloss:0.568567+0.00485629\n",
            "[400]\ttrain-mlogloss:0.265341+0.0023687\ttest-mlogloss:0.568533+0.00478889\n",
            "Wall time: 7min 39s\n",
            "\tmlogloss 0.5684495999999999 for 386 rounds\n",
            "\n",
            "CV with eta=0.05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\ttrain-mlogloss:2.15383+0.000449672\ttest-mlogloss:2.15507+0.000438892\n",
            "[10]\ttrain-mlogloss:1.76717+0.00160301\ttest-mlogloss:1.77957+0.00192715\n",
            "[20]\ttrain-mlogloss:1.46034+0.00291208\ttest-mlogloss:1.48367+0.00241002\n",
            "[30]\ttrain-mlogloss:1.22634+0.00281166\ttest-mlogloss:1.26002+0.00353606\n",
            "[40]\ttrain-mlogloss:1.06346+0.00259375\ttest-mlogloss:1.10581+0.00397298\n",
            "[50]\ttrain-mlogloss:0.948304+0.00230952\ttest-mlogloss:0.998578+0.00551537\n",
            "[60]\ttrain-mlogloss:0.865416+0.00158008\ttest-mlogloss:0.922477+0.00557691\n",
            "[70]\ttrain-mlogloss:0.80226+0.00160682\ttest-mlogloss:0.865699+0.00516529\n",
            "[80]\ttrain-mlogloss:0.753186+0.00144999\ttest-mlogloss:0.822899+0.00558898\n",
            "[90]\ttrain-mlogloss:0.713582+0.00143349\ttest-mlogloss:0.788986+0.0052725\n",
            "[100]\ttrain-mlogloss:0.6812+0.00160747\ttest-mlogloss:0.762173+0.00557842\n",
            "[110]\ttrain-mlogloss:0.653603+0.00168942\ttest-mlogloss:0.740138+0.0053984\n",
            "[120]\ttrain-mlogloss:0.63072+0.00146922\ttest-mlogloss:0.722346+0.00514527\n",
            "[130]\ttrain-mlogloss:0.6111+0.00144625\ttest-mlogloss:0.707467+0.00525754\n",
            "[140]\ttrain-mlogloss:0.593653+0.00171278\ttest-mlogloss:0.694631+0.00503706\n",
            "[150]\ttrain-mlogloss:0.578442+0.00167386\ttest-mlogloss:0.684001+0.00500802\n",
            "[160]\ttrain-mlogloss:0.564797+0.00176125\ttest-mlogloss:0.674784+0.00487101\n",
            "[170]\ttrain-mlogloss:0.552489+0.00207114\ttest-mlogloss:0.666561+0.00497725\n",
            "[180]\ttrain-mlogloss:0.54106+0.00191937\ttest-mlogloss:0.659048+0.00520583\n",
            "[190]\ttrain-mlogloss:0.530522+0.00193733\ttest-mlogloss:0.652347+0.00524161\n",
            "[200]\ttrain-mlogloss:0.520703+0.00202359\ttest-mlogloss:0.646436+0.00497033\n",
            "[210]\ttrain-mlogloss:0.511454+0.00190387\ttest-mlogloss:0.641189+0.0049297\n",
            "[220]\ttrain-mlogloss:0.50234+0.00179794\ttest-mlogloss:0.636052+0.00489721\n",
            "[230]\ttrain-mlogloss:0.494125+0.00186275\ttest-mlogloss:0.631449+0.00498157\n",
            "[240]\ttrain-mlogloss:0.486277+0.00204575\ttest-mlogloss:0.627347+0.00494506\n",
            "[250]\ttrain-mlogloss:0.478662+0.00226852\ttest-mlogloss:0.623464+0.00497981\n",
            "[260]\ttrain-mlogloss:0.471349+0.00229227\ttest-mlogloss:0.619713+0.00504793\n",
            "[270]\ttrain-mlogloss:0.464563+0.00236442\ttest-mlogloss:0.616408+0.00495983\n",
            "[280]\ttrain-mlogloss:0.457926+0.00230311\ttest-mlogloss:0.613439+0.0050233\n",
            "[290]\ttrain-mlogloss:0.451625+0.00202531\ttest-mlogloss:0.610477+0.00486094\n",
            "[300]\ttrain-mlogloss:0.445501+0.00178946\ttest-mlogloss:0.607561+0.00493337\n",
            "[310]\ttrain-mlogloss:0.439605+0.00182053\ttest-mlogloss:0.605082+0.00501405\n",
            "[320]\ttrain-mlogloss:0.434024+0.00171256\ttest-mlogloss:0.602521+0.00505163\n",
            "[330]\ttrain-mlogloss:0.428439+0.00177671\ttest-mlogloss:0.600202+0.00502062\n",
            "[340]\ttrain-mlogloss:0.422897+0.00186125\ttest-mlogloss:0.598053+0.00489585\n",
            "[350]\ttrain-mlogloss:0.417758+0.00201383\ttest-mlogloss:0.595944+0.00490579\n",
            "[360]\ttrain-mlogloss:0.412508+0.00212222\ttest-mlogloss:0.593832+0.00492859\n",
            "[370]\ttrain-mlogloss:0.40755+0.00204257\ttest-mlogloss:0.592049+0.00495203\n",
            "[380]\ttrain-mlogloss:0.40288+0.00207676\ttest-mlogloss:0.5903+0.00485383\n",
            "[390]\ttrain-mlogloss:0.39819+0.0019356\ttest-mlogloss:0.588723+0.00482823\n",
            "[400]\ttrain-mlogloss:0.393794+0.00191055\ttest-mlogloss:0.587191+0.00501261\n",
            "[410]\ttrain-mlogloss:0.38923+0.0019399\ttest-mlogloss:0.585715+0.00498729\n",
            "[420]\ttrain-mlogloss:0.384981+0.00199271\ttest-mlogloss:0.584343+0.00491448\n",
            "[430]\ttrain-mlogloss:0.380631+0.00194272\ttest-mlogloss:0.582873+0.00485941\n",
            "[440]\ttrain-mlogloss:0.376454+0.00195421\ttest-mlogloss:0.581712+0.00486704\n",
            "[450]\ttrain-mlogloss:0.372431+0.00190545\ttest-mlogloss:0.580613+0.00473934\n",
            "[460]\ttrain-mlogloss:0.368382+0.00200649\ttest-mlogloss:0.579564+0.004687\n",
            "[470]\ttrain-mlogloss:0.364554+0.00185394\ttest-mlogloss:0.57861+0.00474804\n",
            "[480]\ttrain-mlogloss:0.360726+0.00203643\ttest-mlogloss:0.577606+0.00454785\n",
            "[490]\ttrain-mlogloss:0.356836+0.00202306\ttest-mlogloss:0.576652+0.00453926\n",
            "[500]\ttrain-mlogloss:0.353258+0.00213918\ttest-mlogloss:0.575822+0.00458709\n",
            "[510]\ttrain-mlogloss:0.349413+0.00218708\ttest-mlogloss:0.574879+0.00454491\n",
            "[520]\ttrain-mlogloss:0.345736+0.00211919\ttest-mlogloss:0.574023+0.00436767\n",
            "[530]\ttrain-mlogloss:0.3422+0.00206554\ttest-mlogloss:0.573399+0.00435917\n",
            "[540]\ttrain-mlogloss:0.338744+0.00206612\ttest-mlogloss:0.572607+0.00442789\n",
            "[550]\ttrain-mlogloss:0.335389+0.00225719\ttest-mlogloss:0.571967+0.0043247\n",
            "[560]\ttrain-mlogloss:0.332102+0.00225608\ttest-mlogloss:0.571493+0.00448532\n",
            "[570]\ttrain-mlogloss:0.32873+0.00219061\ttest-mlogloss:0.570863+0.0045332\n",
            "[580]\ttrain-mlogloss:0.325528+0.00217248\ttest-mlogloss:0.570372+0.00441944\n",
            "[590]\ttrain-mlogloss:0.322408+0.00218786\ttest-mlogloss:0.569963+0.0043787\n",
            "[600]\ttrain-mlogloss:0.319293+0.00215381\ttest-mlogloss:0.569304+0.00436783\n",
            "[610]\ttrain-mlogloss:0.316067+0.0020947\ttest-mlogloss:0.569021+0.0042379\n",
            "[620]\ttrain-mlogloss:0.312983+0.00211307\ttest-mlogloss:0.568551+0.00420734\n",
            "[630]\ttrain-mlogloss:0.310056+0.00218125\ttest-mlogloss:0.568116+0.00399693\n",
            "[640]\ttrain-mlogloss:0.307159+0.00236156\ttest-mlogloss:0.567936+0.00404229\n",
            "[650]\ttrain-mlogloss:0.304279+0.00239702\ttest-mlogloss:0.56765+0.00422993\n",
            "[660]\ttrain-mlogloss:0.301278+0.00233303\ttest-mlogloss:0.567294+0.00420502\n",
            "[670]\ttrain-mlogloss:0.298319+0.00239813\ttest-mlogloss:0.567049+0.00422385\n",
            "[680]\ttrain-mlogloss:0.295518+0.0023421\ttest-mlogloss:0.566809+0.0041825\n",
            "[690]\ttrain-mlogloss:0.292863+0.0022524\ttest-mlogloss:0.566526+0.00421233\n",
            "[700]\ttrain-mlogloss:0.290187+0.00228867\ttest-mlogloss:0.56641+0.00413674\n",
            "[710]\ttrain-mlogloss:0.287439+0.00229554\ttest-mlogloss:0.566249+0.00409142\n",
            "[720]\ttrain-mlogloss:0.28481+0.00230007\ttest-mlogloss:0.56608+0.0040671\n",
            "[730]\ttrain-mlogloss:0.28228+0.00238355\ttest-mlogloss:0.565907+0.00409291\n",
            "[740]\ttrain-mlogloss:0.279605+0.00236461\ttest-mlogloss:0.565639+0.00399834\n",
            "[750]\ttrain-mlogloss:0.277033+0.00243314\ttest-mlogloss:0.565445+0.00392684\n",
            "[760]\ttrain-mlogloss:0.274492+0.00237283\ttest-mlogloss:0.565359+0.00389798\n",
            "[770]\ttrain-mlogloss:0.272059+0.00240458\ttest-mlogloss:0.565399+0.00379194\n",
            "[780]\ttrain-mlogloss:0.269624+0.00253078\ttest-mlogloss:0.565448+0.00378672\n",
            "Wall time: 14min 39s\n",
            "\tmlogloss 0.5653279999999999 for 763 rounds\n",
            "\n",
            "CV with eta=0.01\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\ttrain-mlogloss:2.18849+9.03208e-05\ttest-mlogloss:2.18874+8.791e-05\n",
            "[10]\ttrain-mlogloss:2.10366+0.000526384\ttest-mlogloss:2.10613+0.000540984\n",
            "[20]\ttrain-mlogloss:2.02234+0.000636547\ttest-mlogloss:2.0272+0.000480622\n",
            "[30]\ttrain-mlogloss:1.94379+0.000935239\ttest-mlogloss:1.9509+0.0011136\n",
            "[40]\ttrain-mlogloss:1.86853+0.00133679\ttest-mlogloss:1.8779+0.00164749\n",
            "[50]\ttrain-mlogloss:1.79596+0.00144006\ttest-mlogloss:1.80756+0.00234558\n",
            "[60]\ttrain-mlogloss:1.72696+0.00140016\ttest-mlogloss:1.74081+0.00251419\n",
            "[70]\ttrain-mlogloss:1.66097+0.00161666\ttest-mlogloss:1.67698+0.00267018\n",
            "[80]\ttrain-mlogloss:1.5982+0.00189094\ttest-mlogloss:1.61646+0.00307515\n",
            "[90]\ttrain-mlogloss:1.53857+0.00166023\ttest-mlogloss:1.55902+0.00312918\n",
            "[100]\ttrain-mlogloss:1.48217+0.00160142\ttest-mlogloss:1.50481+0.00340751\n",
            "[110]\ttrain-mlogloss:1.42822+0.00165229\ttest-mlogloss:1.45302+0.00382518\n",
            "[120]\ttrain-mlogloss:1.37752+0.00138336\ttest-mlogloss:1.40443+0.00382046\n",
            "[130]\ttrain-mlogloss:1.32974+0.00113233\ttest-mlogloss:1.35879+0.00429316\n",
            "[140]\ttrain-mlogloss:1.28475+0.0004953\ttest-mlogloss:1.31578+0.00419218\n",
            "[150]\ttrain-mlogloss:1.24295+0.000474004\ttest-mlogloss:1.27593+0.00448931\n",
            "[160]\ttrain-mlogloss:1.20356+0.00048882\ttest-mlogloss:1.2385+0.00457056\n",
            "[170]\ttrain-mlogloss:1.16794+0.000477935\ttest-mlogloss:1.20477+0.00438847\n",
            "[180]\ttrain-mlogloss:1.1346+0.000372982\ttest-mlogloss:1.17323+0.00456428\n",
            "[190]\ttrain-mlogloss:1.10367+0.000589553\ttest-mlogloss:1.14415+0.00427716\n",
            "[200]\ttrain-mlogloss:1.07534+0.000582514\ttest-mlogloss:1.1175+0.00443714\n",
            "[210]\ttrain-mlogloss:1.0488+0.000672524\ttest-mlogloss:1.09261+0.00441424\n",
            "[220]\ttrain-mlogloss:1.02387+0.000892675\ttest-mlogloss:1.06927+0.00439695\n",
            "[230]\ttrain-mlogloss:1.00059+0.000886409\ttest-mlogloss:1.04751+0.00446007\n",
            "[240]\ttrain-mlogloss:0.978758+0.000835545\ttest-mlogloss:1.0272+0.00460842\n",
            "[250]\ttrain-mlogloss:0.958537+0.000665297\ttest-mlogloss:1.00854+0.00479271\n",
            "[260]\ttrain-mlogloss:0.939265+0.000627426\ttest-mlogloss:0.990655+0.00496385\n",
            "[270]\ttrain-mlogloss:0.921328+0.000599585\ttest-mlogloss:0.974099+0.00509392\n",
            "[280]\ttrain-mlogloss:0.904374+0.000736329\ttest-mlogloss:0.958464+0.00517913\n",
            "[290]\ttrain-mlogloss:0.888225+0.000779592\ttest-mlogloss:0.943656+0.00511816\n",
            "[300]\ttrain-mlogloss:0.87299+0.000737359\ttest-mlogloss:0.929784+0.00514799\n",
            "[310]\ttrain-mlogloss:0.858649+0.000651235\ttest-mlogloss:0.916732+0.00526122\n",
            "[320]\ttrain-mlogloss:0.845036+0.000755068\ttest-mlogloss:0.904392+0.00519822\n",
            "[330]\ttrain-mlogloss:0.832071+0.000774122\ttest-mlogloss:0.892683+0.00531351\n",
            "[340]\ttrain-mlogloss:0.819872+0.000745157\ttest-mlogloss:0.881763+0.00546595\n",
            "[350]\ttrain-mlogloss:0.808212+0.000819357\ttest-mlogloss:0.871356+0.00553732\n",
            "[360]\ttrain-mlogloss:0.797093+0.000879921\ttest-mlogloss:0.861408+0.00554388\n",
            "[370]\ttrain-mlogloss:0.786654+0.000950907\ttest-mlogloss:0.85217+0.0056769\n",
            "[380]\ttrain-mlogloss:0.776586+0.000986155\ttest-mlogloss:0.843301+0.00564981\n",
            "[390]\ttrain-mlogloss:0.766971+0.000979648\ttest-mlogloss:0.834888+0.00564964\n",
            "[400]\ttrain-mlogloss:0.757887+0.000947908\ttest-mlogloss:0.826924+0.00572052\n",
            "[410]\ttrain-mlogloss:0.74905+0.000951115\ttest-mlogloss:0.819206+0.00569855\n",
            "[420]\ttrain-mlogloss:0.740644+0.00103772\ttest-mlogloss:0.811967+0.00566054\n",
            "[430]\ttrain-mlogloss:0.732555+0.000922463\ttest-mlogloss:0.805038+0.00584564\n",
            "[440]\ttrain-mlogloss:0.724718+0.000979331\ttest-mlogloss:0.798335+0.00580986\n",
            "[450]\ttrain-mlogloss:0.717259+0.000943386\ttest-mlogloss:0.792017+0.00583272\n",
            "[460]\ttrain-mlogloss:0.710086+0.000919375\ttest-mlogloss:0.786013+0.005873\n",
            "[470]\ttrain-mlogloss:0.70322+0.000932586\ttest-mlogloss:0.780244+0.00593769\n",
            "[480]\ttrain-mlogloss:0.696554+0.00091576\ttest-mlogloss:0.774686+0.00596592\n",
            "[490]\ttrain-mlogloss:0.690133+0.000979081\ttest-mlogloss:0.769359+0.00589585\n",
            "[500]\ttrain-mlogloss:0.683937+0.000962585\ttest-mlogloss:0.764235+0.0058748\n",
            "[510]\ttrain-mlogloss:0.678008+0.000955164\ttest-mlogloss:0.759391+0.00588899\n",
            "[520]\ttrain-mlogloss:0.672291+0.000981653\ttest-mlogloss:0.754721+0.00582071\n",
            "[530]\ttrain-mlogloss:0.666729+0.000989849\ttest-mlogloss:0.75022+0.00580842\n",
            "[540]\ttrain-mlogloss:0.66138+0.00100801\ttest-mlogloss:0.745939+0.00581762\n",
            "[550]\ttrain-mlogloss:0.656155+0.000972868\ttest-mlogloss:0.741783+0.00584823\n",
            "[560]\ttrain-mlogloss:0.651191+0.000947036\ttest-mlogloss:0.737845+0.00582372\n",
            "[570]\ttrain-mlogloss:0.646428+0.000973264\ttest-mlogloss:0.734071+0.00576706\n",
            "[580]\ttrain-mlogloss:0.641738+0.000960281\ttest-mlogloss:0.730422+0.00575258\n",
            "[590]\ttrain-mlogloss:0.637173+0.000964848\ttest-mlogloss:0.726899+0.00575082\n",
            "[600]\ttrain-mlogloss:0.632788+0.00100381\ttest-mlogloss:0.723472+0.00570403\n",
            "[610]\ttrain-mlogloss:0.628555+0.00101959\ttest-mlogloss:0.720219+0.00565969\n",
            "[620]\ttrain-mlogloss:0.624377+0.00104234\ttest-mlogloss:0.717056+0.00569227\n",
            "[630]\ttrain-mlogloss:0.62036+0.00112083\ttest-mlogloss:0.714029+0.0056447\n",
            "[640]\ttrain-mlogloss:0.616442+0.00116011\ttest-mlogloss:0.711057+0.00559875\n",
            "[650]\ttrain-mlogloss:0.612634+0.00121484\ttest-mlogloss:0.708192+0.00557461\n",
            "[660]\ttrain-mlogloss:0.608888+0.00114408\ttest-mlogloss:0.705414+0.00562142\n",
            "[670]\ttrain-mlogloss:0.60532+0.00110173\ttest-mlogloss:0.702757+0.00562594\n",
            "[680]\ttrain-mlogloss:0.601844+0.00112884\ttest-mlogloss:0.700169+0.00562769\n",
            "[690]\ttrain-mlogloss:0.598452+0.00117581\ttest-mlogloss:0.697674+0.00559411\n",
            "[700]\ttrain-mlogloss:0.595141+0.00114963\ttest-mlogloss:0.695287+0.00559744\n",
            "[710]\ttrain-mlogloss:0.591914+0.00113631\ttest-mlogloss:0.692882+0.00561603\n",
            "[720]\ttrain-mlogloss:0.588674+0.00111235\ttest-mlogloss:0.690604+0.00563696\n",
            "[730]\ttrain-mlogloss:0.585592+0.00118002\ttest-mlogloss:0.688433+0.0055971\n",
            "[740]\ttrain-mlogloss:0.582572+0.00122476\ttest-mlogloss:0.686295+0.00557805\n",
            "[750]\ttrain-mlogloss:0.579678+0.00127132\ttest-mlogloss:0.684243+0.00558608\n",
            "[760]\ttrain-mlogloss:0.576752+0.00130267\ttest-mlogloss:0.682208+0.00560763\n",
            "[770]\ttrain-mlogloss:0.573966+0.00128077\ttest-mlogloss:0.680293+0.00559602\n",
            "[780]\ttrain-mlogloss:0.571172+0.00141313\ttest-mlogloss:0.678412+0.00549286\n",
            "[790]\ttrain-mlogloss:0.568482+0.00141989\ttest-mlogloss:0.676547+0.00548017\n",
            "[800]\ttrain-mlogloss:0.565861+0.00151633\ttest-mlogloss:0.674715+0.00545232\n",
            "[810]\ttrain-mlogloss:0.563198+0.00156014\ttest-mlogloss:0.672913+0.00543784\n",
            "[820]\ttrain-mlogloss:0.560645+0.00150268\ttest-mlogloss:0.671213+0.00543948\n",
            "[830]\ttrain-mlogloss:0.558107+0.00153989\ttest-mlogloss:0.669537+0.00541757\n",
            "[840]\ttrain-mlogloss:0.555655+0.00151051\ttest-mlogloss:0.667912+0.00543283\n",
            "[850]\ttrain-mlogloss:0.553199+0.00155193\ttest-mlogloss:0.66632+0.00538328\n",
            "[860]\ttrain-mlogloss:0.550751+0.00155523\ttest-mlogloss:0.664742+0.00538594\n",
            "[870]\ttrain-mlogloss:0.548409+0.00158031\ttest-mlogloss:0.663226+0.00533349\n",
            "[880]\ttrain-mlogloss:0.546115+0.00155994\ttest-mlogloss:0.661775+0.00533234\n",
            "[890]\ttrain-mlogloss:0.543825+0.00155262\ttest-mlogloss:0.660324+0.00531581\n",
            "[900]\ttrain-mlogloss:0.541617+0.00154392\ttest-mlogloss:0.658908+0.00529559\n",
            "[910]\ttrain-mlogloss:0.539374+0.001498\ttest-mlogloss:0.657502+0.00530102\n",
            "[920]\ttrain-mlogloss:0.5372+0.00150524\ttest-mlogloss:0.65615+0.0053181\n",
            "[930]\ttrain-mlogloss:0.535065+0.00150188\ttest-mlogloss:0.654786+0.00529168\n",
            "[940]\ttrain-mlogloss:0.532929+0.00154699\ttest-mlogloss:0.653467+0.00524664\n",
            "[950]\ttrain-mlogloss:0.53084+0.00158621\ttest-mlogloss:0.652202+0.00524043\n",
            "[960]\ttrain-mlogloss:0.528771+0.00157426\ttest-mlogloss:0.650922+0.00524724\n",
            "[970]\ttrain-mlogloss:0.526842+0.00157634\ttest-mlogloss:0.649746+0.00522708\n",
            "[980]\ttrain-mlogloss:0.524831+0.00157338\ttest-mlogloss:0.648527+0.00523981\n",
            "[990]\ttrain-mlogloss:0.522858+0.00156824\ttest-mlogloss:0.64734+0.00523608\n",
            "[998]\ttrain-mlogloss:0.52127+0.001571\ttest-mlogloss:0.646381+0.00524224\n",
            "Wall time: 18min 52s\n",
            "\tmlogloss 0.6463808 for 998 rounds\n",
            "\n",
            "CV with eta=0.005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0]\ttrain-mlogloss:2.19285+4.51752e-05\ttest-mlogloss:2.19298+4.42421e-05\n",
            "[10]\ttrain-mlogloss:2.15004+0.000259458\ttest-mlogloss:2.15129+0.000235964\n",
            "[20]\ttrain-mlogloss:2.10813+0.000307317\ttest-mlogloss:2.11056+0.000247996\n",
            "[30]\ttrain-mlogloss:2.06674+0.000471299\ttest-mlogloss:2.07027+0.000521665\n",
            "[40]\ttrain-mlogloss:2.02623+0.000699834\ttest-mlogloss:2.03091+0.00073056\n",
            "[50]\ttrain-mlogloss:1.98625+0.000844118\ttest-mlogloss:1.99206+0.00113689\n",
            "[60]\ttrain-mlogloss:1.94738+0.000828416\ttest-mlogloss:1.95436+0.00123577\n",
            "[70]\ttrain-mlogloss:1.90925+0.00101785\ttest-mlogloss:1.91741+0.00132619\n",
            "[80]\ttrain-mlogloss:1.87197+0.00124584\ttest-mlogloss:1.88127+0.00152795\n",
            "[90]\ttrain-mlogloss:1.83553+0.0011437\ttest-mlogloss:1.84591+0.00160609\n",
            "[100]\ttrain-mlogloss:1.8+0.00115591\ttest-mlogloss:1.81148+0.00176789\n",
            "[110]\ttrain-mlogloss:1.76481+0.00121152\ttest-mlogloss:1.7774+0.00205016\n",
            "[120]\ttrain-mlogloss:1.7305+0.00115036\ttest-mlogloss:1.7442+0.00229978\n",
            "[130]\ttrain-mlogloss:1.69702+0.0010787\ttest-mlogloss:1.71183+0.00267434\n",
            "[140]\ttrain-mlogloss:1.66425+0.000804648\ttest-mlogloss:1.6802+0.00278738\n",
            "[150]\ttrain-mlogloss:1.63241+0.00089742\ttest-mlogloss:1.6495+0.00302623\n",
            "[160]\ttrain-mlogloss:1.60114+0.000897966\ttest-mlogloss:1.6193+0.00317548\n",
            "[170]\ttrain-mlogloss:1.57097+0.000739689\ttest-mlogloss:1.59021+0.00319017\n",
            "[180]\ttrain-mlogloss:1.54133+0.0007312\ttest-mlogloss:1.56166+0.00340299\n",
            "[190]\ttrain-mlogloss:1.51247+0.000582931\ttest-mlogloss:1.53387+0.00336896\n",
            "[200]\ttrain-mlogloss:1.48465+0.000460557\ttest-mlogloss:1.50711+0.00352181\n",
            "[210]\ttrain-mlogloss:1.45744+0.000440731\ttest-mlogloss:1.48098+0.0035035\n",
            "[220]\ttrain-mlogloss:1.43086+0.000396918\ttest-mlogloss:1.45547+0.00349721\n",
            "[230]\ttrain-mlogloss:1.40502+0.000332564\ttest-mlogloss:1.43068+0.00353673\n",
            "[240]\ttrain-mlogloss:1.37982+0.000351609\ttest-mlogloss:1.40653+0.00378094\n",
            "[250]\ttrain-mlogloss:1.3558+0.000360137\ttest-mlogloss:1.38359+0.00386402\n",
            "[260]\ttrain-mlogloss:1.33211+0.000253107\ttest-mlogloss:1.36093+0.00399141\n",
            "[270]\ttrain-mlogloss:1.30942+0.00037818\ttest-mlogloss:1.33931+0.00406038\n",
            "[280]\ttrain-mlogloss:1.28737+0.000330272\ttest-mlogloss:1.31824+0.00408256\n",
            "[290]\ttrain-mlogloss:1.2659+0.000442747\ttest-mlogloss:1.29774+0.00404035\n",
            "[300]\ttrain-mlogloss:1.24524+0.000538953\ttest-mlogloss:1.27807+0.00392352\n",
            "[310]\ttrain-mlogloss:1.22522+0.000570755\ttest-mlogloss:1.25902+0.00396122\n",
            "[320]\ttrain-mlogloss:1.20595+0.000789004\ttest-mlogloss:1.24072+0.00380116\n",
            "[330]\ttrain-mlogloss:1.1874+0.000830835\ttest-mlogloss:1.2231+0.00384973\n",
            "[340]\ttrain-mlogloss:1.16976+0.00078443\ttest-mlogloss:1.20637+0.00399668\n",
            "[350]\ttrain-mlogloss:1.15268+0.000846694\ttest-mlogloss:1.19018+0.00394619\n",
            "[360]\ttrain-mlogloss:1.1362+0.000913267\ttest-mlogloss:1.1746+0.00395445\n",
            "[370]\ttrain-mlogloss:1.1206+0.000990643\ttest-mlogloss:1.15988+0.00394764\n",
            "[380]\ttrain-mlogloss:1.10537+0.000931609\ttest-mlogloss:1.14552+0.0039918\n",
            "[390]\ttrain-mlogloss:1.09076+0.0008646\ttest-mlogloss:1.13181+0.0040224\n",
            "[400]\ttrain-mlogloss:1.07672+0.000887603\ttest-mlogloss:1.11861+0.0040925\n",
            "[410]\ttrain-mlogloss:1.06304+0.00102718\ttest-mlogloss:1.10578+0.00399973\n",
            "[420]\ttrain-mlogloss:1.04995+0.00109554\ttest-mlogloss:1.09352+0.00397505\n",
            "[430]\ttrain-mlogloss:1.03721+0.000951777\ttest-mlogloss:1.08159+0.00417669\n",
            "[440]\ttrain-mlogloss:1.02492+0.00100714\ttest-mlogloss:1.07012+0.00419948\n",
            "[450]\ttrain-mlogloss:1.01313+0.000923625\ttest-mlogloss:1.05911+0.00430591\n",
            "[460]\ttrain-mlogloss:1.00169+0.000883498\ttest-mlogloss:1.04845+0.00439463\n",
            "[470]\ttrain-mlogloss:0.990685+0.00084738\ttest-mlogloss:1.03821+0.00451586\n",
            "[480]\ttrain-mlogloss:0.979898+0.000846016\ttest-mlogloss:1.02815+0.00455514\n",
            "[490]\ttrain-mlogloss:0.969542+0.000900982\ttest-mlogloss:1.01857+0.00453196\n",
            "[500]\ttrain-mlogloss:0.959422+0.000854028\ttest-mlogloss:1.00917+0.00458088\n",
            "[510]\ttrain-mlogloss:0.94969+0.000889633\ttest-mlogloss:1.00017+0.00458776\n",
            "[520]\ttrain-mlogloss:0.940164+0.000913998\ttest-mlogloss:0.991346+0.00462213\n",
            "[530]\ttrain-mlogloss:0.930877+0.00100341\ttest-mlogloss:0.982789+0.00457347\n",
            "[540]\ttrain-mlogloss:0.921964+0.000956026\ttest-mlogloss:0.974565+0.00465573\n",
            "[550]\ttrain-mlogloss:0.913252+0.000920009\ttest-mlogloss:0.966526+0.00473276\n",
            "[560]\ttrain-mlogloss:0.904814+0.000897014\ttest-mlogloss:0.958759+0.0047603\n",
            "[570]\ttrain-mlogloss:0.896672+0.000951662\ttest-mlogloss:0.951283+0.00470957\n",
            "[580]\ttrain-mlogloss:0.888803+0.00100832\ttest-mlogloss:0.944094+0.00468405\n",
            "[590]\ttrain-mlogloss:0.881039+0.00108119\ttest-mlogloss:0.936983+0.00468067\n",
            "[600]\ttrain-mlogloss:0.8735+0.00108112\ttest-mlogloss:0.930096+0.00469357\n",
            "[610]\ttrain-mlogloss:0.86624+0.00107459\ttest-mlogloss:0.923495+0.00471941\n",
            "[620]\ttrain-mlogloss:0.859123+0.00110535\ttest-mlogloss:0.91705+0.00473859\n",
            "[630]\ttrain-mlogloss:0.852268+0.00113046\ttest-mlogloss:0.910816+0.00473917\n",
            "[640]\ttrain-mlogloss:0.845569+0.00114457\ttest-mlogloss:0.904745+0.00475862\n",
            "[650]\ttrain-mlogloss:0.839035+0.00117794\ttest-mlogloss:0.898833+0.00481798\n",
            "[660]\ttrain-mlogloss:0.832684+0.00111116\ttest-mlogloss:0.893097+0.00492917\n",
            "[670]\ttrain-mlogloss:0.826512+0.00111782\ttest-mlogloss:0.887524+0.00496199\n",
            "[680]\ttrain-mlogloss:0.820472+0.0011103\ttest-mlogloss:0.88211+0.00504202\n",
            "[690]\ttrain-mlogloss:0.814561+0.00109738\ttest-mlogloss:0.876832+0.00509915\n",
            "[700]\ttrain-mlogloss:0.808838+0.00111077\ttest-mlogloss:0.871742+0.00508341\n",
            "[710]\ttrain-mlogloss:0.803248+0.00109572\ttest-mlogloss:0.866768+0.00515516\n",
            "[720]\ttrain-mlogloss:0.797714+0.00105616\ttest-mlogloss:0.861863+0.0052458\n",
            "[730]\ttrain-mlogloss:0.792308+0.00102463\ttest-mlogloss:0.857066+0.00530114\n",
            "[740]\ttrain-mlogloss:0.787129+0.00106308\ttest-mlogloss:0.852519+0.00531057\n",
            "[750]\ttrain-mlogloss:0.782074+0.0011322\ttest-mlogloss:0.848068+0.0052489\n",
            "[760]\ttrain-mlogloss:0.777066+0.00111138\ttest-mlogloss:0.843659+0.00527585\n",
            "[770]\ttrain-mlogloss:0.772238+0.00103002\ttest-mlogloss:0.839428+0.00536556\n",
            "[780]\ttrain-mlogloss:0.767545+0.00106931\ttest-mlogloss:0.835338+0.005376\n",
            "[790]\ttrain-mlogloss:0.762918+0.00111435\ttest-mlogloss:0.831305+0.00535242\n",
            "[800]\ttrain-mlogloss:0.758399+0.00114778\ttest-mlogloss:0.827358+0.00531677\n",
            "[810]\ttrain-mlogloss:0.753956+0.00112552\ttest-mlogloss:0.823464+0.00538616\n",
            "[820]\ttrain-mlogloss:0.749595+0.001099\ttest-mlogloss:0.819682+0.00544719\n",
            "[830]\ttrain-mlogloss:0.745319+0.00108414\ttest-mlogloss:0.816002+0.00546448\n",
            "[840]\ttrain-mlogloss:0.741103+0.00103304\ttest-mlogloss:0.812366+0.00554566\n",
            "[850]\ttrain-mlogloss:0.737033+0.001041\ttest-mlogloss:0.808857+0.00555114\n",
            "[860]\ttrain-mlogloss:0.733006+0.00104277\ttest-mlogloss:0.805393+0.00554896\n",
            "[870]\ttrain-mlogloss:0.729069+0.001029\ttest-mlogloss:0.802056+0.00557301\n",
            "[880]\ttrain-mlogloss:0.725229+0.00104471\ttest-mlogloss:0.798777+0.00555886\n",
            "[890]\ttrain-mlogloss:0.721439+0.0010653\ttest-mlogloss:0.795547+0.00556049\n",
            "[900]\ttrain-mlogloss:0.717681+0.00107338\ttest-mlogloss:0.792379+0.00559389\n",
            "[910]\ttrain-mlogloss:0.714021+0.0010163\ttest-mlogloss:0.789271+0.00563314\n",
            "[920]\ttrain-mlogloss:0.710433+0.00105293\ttest-mlogloss:0.786249+0.00564177\n",
            "[930]\ttrain-mlogloss:0.706926+0.00105489\ttest-mlogloss:0.7833+0.00566633\n",
            "[940]\ttrain-mlogloss:0.703508+0.00106645\ttest-mlogloss:0.780434+0.00565564\n",
            "[950]\ttrain-mlogloss:0.700088+0.00106499\ttest-mlogloss:0.777577+0.00566627\n",
            "[960]\ttrain-mlogloss:0.696771+0.00110108\ttest-mlogloss:0.774797+0.00566173\n",
            "[970]\ttrain-mlogloss:0.693552+0.0011258\ttest-mlogloss:0.772147+0.0056217\n",
            "[980]\ttrain-mlogloss:0.690364+0.00112068\ttest-mlogloss:0.769496+0.00561231\n",
            "[990]\ttrain-mlogloss:0.687222+0.00109723\ttest-mlogloss:0.766909+0.00565637\n",
            "[998]\ttrain-mlogloss:0.68476+0.00109458\ttest-mlogloss:0.764886+0.00565925\n",
            "Wall time: 18min 54s\n",
            "\tmlogloss 0.7648858 for 998 rounds\n",
            "\n",
            "Best params: 0.05, Mmlogloss: 0.5653279999999999\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp4rDoCSybGX",
        "colab_type": "text"
      },
      "source": [
        "Update eta = 0.1 \n",
        "+ early stopping = 20 => Test-mlogloss:0.551874\n",
        "+ early_stopping_rounds=50 => Test-mlogloss:0.551874\n",
        "\n",
        "Increasing early spotting doesnt improve so just hang with 20."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9MbsZPsybGZ",
        "colab_type": "code",
        "colab": {},
        "outputId": "4b941606-1d87-4820-bea2-ea7851924175"
      },
      "source": [
        "params = {\n",
        "    'eta': 0.1,  \n",
        "    'n_estimators': 1000,\n",
        "    'silent': True,  # option for logging\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass tasks\n",
        "    'num_class': 9,  # number of classes to predic\n",
        "    'max_depth': 5,  # depth of the trees in the boosting process\n",
        "    'seed': 42,\n",
        "    'eval_metric': 'mlogloss', #cross-entropy is chosen as evaluation metric\n",
        "    'min_child_weight':4,\n",
        "    'gamma':0.0,\n",
        "    'subsample':0.9,\n",
        "    'colsample_bytree':0.8,\n",
        "    'nthread': 16,\n",
        "    #'scale_pos_weight':1,\n",
        "    'reg_alpha':0,\n",
        "    'max_delta_step':1\n",
        "    \n",
        "    }\n",
        "num_round = 50\n",
        "\n",
        "#training the model\n",
        "#model = xgb.train(params, dtrainR, num_round) #change dtrainR w/ dtrain\n",
        "\n",
        "num_boost_round = 999\n",
        "model = xgb.train(\n",
        "    params,\n",
        "    dtrain,#change dtrainR w/ dtrain\n",
        "    num_boost_round=num_boost_round,\n",
        "    evals=[(dtrain, \"Train\"),(dval, \"Test\")],\n",
        "    early_stopping_rounds=20\n",
        ")\n",
        "\n",
        "probsTrain = model.predict(dtrain)\n",
        "probsVal = model.predict(dval)\n",
        "print(\"Training Class Probabilities for First 5 Instances:\\n\",probsTrain[:5])\n",
        "print(\"Validation Class Probabilities for First 5 Instances:\\n\",probsVal[:5])\n",
        "best_predsTrain = np.asarray([np.argmax(line) for line in probsTrain])\n",
        "best_predsVal = np.asarray([np.argmax(line) for line in probsVal])\n",
        "\n",
        "print(\"Best Predictions for Train:\\n\", best_predsTrain+1)\n",
        "print(\"Best Predictions for Validation:\\n\", best_predsVal+1)\n",
        "#print(min(best_preds+1),max(best_preds+1))\n",
        "\n",
        "print(\"Precision Score of the Training Set= \",precision_score(y_train, best_predsTrain, average='macro'))#change y_train(y_val) to y_trainR(y_valR)\n",
        "print(\"Precision Score of the Validation Set= \",precision_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Recall Score of the Training Set= \",recall_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"Recall Score of the Validation Set= \",recall_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"F1 Score of the Training Set= \",f1_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"F1 Score of the Validation Set= \",f1_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Accuracy Score the Training Set= \", accuracy_score(y_train, best_predsTrain))\n",
        "print(\"Accuracy Score of the Validation Set= \", accuracy_score(y_val, best_predsVal))\n",
        "scoreTrain = log_loss(y_train, probsTrain)\n",
        "scoreVal = log_loss(y_val, probsVal)\n",
        "print(\"Logloss Score Training Set= \", scoreTrain)\n",
        "print(\"Logloss Score of the Validation Set= \", scoreVal)\n",
        "\n",
        "print(\"Confusion Matrix of the Training Set: \\n\")\n",
        "print(confusion_matrix(y_train, best_predsTrain))\n",
        "print(\"Confusion Matrix of the Validation Set: \\n\")\n",
        "print(confusion_matrix(y_val,best_predsVal))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\tTrain-mlogloss:2.10964\tTest-mlogloss:2.11161\n",
            "Multiple eval metrics have been passed: 'Test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until Test-mlogloss hasn't improved in 50 rounds.\n",
            "[1]\tTrain-mlogloss:2.02979\tTest-mlogloss:2.03341\n",
            "[2]\tTrain-mlogloss:1.95173\tTest-mlogloss:1.95688\n",
            "[3]\tTrain-mlogloss:1.87651\tTest-mlogloss:1.88348\n",
            "[4]\tTrain-mlogloss:1.80375\tTest-mlogloss:1.81224\n",
            "[5]\tTrain-mlogloss:1.73657\tTest-mlogloss:1.7471\n",
            "[6]\tTrain-mlogloss:1.66844\tTest-mlogloss:1.68091\n",
            "[7]\tTrain-mlogloss:1.60582\tTest-mlogloss:1.61971\n",
            "[8]\tTrain-mlogloss:1.54458\tTest-mlogloss:1.56074\n",
            "[9]\tTrain-mlogloss:1.48618\tTest-mlogloss:1.50401\n",
            "[10]\tTrain-mlogloss:1.43183\tTest-mlogloss:1.45128\n",
            "[11]\tTrain-mlogloss:1.38002\tTest-mlogloss:1.40072\n",
            "[12]\tTrain-mlogloss:1.33528\tTest-mlogloss:1.35802\n",
            "[13]\tTrain-mlogloss:1.29138\tTest-mlogloss:1.31581\n",
            "[14]\tTrain-mlogloss:1.25241\tTest-mlogloss:1.27814\n",
            "[15]\tTrain-mlogloss:1.21773\tTest-mlogloss:1.24527\n",
            "[16]\tTrain-mlogloss:1.18028\tTest-mlogloss:1.20962\n",
            "[17]\tTrain-mlogloss:1.14661\tTest-mlogloss:1.17758\n",
            "[18]\tTrain-mlogloss:1.11445\tTest-mlogloss:1.14677\n",
            "[19]\tTrain-mlogloss:1.08442\tTest-mlogloss:1.11811\n",
            "[20]\tTrain-mlogloss:1.05704\tTest-mlogloss:1.09173\n",
            "[21]\tTrain-mlogloss:1.0324\tTest-mlogloss:1.06837\n",
            "[22]\tTrain-mlogloss:1.01055\tTest-mlogloss:1.04732\n",
            "[23]\tTrain-mlogloss:0.989829\tTest-mlogloss:1.0278\n",
            "[24]\tTrain-mlogloss:0.969199\tTest-mlogloss:1.00849\n",
            "[25]\tTrain-mlogloss:0.949144\tTest-mlogloss:0.989542\n",
            "[26]\tTrain-mlogloss:0.930561\tTest-mlogloss:0.972062\n",
            "[27]\tTrain-mlogloss:0.914805\tTest-mlogloss:0.957343\n",
            "[28]\tTrain-mlogloss:0.898269\tTest-mlogloss:0.941397\n",
            "[29]\tTrain-mlogloss:0.882564\tTest-mlogloss:0.926436\n",
            "[30]\tTrain-mlogloss:0.867614\tTest-mlogloss:0.912394\n",
            "[31]\tTrain-mlogloss:0.853748\tTest-mlogloss:0.89934\n",
            "[32]\tTrain-mlogloss:0.842024\tTest-mlogloss:0.888712\n",
            "[33]\tTrain-mlogloss:0.828996\tTest-mlogloss:0.876831\n",
            "[34]\tTrain-mlogloss:0.816862\tTest-mlogloss:0.865636\n",
            "[35]\tTrain-mlogloss:0.806087\tTest-mlogloss:0.855428\n",
            "[36]\tTrain-mlogloss:0.795928\tTest-mlogloss:0.846136\n",
            "[37]\tTrain-mlogloss:0.785815\tTest-mlogloss:0.837541\n",
            "[38]\tTrain-mlogloss:0.776595\tTest-mlogloss:0.829502\n",
            "[39]\tTrain-mlogloss:0.767324\tTest-mlogloss:0.821593\n",
            "[40]\tTrain-mlogloss:0.758649\tTest-mlogloss:0.8142\n",
            "[41]\tTrain-mlogloss:0.750614\tTest-mlogloss:0.807431\n",
            "[42]\tTrain-mlogloss:0.742156\tTest-mlogloss:0.799876\n",
            "[43]\tTrain-mlogloss:0.734815\tTest-mlogloss:0.793626\n",
            "[44]\tTrain-mlogloss:0.727647\tTest-mlogloss:0.787329\n",
            "[45]\tTrain-mlogloss:0.721596\tTest-mlogloss:0.782299\n",
            "[46]\tTrain-mlogloss:0.715131\tTest-mlogloss:0.77694\n",
            "[47]\tTrain-mlogloss:0.708206\tTest-mlogloss:0.770895\n",
            "[48]\tTrain-mlogloss:0.701462\tTest-mlogloss:0.765034\n",
            "[49]\tTrain-mlogloss:0.69542\tTest-mlogloss:0.7597\n",
            "[50]\tTrain-mlogloss:0.689685\tTest-mlogloss:0.754904\n",
            "[51]\tTrain-mlogloss:0.683894\tTest-mlogloss:0.750013\n",
            "[52]\tTrain-mlogloss:0.678843\tTest-mlogloss:0.746012\n",
            "[53]\tTrain-mlogloss:0.67336\tTest-mlogloss:0.74181\n",
            "[54]\tTrain-mlogloss:0.668624\tTest-mlogloss:0.737817\n",
            "[55]\tTrain-mlogloss:0.66394\tTest-mlogloss:0.733995\n",
            "[56]\tTrain-mlogloss:0.659187\tTest-mlogloss:0.730151\n",
            "[57]\tTrain-mlogloss:0.654889\tTest-mlogloss:0.72688\n",
            "[58]\tTrain-mlogloss:0.650411\tTest-mlogloss:0.723068\n",
            "[59]\tTrain-mlogloss:0.646051\tTest-mlogloss:0.719757\n",
            "[60]\tTrain-mlogloss:0.641652\tTest-mlogloss:0.716439\n",
            "[61]\tTrain-mlogloss:0.637441\tTest-mlogloss:0.71306\n",
            "[62]\tTrain-mlogloss:0.633686\tTest-mlogloss:0.709869\n",
            "[63]\tTrain-mlogloss:0.629852\tTest-mlogloss:0.707135\n",
            "[64]\tTrain-mlogloss:0.626341\tTest-mlogloss:0.704364\n",
            "[65]\tTrain-mlogloss:0.622664\tTest-mlogloss:0.701661\n",
            "[66]\tTrain-mlogloss:0.618972\tTest-mlogloss:0.698796\n",
            "[67]\tTrain-mlogloss:0.615758\tTest-mlogloss:0.696526\n",
            "[68]\tTrain-mlogloss:0.612359\tTest-mlogloss:0.693798\n",
            "[69]\tTrain-mlogloss:0.609249\tTest-mlogloss:0.691429\n",
            "[70]\tTrain-mlogloss:0.606075\tTest-mlogloss:0.689283\n",
            "[71]\tTrain-mlogloss:0.602812\tTest-mlogloss:0.686903\n",
            "[72]\tTrain-mlogloss:0.599878\tTest-mlogloss:0.684778\n",
            "[73]\tTrain-mlogloss:0.596798\tTest-mlogloss:0.682582\n",
            "[74]\tTrain-mlogloss:0.594133\tTest-mlogloss:0.680698\n",
            "[75]\tTrain-mlogloss:0.591036\tTest-mlogloss:0.678485\n",
            "[76]\tTrain-mlogloss:0.588352\tTest-mlogloss:0.676448\n",
            "[77]\tTrain-mlogloss:0.58583\tTest-mlogloss:0.674461\n",
            "[78]\tTrain-mlogloss:0.5833\tTest-mlogloss:0.672675\n",
            "[79]\tTrain-mlogloss:0.580758\tTest-mlogloss:0.671061\n",
            "[80]\tTrain-mlogloss:0.57792\tTest-mlogloss:0.669371\n",
            "[81]\tTrain-mlogloss:0.575589\tTest-mlogloss:0.667687\n",
            "[82]\tTrain-mlogloss:0.573194\tTest-mlogloss:0.666027\n",
            "[83]\tTrain-mlogloss:0.570926\tTest-mlogloss:0.664324\n",
            "[84]\tTrain-mlogloss:0.568574\tTest-mlogloss:0.662628\n",
            "[85]\tTrain-mlogloss:0.566097\tTest-mlogloss:0.661111\n",
            "[86]\tTrain-mlogloss:0.563718\tTest-mlogloss:0.659328\n",
            "[87]\tTrain-mlogloss:0.561949\tTest-mlogloss:0.658038\n",
            "[88]\tTrain-mlogloss:0.559561\tTest-mlogloss:0.656386\n",
            "[89]\tTrain-mlogloss:0.557433\tTest-mlogloss:0.654988\n",
            "[90]\tTrain-mlogloss:0.555426\tTest-mlogloss:0.653674\n",
            "[91]\tTrain-mlogloss:0.553246\tTest-mlogloss:0.652208\n",
            "[92]\tTrain-mlogloss:0.550825\tTest-mlogloss:0.65064\n",
            "[93]\tTrain-mlogloss:0.548697\tTest-mlogloss:0.649275\n",
            "[94]\tTrain-mlogloss:0.546673\tTest-mlogloss:0.647879\n",
            "[95]\tTrain-mlogloss:0.544481\tTest-mlogloss:0.64659\n",
            "[96]\tTrain-mlogloss:0.542472\tTest-mlogloss:0.645334\n",
            "[97]\tTrain-mlogloss:0.540263\tTest-mlogloss:0.643829\n",
            "[98]\tTrain-mlogloss:0.538326\tTest-mlogloss:0.642659\n",
            "[99]\tTrain-mlogloss:0.536289\tTest-mlogloss:0.641632\n",
            "[100]\tTrain-mlogloss:0.534873\tTest-mlogloss:0.640869\n",
            "[101]\tTrain-mlogloss:0.533066\tTest-mlogloss:0.639762\n",
            "[102]\tTrain-mlogloss:0.531701\tTest-mlogloss:0.639108\n",
            "[103]\tTrain-mlogloss:0.530191\tTest-mlogloss:0.638137\n",
            "[104]\tTrain-mlogloss:0.528615\tTest-mlogloss:0.637155\n",
            "[105]\tTrain-mlogloss:0.526901\tTest-mlogloss:0.636487\n",
            "[106]\tTrain-mlogloss:0.525001\tTest-mlogloss:0.635284\n",
            "[107]\tTrain-mlogloss:0.523474\tTest-mlogloss:0.634407\n",
            "[108]\tTrain-mlogloss:0.521757\tTest-mlogloss:0.633227\n",
            "[109]\tTrain-mlogloss:0.520129\tTest-mlogloss:0.632017\n",
            "[110]\tTrain-mlogloss:0.518227\tTest-mlogloss:0.631096\n",
            "[111]\tTrain-mlogloss:0.516759\tTest-mlogloss:0.630321\n",
            "[112]\tTrain-mlogloss:0.515418\tTest-mlogloss:0.629541\n",
            "[113]\tTrain-mlogloss:0.5137\tTest-mlogloss:0.628616\n",
            "[114]\tTrain-mlogloss:0.512165\tTest-mlogloss:0.627842\n",
            "[115]\tTrain-mlogloss:0.5106\tTest-mlogloss:0.626831\n",
            "[116]\tTrain-mlogloss:0.509025\tTest-mlogloss:0.625995\n",
            "[117]\tTrain-mlogloss:0.507627\tTest-mlogloss:0.625339\n",
            "[118]\tTrain-mlogloss:0.506121\tTest-mlogloss:0.624478\n",
            "[119]\tTrain-mlogloss:0.504384\tTest-mlogloss:0.623215\n",
            "[120]\tTrain-mlogloss:0.503099\tTest-mlogloss:0.622431\n",
            "[121]\tTrain-mlogloss:0.501553\tTest-mlogloss:0.621628\n",
            "[122]\tTrain-mlogloss:0.49999\tTest-mlogloss:0.620653\n",
            "[123]\tTrain-mlogloss:0.498772\tTest-mlogloss:0.620127\n",
            "[124]\tTrain-mlogloss:0.497164\tTest-mlogloss:0.619502\n",
            "[125]\tTrain-mlogloss:0.495862\tTest-mlogloss:0.618773\n",
            "[126]\tTrain-mlogloss:0.494456\tTest-mlogloss:0.617977\n",
            "[127]\tTrain-mlogloss:0.492939\tTest-mlogloss:0.617237\n",
            "[128]\tTrain-mlogloss:0.491323\tTest-mlogloss:0.616552\n",
            "[129]\tTrain-mlogloss:0.489556\tTest-mlogloss:0.615499\n",
            "[130]\tTrain-mlogloss:0.488243\tTest-mlogloss:0.614647\n",
            "[131]\tTrain-mlogloss:0.486965\tTest-mlogloss:0.613999\n",
            "[132]\tTrain-mlogloss:0.485872\tTest-mlogloss:0.613255\n",
            "[133]\tTrain-mlogloss:0.484561\tTest-mlogloss:0.6127\n",
            "[134]\tTrain-mlogloss:0.483305\tTest-mlogloss:0.611925\n",
            "[135]\tTrain-mlogloss:0.481831\tTest-mlogloss:0.611174\n",
            "[136]\tTrain-mlogloss:0.480649\tTest-mlogloss:0.610552\n",
            "[137]\tTrain-mlogloss:0.479404\tTest-mlogloss:0.609845\n",
            "[138]\tTrain-mlogloss:0.478242\tTest-mlogloss:0.609389\n",
            "[139]\tTrain-mlogloss:0.476984\tTest-mlogloss:0.608723\n",
            "[140]\tTrain-mlogloss:0.475696\tTest-mlogloss:0.607958\n",
            "[141]\tTrain-mlogloss:0.474514\tTest-mlogloss:0.607643\n",
            "[142]\tTrain-mlogloss:0.473297\tTest-mlogloss:0.607311\n",
            "[143]\tTrain-mlogloss:0.472081\tTest-mlogloss:0.606596\n",
            "[144]\tTrain-mlogloss:0.470898\tTest-mlogloss:0.605914\n",
            "[145]\tTrain-mlogloss:0.469641\tTest-mlogloss:0.605568\n",
            "[146]\tTrain-mlogloss:0.468461\tTest-mlogloss:0.605037\n",
            "[147]\tTrain-mlogloss:0.467087\tTest-mlogloss:0.604542\n",
            "[148]\tTrain-mlogloss:0.465768\tTest-mlogloss:0.603959\n",
            "[149]\tTrain-mlogloss:0.464698\tTest-mlogloss:0.603646\n",
            "[150]\tTrain-mlogloss:0.463591\tTest-mlogloss:0.603204\n",
            "[151]\tTrain-mlogloss:0.462636\tTest-mlogloss:0.60266\n",
            "[152]\tTrain-mlogloss:0.461374\tTest-mlogloss:0.602148\n",
            "[153]\tTrain-mlogloss:0.460366\tTest-mlogloss:0.601711\n",
            "[154]\tTrain-mlogloss:0.459469\tTest-mlogloss:0.601379\n",
            "[155]\tTrain-mlogloss:0.458484\tTest-mlogloss:0.600712\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[156]\tTrain-mlogloss:0.457524\tTest-mlogloss:0.600107\n",
            "[157]\tTrain-mlogloss:0.456454\tTest-mlogloss:0.599911\n",
            "[158]\tTrain-mlogloss:0.455516\tTest-mlogloss:0.599414\n",
            "[159]\tTrain-mlogloss:0.454496\tTest-mlogloss:0.599129\n",
            "[160]\tTrain-mlogloss:0.453562\tTest-mlogloss:0.598502\n",
            "[161]\tTrain-mlogloss:0.452507\tTest-mlogloss:0.598062\n",
            "[162]\tTrain-mlogloss:0.451379\tTest-mlogloss:0.597658\n",
            "[163]\tTrain-mlogloss:0.450362\tTest-mlogloss:0.597197\n",
            "[164]\tTrain-mlogloss:0.449133\tTest-mlogloss:0.596589\n",
            "[165]\tTrain-mlogloss:0.447705\tTest-mlogloss:0.595947\n",
            "[166]\tTrain-mlogloss:0.44664\tTest-mlogloss:0.59561\n",
            "[167]\tTrain-mlogloss:0.445438\tTest-mlogloss:0.59523\n",
            "[168]\tTrain-mlogloss:0.444249\tTest-mlogloss:0.594791\n",
            "[169]\tTrain-mlogloss:0.443265\tTest-mlogloss:0.594263\n",
            "[170]\tTrain-mlogloss:0.441986\tTest-mlogloss:0.593746\n",
            "[171]\tTrain-mlogloss:0.441\tTest-mlogloss:0.593494\n",
            "[172]\tTrain-mlogloss:0.439896\tTest-mlogloss:0.593044\n",
            "[173]\tTrain-mlogloss:0.4391\tTest-mlogloss:0.59268\n",
            "[174]\tTrain-mlogloss:0.438095\tTest-mlogloss:0.592313\n",
            "[175]\tTrain-mlogloss:0.437008\tTest-mlogloss:0.591959\n",
            "[176]\tTrain-mlogloss:0.435763\tTest-mlogloss:0.591374\n",
            "[177]\tTrain-mlogloss:0.434959\tTest-mlogloss:0.590976\n",
            "[178]\tTrain-mlogloss:0.434054\tTest-mlogloss:0.590593\n",
            "[179]\tTrain-mlogloss:0.433064\tTest-mlogloss:0.590115\n",
            "[180]\tTrain-mlogloss:0.432257\tTest-mlogloss:0.589775\n",
            "[181]\tTrain-mlogloss:0.431196\tTest-mlogloss:0.58935\n",
            "[182]\tTrain-mlogloss:0.430234\tTest-mlogloss:0.588779\n",
            "[183]\tTrain-mlogloss:0.429475\tTest-mlogloss:0.588437\n",
            "[184]\tTrain-mlogloss:0.428732\tTest-mlogloss:0.588105\n",
            "[185]\tTrain-mlogloss:0.427674\tTest-mlogloss:0.587568\n",
            "[186]\tTrain-mlogloss:0.426854\tTest-mlogloss:0.587365\n",
            "[187]\tTrain-mlogloss:0.426175\tTest-mlogloss:0.587021\n",
            "[188]\tTrain-mlogloss:0.42534\tTest-mlogloss:0.586651\n",
            "[189]\tTrain-mlogloss:0.423901\tTest-mlogloss:0.58603\n",
            "[190]\tTrain-mlogloss:0.42305\tTest-mlogloss:0.58578\n",
            "[191]\tTrain-mlogloss:0.422087\tTest-mlogloss:0.585422\n",
            "[192]\tTrain-mlogloss:0.420987\tTest-mlogloss:0.584986\n",
            "[193]\tTrain-mlogloss:0.420076\tTest-mlogloss:0.5847\n",
            "[194]\tTrain-mlogloss:0.419195\tTest-mlogloss:0.584552\n",
            "[195]\tTrain-mlogloss:0.418217\tTest-mlogloss:0.584305\n",
            "[196]\tTrain-mlogloss:0.417269\tTest-mlogloss:0.584029\n",
            "[197]\tTrain-mlogloss:0.416402\tTest-mlogloss:0.583915\n",
            "[198]\tTrain-mlogloss:0.415612\tTest-mlogloss:0.583529\n",
            "[199]\tTrain-mlogloss:0.415057\tTest-mlogloss:0.583365\n",
            "[200]\tTrain-mlogloss:0.414157\tTest-mlogloss:0.582933\n",
            "[201]\tTrain-mlogloss:0.413151\tTest-mlogloss:0.582556\n",
            "[202]\tTrain-mlogloss:0.412448\tTest-mlogloss:0.582217\n",
            "[203]\tTrain-mlogloss:0.411571\tTest-mlogloss:0.581903\n",
            "[204]\tTrain-mlogloss:0.410588\tTest-mlogloss:0.581585\n",
            "[205]\tTrain-mlogloss:0.409886\tTest-mlogloss:0.581127\n",
            "[206]\tTrain-mlogloss:0.409201\tTest-mlogloss:0.580943\n",
            "[207]\tTrain-mlogloss:0.408201\tTest-mlogloss:0.580658\n",
            "[208]\tTrain-mlogloss:0.407263\tTest-mlogloss:0.580099\n",
            "[209]\tTrain-mlogloss:0.406498\tTest-mlogloss:0.580068\n",
            "[210]\tTrain-mlogloss:0.405463\tTest-mlogloss:0.57972\n",
            "[211]\tTrain-mlogloss:0.404764\tTest-mlogloss:0.57947\n",
            "[212]\tTrain-mlogloss:0.403826\tTest-mlogloss:0.579104\n",
            "[213]\tTrain-mlogloss:0.403162\tTest-mlogloss:0.578989\n",
            "[214]\tTrain-mlogloss:0.40254\tTest-mlogloss:0.578843\n",
            "[215]\tTrain-mlogloss:0.401689\tTest-mlogloss:0.578483\n",
            "[216]\tTrain-mlogloss:0.40084\tTest-mlogloss:0.578224\n",
            "[217]\tTrain-mlogloss:0.39996\tTest-mlogloss:0.577951\n",
            "[218]\tTrain-mlogloss:0.399035\tTest-mlogloss:0.577619\n",
            "[219]\tTrain-mlogloss:0.398409\tTest-mlogloss:0.577526\n",
            "[220]\tTrain-mlogloss:0.39767\tTest-mlogloss:0.577302\n",
            "[221]\tTrain-mlogloss:0.39682\tTest-mlogloss:0.577082\n",
            "[222]\tTrain-mlogloss:0.39618\tTest-mlogloss:0.576835\n",
            "[223]\tTrain-mlogloss:0.395484\tTest-mlogloss:0.576583\n",
            "[224]\tTrain-mlogloss:0.394688\tTest-mlogloss:0.576306\n",
            "[225]\tTrain-mlogloss:0.393791\tTest-mlogloss:0.576013\n",
            "[226]\tTrain-mlogloss:0.392792\tTest-mlogloss:0.575522\n",
            "[227]\tTrain-mlogloss:0.392096\tTest-mlogloss:0.575216\n",
            "[228]\tTrain-mlogloss:0.390765\tTest-mlogloss:0.574565\n",
            "[229]\tTrain-mlogloss:0.390122\tTest-mlogloss:0.574378\n",
            "[230]\tTrain-mlogloss:0.389332\tTest-mlogloss:0.574084\n",
            "[231]\tTrain-mlogloss:0.388545\tTest-mlogloss:0.573807\n",
            "[232]\tTrain-mlogloss:0.387872\tTest-mlogloss:0.573614\n",
            "[233]\tTrain-mlogloss:0.387095\tTest-mlogloss:0.573371\n",
            "[234]\tTrain-mlogloss:0.3865\tTest-mlogloss:0.573056\n",
            "[235]\tTrain-mlogloss:0.385676\tTest-mlogloss:0.572761\n",
            "[236]\tTrain-mlogloss:0.384753\tTest-mlogloss:0.572329\n",
            "[237]\tTrain-mlogloss:0.383983\tTest-mlogloss:0.572007\n",
            "[238]\tTrain-mlogloss:0.383432\tTest-mlogloss:0.571855\n",
            "[239]\tTrain-mlogloss:0.382678\tTest-mlogloss:0.571501\n",
            "[240]\tTrain-mlogloss:0.381912\tTest-mlogloss:0.571342\n",
            "[241]\tTrain-mlogloss:0.381224\tTest-mlogloss:0.571204\n",
            "[242]\tTrain-mlogloss:0.380577\tTest-mlogloss:0.571125\n",
            "[243]\tTrain-mlogloss:0.379809\tTest-mlogloss:0.570852\n",
            "[244]\tTrain-mlogloss:0.379007\tTest-mlogloss:0.570536\n",
            "[245]\tTrain-mlogloss:0.378279\tTest-mlogloss:0.57047\n",
            "[246]\tTrain-mlogloss:0.3773\tTest-mlogloss:0.570226\n",
            "[247]\tTrain-mlogloss:0.376676\tTest-mlogloss:0.570031\n",
            "[248]\tTrain-mlogloss:0.376221\tTest-mlogloss:0.569863\n",
            "[249]\tTrain-mlogloss:0.375449\tTest-mlogloss:0.569667\n",
            "[250]\tTrain-mlogloss:0.374758\tTest-mlogloss:0.569585\n",
            "[251]\tTrain-mlogloss:0.374215\tTest-mlogloss:0.569506\n",
            "[252]\tTrain-mlogloss:0.373454\tTest-mlogloss:0.569262\n",
            "[253]\tTrain-mlogloss:0.37287\tTest-mlogloss:0.569025\n",
            "[254]\tTrain-mlogloss:0.37214\tTest-mlogloss:0.568752\n",
            "[255]\tTrain-mlogloss:0.371437\tTest-mlogloss:0.568539\n",
            "[256]\tTrain-mlogloss:0.370667\tTest-mlogloss:0.568215\n",
            "[257]\tTrain-mlogloss:0.369938\tTest-mlogloss:0.567949\n",
            "[258]\tTrain-mlogloss:0.369219\tTest-mlogloss:0.567698\n",
            "[259]\tTrain-mlogloss:0.368599\tTest-mlogloss:0.567589\n",
            "[260]\tTrain-mlogloss:0.36799\tTest-mlogloss:0.567393\n",
            "[261]\tTrain-mlogloss:0.36738\tTest-mlogloss:0.567474\n",
            "[262]\tTrain-mlogloss:0.366659\tTest-mlogloss:0.567013\n",
            "[263]\tTrain-mlogloss:0.366042\tTest-mlogloss:0.566843\n",
            "[264]\tTrain-mlogloss:0.365426\tTest-mlogloss:0.566747\n",
            "[265]\tTrain-mlogloss:0.364517\tTest-mlogloss:0.566377\n",
            "[266]\tTrain-mlogloss:0.364015\tTest-mlogloss:0.566301\n",
            "[267]\tTrain-mlogloss:0.36332\tTest-mlogloss:0.566142\n",
            "[268]\tTrain-mlogloss:0.362785\tTest-mlogloss:0.565936\n",
            "[269]\tTrain-mlogloss:0.361971\tTest-mlogloss:0.565689\n",
            "[270]\tTrain-mlogloss:0.361296\tTest-mlogloss:0.565557\n",
            "[271]\tTrain-mlogloss:0.360606\tTest-mlogloss:0.565364\n",
            "[272]\tTrain-mlogloss:0.359858\tTest-mlogloss:0.565313\n",
            "[273]\tTrain-mlogloss:0.359212\tTest-mlogloss:0.565121\n",
            "[274]\tTrain-mlogloss:0.358712\tTest-mlogloss:0.564904\n",
            "[275]\tTrain-mlogloss:0.358212\tTest-mlogloss:0.564819\n",
            "[276]\tTrain-mlogloss:0.357553\tTest-mlogloss:0.564653\n",
            "[277]\tTrain-mlogloss:0.356893\tTest-mlogloss:0.564415\n",
            "[278]\tTrain-mlogloss:0.356128\tTest-mlogloss:0.564205\n",
            "[279]\tTrain-mlogloss:0.355561\tTest-mlogloss:0.563947\n",
            "[280]\tTrain-mlogloss:0.354905\tTest-mlogloss:0.563773\n",
            "[281]\tTrain-mlogloss:0.354349\tTest-mlogloss:0.563641\n",
            "[282]\tTrain-mlogloss:0.353882\tTest-mlogloss:0.563509\n",
            "[283]\tTrain-mlogloss:0.353132\tTest-mlogloss:0.563304\n",
            "[284]\tTrain-mlogloss:0.352519\tTest-mlogloss:0.5631\n",
            "[285]\tTrain-mlogloss:0.351792\tTest-mlogloss:0.56288\n",
            "[286]\tTrain-mlogloss:0.351116\tTest-mlogloss:0.562806\n",
            "[287]\tTrain-mlogloss:0.35048\tTest-mlogloss:0.562696\n",
            "[288]\tTrain-mlogloss:0.350082\tTest-mlogloss:0.56254\n",
            "[289]\tTrain-mlogloss:0.349532\tTest-mlogloss:0.562266\n",
            "[290]\tTrain-mlogloss:0.348989\tTest-mlogloss:0.562116\n",
            "[291]\tTrain-mlogloss:0.348564\tTest-mlogloss:0.562153\n",
            "[292]\tTrain-mlogloss:0.34804\tTest-mlogloss:0.562065\n",
            "[293]\tTrain-mlogloss:0.347441\tTest-mlogloss:0.561737\n",
            "[294]\tTrain-mlogloss:0.346703\tTest-mlogloss:0.561602\n",
            "[295]\tTrain-mlogloss:0.345963\tTest-mlogloss:0.561577\n",
            "[296]\tTrain-mlogloss:0.345211\tTest-mlogloss:0.561341\n",
            "[297]\tTrain-mlogloss:0.344667\tTest-mlogloss:0.561287\n",
            "[298]\tTrain-mlogloss:0.34412\tTest-mlogloss:0.561006\n",
            "[299]\tTrain-mlogloss:0.343355\tTest-mlogloss:0.560935\n",
            "[300]\tTrain-mlogloss:0.342767\tTest-mlogloss:0.560665\n",
            "[301]\tTrain-mlogloss:0.342258\tTest-mlogloss:0.560607\n",
            "[302]\tTrain-mlogloss:0.34182\tTest-mlogloss:0.560658\n",
            "[303]\tTrain-mlogloss:0.341285\tTest-mlogloss:0.560793\n",
            "[304]\tTrain-mlogloss:0.340637\tTest-mlogloss:0.560863\n",
            "[305]\tTrain-mlogloss:0.339914\tTest-mlogloss:0.560662\n",
            "[306]\tTrain-mlogloss:0.339314\tTest-mlogloss:0.560556\n",
            "[307]\tTrain-mlogloss:0.338754\tTest-mlogloss:0.56032\n",
            "[308]\tTrain-mlogloss:0.338014\tTest-mlogloss:0.560036\n",
            "[309]\tTrain-mlogloss:0.337344\tTest-mlogloss:0.559923\n",
            "[310]\tTrain-mlogloss:0.33659\tTest-mlogloss:0.559818\n",
            "[311]\tTrain-mlogloss:0.335901\tTest-mlogloss:0.55981\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[312]\tTrain-mlogloss:0.335393\tTest-mlogloss:0.559803\n",
            "[313]\tTrain-mlogloss:0.334766\tTest-mlogloss:0.559606\n",
            "[314]\tTrain-mlogloss:0.334442\tTest-mlogloss:0.559535\n",
            "[315]\tTrain-mlogloss:0.333865\tTest-mlogloss:0.55956\n",
            "[316]\tTrain-mlogloss:0.333392\tTest-mlogloss:0.559454\n",
            "[317]\tTrain-mlogloss:0.332693\tTest-mlogloss:0.559406\n",
            "[318]\tTrain-mlogloss:0.332243\tTest-mlogloss:0.559332\n",
            "[319]\tTrain-mlogloss:0.331778\tTest-mlogloss:0.559316\n",
            "[320]\tTrain-mlogloss:0.331303\tTest-mlogloss:0.559342\n",
            "[321]\tTrain-mlogloss:0.330797\tTest-mlogloss:0.559189\n",
            "[322]\tTrain-mlogloss:0.330108\tTest-mlogloss:0.559026\n",
            "[323]\tTrain-mlogloss:0.329627\tTest-mlogloss:0.559099\n",
            "[324]\tTrain-mlogloss:0.329237\tTest-mlogloss:0.559032\n",
            "[325]\tTrain-mlogloss:0.328818\tTest-mlogloss:0.55904\n",
            "[326]\tTrain-mlogloss:0.328339\tTest-mlogloss:0.559106\n",
            "[327]\tTrain-mlogloss:0.327772\tTest-mlogloss:0.559025\n",
            "[328]\tTrain-mlogloss:0.327077\tTest-mlogloss:0.558947\n",
            "[329]\tTrain-mlogloss:0.326655\tTest-mlogloss:0.558832\n",
            "[330]\tTrain-mlogloss:0.326112\tTest-mlogloss:0.558564\n",
            "[331]\tTrain-mlogloss:0.325598\tTest-mlogloss:0.558397\n",
            "[332]\tTrain-mlogloss:0.324967\tTest-mlogloss:0.558224\n",
            "[333]\tTrain-mlogloss:0.324441\tTest-mlogloss:0.558155\n",
            "[334]\tTrain-mlogloss:0.324073\tTest-mlogloss:0.558004\n",
            "[335]\tTrain-mlogloss:0.323591\tTest-mlogloss:0.557718\n",
            "[336]\tTrain-mlogloss:0.323115\tTest-mlogloss:0.557629\n",
            "[337]\tTrain-mlogloss:0.322463\tTest-mlogloss:0.557567\n",
            "[338]\tTrain-mlogloss:0.321939\tTest-mlogloss:0.557639\n",
            "[339]\tTrain-mlogloss:0.321473\tTest-mlogloss:0.55748\n",
            "[340]\tTrain-mlogloss:0.321006\tTest-mlogloss:0.557248\n",
            "[341]\tTrain-mlogloss:0.32049\tTest-mlogloss:0.557231\n",
            "[342]\tTrain-mlogloss:0.319993\tTest-mlogloss:0.556976\n",
            "[343]\tTrain-mlogloss:0.319483\tTest-mlogloss:0.556761\n",
            "[344]\tTrain-mlogloss:0.31905\tTest-mlogloss:0.556724\n",
            "[345]\tTrain-mlogloss:0.318514\tTest-mlogloss:0.55673\n",
            "[346]\tTrain-mlogloss:0.317921\tTest-mlogloss:0.556779\n",
            "[347]\tTrain-mlogloss:0.317257\tTest-mlogloss:0.556685\n",
            "[348]\tTrain-mlogloss:0.31673\tTest-mlogloss:0.556888\n",
            "[349]\tTrain-mlogloss:0.316292\tTest-mlogloss:0.556737\n",
            "[350]\tTrain-mlogloss:0.315984\tTest-mlogloss:0.556762\n",
            "[351]\tTrain-mlogloss:0.315617\tTest-mlogloss:0.556762\n",
            "[352]\tTrain-mlogloss:0.315248\tTest-mlogloss:0.556804\n",
            "[353]\tTrain-mlogloss:0.314949\tTest-mlogloss:0.556805\n",
            "[354]\tTrain-mlogloss:0.314373\tTest-mlogloss:0.556671\n",
            "[355]\tTrain-mlogloss:0.31388\tTest-mlogloss:0.556722\n",
            "[356]\tTrain-mlogloss:0.313407\tTest-mlogloss:0.556727\n",
            "[357]\tTrain-mlogloss:0.312809\tTest-mlogloss:0.556711\n",
            "[358]\tTrain-mlogloss:0.312082\tTest-mlogloss:0.556754\n",
            "[359]\tTrain-mlogloss:0.311503\tTest-mlogloss:0.556642\n",
            "[360]\tTrain-mlogloss:0.311045\tTest-mlogloss:0.556488\n",
            "[361]\tTrain-mlogloss:0.310659\tTest-mlogloss:0.556386\n",
            "[362]\tTrain-mlogloss:0.310248\tTest-mlogloss:0.556412\n",
            "[363]\tTrain-mlogloss:0.30988\tTest-mlogloss:0.556539\n",
            "[364]\tTrain-mlogloss:0.309461\tTest-mlogloss:0.556617\n",
            "[365]\tTrain-mlogloss:0.309058\tTest-mlogloss:0.556464\n",
            "[366]\tTrain-mlogloss:0.30844\tTest-mlogloss:0.556334\n",
            "[367]\tTrain-mlogloss:0.307845\tTest-mlogloss:0.556148\n",
            "[368]\tTrain-mlogloss:0.307428\tTest-mlogloss:0.556213\n",
            "[369]\tTrain-mlogloss:0.307036\tTest-mlogloss:0.556223\n",
            "[370]\tTrain-mlogloss:0.306502\tTest-mlogloss:0.556162\n",
            "[371]\tTrain-mlogloss:0.305931\tTest-mlogloss:0.556123\n",
            "[372]\tTrain-mlogloss:0.305599\tTest-mlogloss:0.556146\n",
            "[373]\tTrain-mlogloss:0.304973\tTest-mlogloss:0.556145\n",
            "[374]\tTrain-mlogloss:0.304576\tTest-mlogloss:0.556047\n",
            "[375]\tTrain-mlogloss:0.304065\tTest-mlogloss:0.555982\n",
            "[376]\tTrain-mlogloss:0.303626\tTest-mlogloss:0.556167\n",
            "[377]\tTrain-mlogloss:0.303049\tTest-mlogloss:0.555933\n",
            "[378]\tTrain-mlogloss:0.302611\tTest-mlogloss:0.555732\n",
            "[379]\tTrain-mlogloss:0.302087\tTest-mlogloss:0.555713\n",
            "[380]\tTrain-mlogloss:0.301371\tTest-mlogloss:0.555639\n",
            "[381]\tTrain-mlogloss:0.300733\tTest-mlogloss:0.555372\n",
            "[382]\tTrain-mlogloss:0.300199\tTest-mlogloss:0.555405\n",
            "[383]\tTrain-mlogloss:0.299708\tTest-mlogloss:0.555179\n",
            "[384]\tTrain-mlogloss:0.299043\tTest-mlogloss:0.555125\n",
            "[385]\tTrain-mlogloss:0.298732\tTest-mlogloss:0.554899\n",
            "[386]\tTrain-mlogloss:0.298165\tTest-mlogloss:0.55484\n",
            "[387]\tTrain-mlogloss:0.297637\tTest-mlogloss:0.554698\n",
            "[388]\tTrain-mlogloss:0.29714\tTest-mlogloss:0.554578\n",
            "[389]\tTrain-mlogloss:0.296625\tTest-mlogloss:0.55436\n",
            "[390]\tTrain-mlogloss:0.296252\tTest-mlogloss:0.554412\n",
            "[391]\tTrain-mlogloss:0.295888\tTest-mlogloss:0.554338\n",
            "[392]\tTrain-mlogloss:0.295481\tTest-mlogloss:0.55429\n",
            "[393]\tTrain-mlogloss:0.295073\tTest-mlogloss:0.55416\n",
            "[394]\tTrain-mlogloss:0.294522\tTest-mlogloss:0.55387\n",
            "[395]\tTrain-mlogloss:0.293926\tTest-mlogloss:0.554013\n",
            "[396]\tTrain-mlogloss:0.293576\tTest-mlogloss:0.554077\n",
            "[397]\tTrain-mlogloss:0.293309\tTest-mlogloss:0.554111\n",
            "[398]\tTrain-mlogloss:0.292825\tTest-mlogloss:0.554151\n",
            "[399]\tTrain-mlogloss:0.292283\tTest-mlogloss:0.553991\n",
            "[400]\tTrain-mlogloss:0.291825\tTest-mlogloss:0.553827\n",
            "[401]\tTrain-mlogloss:0.29149\tTest-mlogloss:0.553723\n",
            "[402]\tTrain-mlogloss:0.290938\tTest-mlogloss:0.553667\n",
            "[403]\tTrain-mlogloss:0.290509\tTest-mlogloss:0.553614\n",
            "[404]\tTrain-mlogloss:0.289962\tTest-mlogloss:0.553436\n",
            "[405]\tTrain-mlogloss:0.289522\tTest-mlogloss:0.553481\n",
            "[406]\tTrain-mlogloss:0.289057\tTest-mlogloss:0.553241\n",
            "[407]\tTrain-mlogloss:0.288619\tTest-mlogloss:0.553214\n",
            "[408]\tTrain-mlogloss:0.288135\tTest-mlogloss:0.553131\n",
            "[409]\tTrain-mlogloss:0.287789\tTest-mlogloss:0.55322\n",
            "[410]\tTrain-mlogloss:0.287226\tTest-mlogloss:0.553086\n",
            "[411]\tTrain-mlogloss:0.286697\tTest-mlogloss:0.552947\n",
            "[412]\tTrain-mlogloss:0.286252\tTest-mlogloss:0.55287\n",
            "[413]\tTrain-mlogloss:0.285881\tTest-mlogloss:0.552921\n",
            "[414]\tTrain-mlogloss:0.285606\tTest-mlogloss:0.552866\n",
            "[415]\tTrain-mlogloss:0.285248\tTest-mlogloss:0.552903\n",
            "[416]\tTrain-mlogloss:0.284687\tTest-mlogloss:0.552895\n",
            "[417]\tTrain-mlogloss:0.284323\tTest-mlogloss:0.552892\n",
            "[418]\tTrain-mlogloss:0.283789\tTest-mlogloss:0.552925\n",
            "[419]\tTrain-mlogloss:0.283384\tTest-mlogloss:0.552873\n",
            "[420]\tTrain-mlogloss:0.283054\tTest-mlogloss:0.552889\n",
            "[421]\tTrain-mlogloss:0.282603\tTest-mlogloss:0.552841\n",
            "[422]\tTrain-mlogloss:0.282133\tTest-mlogloss:0.552776\n",
            "[423]\tTrain-mlogloss:0.281763\tTest-mlogloss:0.552834\n",
            "[424]\tTrain-mlogloss:0.281304\tTest-mlogloss:0.552822\n",
            "[425]\tTrain-mlogloss:0.280827\tTest-mlogloss:0.552885\n",
            "[426]\tTrain-mlogloss:0.280467\tTest-mlogloss:0.552906\n",
            "[427]\tTrain-mlogloss:0.280018\tTest-mlogloss:0.552979\n",
            "[428]\tTrain-mlogloss:0.279493\tTest-mlogloss:0.552802\n",
            "[429]\tTrain-mlogloss:0.27901\tTest-mlogloss:0.552589\n",
            "[430]\tTrain-mlogloss:0.278482\tTest-mlogloss:0.552549\n",
            "[431]\tTrain-mlogloss:0.278164\tTest-mlogloss:0.552558\n",
            "[432]\tTrain-mlogloss:0.277791\tTest-mlogloss:0.552454\n",
            "[433]\tTrain-mlogloss:0.277439\tTest-mlogloss:0.552532\n",
            "[434]\tTrain-mlogloss:0.27706\tTest-mlogloss:0.552512\n",
            "[435]\tTrain-mlogloss:0.276578\tTest-mlogloss:0.552371\n",
            "[436]\tTrain-mlogloss:0.276157\tTest-mlogloss:0.55241\n",
            "[437]\tTrain-mlogloss:0.275577\tTest-mlogloss:0.55242\n",
            "[438]\tTrain-mlogloss:0.27513\tTest-mlogloss:0.552454\n",
            "[439]\tTrain-mlogloss:0.274635\tTest-mlogloss:0.552331\n",
            "[440]\tTrain-mlogloss:0.274274\tTest-mlogloss:0.552363\n",
            "[441]\tTrain-mlogloss:0.273869\tTest-mlogloss:0.552375\n",
            "[442]\tTrain-mlogloss:0.273422\tTest-mlogloss:0.552207\n",
            "[443]\tTrain-mlogloss:0.273139\tTest-mlogloss:0.552287\n",
            "[444]\tTrain-mlogloss:0.272679\tTest-mlogloss:0.552141\n",
            "[445]\tTrain-mlogloss:0.272055\tTest-mlogloss:0.552038\n",
            "[446]\tTrain-mlogloss:0.271626\tTest-mlogloss:0.551954\n",
            "[447]\tTrain-mlogloss:0.271145\tTest-mlogloss:0.551874\n",
            "[448]\tTrain-mlogloss:0.270829\tTest-mlogloss:0.551906\n",
            "[449]\tTrain-mlogloss:0.27045\tTest-mlogloss:0.551986\n",
            "[450]\tTrain-mlogloss:0.270016\tTest-mlogloss:0.552038\n",
            "[451]\tTrain-mlogloss:0.269688\tTest-mlogloss:0.552088\n",
            "[452]\tTrain-mlogloss:0.269364\tTest-mlogloss:0.552143\n",
            "[453]\tTrain-mlogloss:0.268872\tTest-mlogloss:0.552187\n",
            "[454]\tTrain-mlogloss:0.268548\tTest-mlogloss:0.55212\n",
            "[455]\tTrain-mlogloss:0.268117\tTest-mlogloss:0.552135\n",
            "[456]\tTrain-mlogloss:0.267695\tTest-mlogloss:0.552084\n",
            "[457]\tTrain-mlogloss:0.267462\tTest-mlogloss:0.55202\n",
            "[458]\tTrain-mlogloss:0.267052\tTest-mlogloss:0.552071\n",
            "[459]\tTrain-mlogloss:0.266673\tTest-mlogloss:0.552233\n",
            "[460]\tTrain-mlogloss:0.266213\tTest-mlogloss:0.552301\n",
            "[461]\tTrain-mlogloss:0.265711\tTest-mlogloss:0.55211\n",
            "[462]\tTrain-mlogloss:0.265344\tTest-mlogloss:0.552019\n",
            "[463]\tTrain-mlogloss:0.265133\tTest-mlogloss:0.552092\n",
            "[464]\tTrain-mlogloss:0.264837\tTest-mlogloss:0.552121\n",
            "[465]\tTrain-mlogloss:0.264469\tTest-mlogloss:0.552063\n",
            "[466]\tTrain-mlogloss:0.264124\tTest-mlogloss:0.552151\n",
            "[467]\tTrain-mlogloss:0.263581\tTest-mlogloss:0.552225\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[468]\tTrain-mlogloss:0.263152\tTest-mlogloss:0.552315\n",
            "[469]\tTrain-mlogloss:0.262844\tTest-mlogloss:0.552447\n",
            "[470]\tTrain-mlogloss:0.262365\tTest-mlogloss:0.552329\n",
            "[471]\tTrain-mlogloss:0.262\tTest-mlogloss:0.552259\n",
            "[472]\tTrain-mlogloss:0.261595\tTest-mlogloss:0.552354\n",
            "[473]\tTrain-mlogloss:0.261236\tTest-mlogloss:0.552337\n",
            "[474]\tTrain-mlogloss:0.260797\tTest-mlogloss:0.552264\n",
            "[475]\tTrain-mlogloss:0.260497\tTest-mlogloss:0.552341\n",
            "[476]\tTrain-mlogloss:0.260117\tTest-mlogloss:0.552316\n",
            "[477]\tTrain-mlogloss:0.259797\tTest-mlogloss:0.552431\n",
            "[478]\tTrain-mlogloss:0.259431\tTest-mlogloss:0.552656\n",
            "[479]\tTrain-mlogloss:0.259008\tTest-mlogloss:0.552679\n",
            "[480]\tTrain-mlogloss:0.258684\tTest-mlogloss:0.552691\n",
            "[481]\tTrain-mlogloss:0.258304\tTest-mlogloss:0.552688\n",
            "[482]\tTrain-mlogloss:0.257977\tTest-mlogloss:0.552659\n",
            "[483]\tTrain-mlogloss:0.257601\tTest-mlogloss:0.552507\n",
            "[484]\tTrain-mlogloss:0.257254\tTest-mlogloss:0.552639\n",
            "[485]\tTrain-mlogloss:0.256849\tTest-mlogloss:0.552666\n",
            "[486]\tTrain-mlogloss:0.256486\tTest-mlogloss:0.552677\n",
            "[487]\tTrain-mlogloss:0.256078\tTest-mlogloss:0.552561\n",
            "[488]\tTrain-mlogloss:0.255699\tTest-mlogloss:0.552666\n",
            "[489]\tTrain-mlogloss:0.255438\tTest-mlogloss:0.55267\n",
            "[490]\tTrain-mlogloss:0.255156\tTest-mlogloss:0.552678\n",
            "[491]\tTrain-mlogloss:0.254742\tTest-mlogloss:0.552704\n",
            "[492]\tTrain-mlogloss:0.254367\tTest-mlogloss:0.552845\n",
            "[493]\tTrain-mlogloss:0.254117\tTest-mlogloss:0.552833\n",
            "[494]\tTrain-mlogloss:0.253813\tTest-mlogloss:0.55281\n",
            "[495]\tTrain-mlogloss:0.253347\tTest-mlogloss:0.552756\n",
            "[496]\tTrain-mlogloss:0.253095\tTest-mlogloss:0.552775\n",
            "[497]\tTrain-mlogloss:0.252745\tTest-mlogloss:0.552632\n",
            "Stopping. Best iteration:\n",
            "[447]\tTrain-mlogloss:0.271145\tTest-mlogloss:0.551874\n",
            "\n",
            "Training Class Probabilities for First 5 Instances:\n",
            " [[4.4219603e-05 2.1225909e-02 8.5261059e-01 3.3787191e-02 2.7568467e-08\n",
            "  7.0886956e-05 9.2216879e-02 3.9895633e-05 4.3748946e-06]\n",
            " [1.2478681e-04 6.6271758e-01 3.0772415e-01 2.7953332e-02 5.1406193e-08\n",
            "  1.3183797e-04 1.1238467e-03 1.5957066e-04 6.4900007e-05]\n",
            " [2.2087673e-04 5.1813257e-01 4.6243563e-01 8.0203423e-03 6.1552442e-04\n",
            "  1.2091803e-03 9.6514839e-04 7.1064495e-03 1.2942263e-03]\n",
            " [3.1512395e-02 7.9145543e-03 8.1063537e-03 4.4944901e-02 1.7388422e-05\n",
            "  7.6733106e-01 1.3313659e-01 2.5964847e-03 4.4402662e-03]\n",
            " [2.9424939e-03 3.4179029e-06 5.4771544e-06 1.3154313e-04 6.8506488e-08\n",
            "  9.9478805e-01 2.4930146e-04 6.5362814e-04 1.2260262e-03]]\n",
            "Validation Class Probabilities for First 5 Instances:\n",
            " [[1.8105314e-03 6.8309885e-01 1.7226091e-02 2.7604827e-01 3.2683169e-05\n",
            "  2.4962241e-03 2.1635329e-03 1.7070794e-03 1.5416765e-02]\n",
            " [1.6822316e-05 3.8147313e-04 1.2682630e-04 1.0072765e-03 9.9831283e-01\n",
            "  8.1603430e-06 4.9238988e-05 6.0402846e-05 3.6956106e-05]\n",
            " [3.9789261e-04 2.6416725e-01 3.0925483e-01 4.1451344e-01 9.8476030e-06\n",
            "  3.7147244e-04 8.4411595e-03 2.0137648e-03 8.3039451e-04]\n",
            " [8.1219214e-06 9.2886049e-01 3.8879469e-02 3.2087371e-02 8.7351856e-07\n",
            "  5.5689572e-05 7.1575319e-05 2.5928255e-05 1.0443423e-05]\n",
            " [4.3729365e-02 1.3196860e-02 5.6324424e-03 1.2677867e-03 1.6545584e-04\n",
            "  5.5580302e-03 1.4369524e-03 7.0474730e-03 9.2196566e-01]]\n",
            "Best Predictions for Train:\n",
            " [3 2 2 ... 2 6 6]\n",
            "Best Predictions for Validation:\n",
            " [2 5 4 ... 2 3 2]\n",
            "Precision Score of the Training Set=  0.8676077138046421\n",
            "Precision Score of the Validation Set=  0.7609528298390099\n",
            "Recall Score of the Training Set=  0.9241761575142812\n",
            "Recall Score of the Validation Set=  0.8024046342580875\n",
            "F1 Score of the Training Set=  0.890310711277205\n",
            "F1 Score of the Validation Set=  0.777448904663087\n",
            "Accuracy Score the Training Set=  0.8933982465354935\n",
            "Accuracy Score of the Validation Set=  0.803813833225598\n",
            "Logloss Score Training Set=  0.3095581815948562\n",
            "Logloss Score of the Validation Set=  0.5139817235747838\n",
            "Confusion Matrix of the Training Set: \n",
            "\n",
            "[[ 1502     0     1     0     0     3    15    10    12]\n",
            " [   29  9968  1963   681    10    10   188    18    31]\n",
            " [    7   636  5295   331     0     1   127     4     2]\n",
            " [    4    81    87  1968     0     1    12     0     0]\n",
            " [    1     0     0     0  2189     0     1     0     0]\n",
            " [  122    20    15    26     0 10858   116    77    74]\n",
            " [   18    18    29    20     2     5  2173     5     1]\n",
            " [  127    20    17     1     1    26    52  6475    52]\n",
            " [   98    17     4     5     0     9    11    23  3797]]\n",
            "Confusion Matrix of the Validation Set: \n",
            "\n",
            "[[ 266    6    2    4    1   16   14   30   47]\n",
            " [  10 2240  664  203    6   10   67    7   17]\n",
            " [   2  341 1049  121    0    4   76    5    3]\n",
            " [   0   57   66  392    0    7   15    0    1]\n",
            " [   1    4    1    0  540    1    1    0    0]\n",
            " [  37   11    4   12    0 2625   48   41   49]\n",
            " [  24   21   43   10    3   13  432   18    4]\n",
            " [  61    7    6    0    0   23   30 1545   21]\n",
            " [  67    5    0    2    2   18    8   30  859]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPc62_FJybGe",
        "colab_type": "text"
      },
      "source": [
        "Decreasing max depth to 3 gives worse result: Test-mlogloss:0.569018\n",
        "Overfitting cant be solved! Why not try without weights??\n",
        "+  Test-mlogloss:0.467937\n",
        "+ Logloss Score Training Set=  0.22106428668160558\n",
        "+ Logloss Score of the Validation Set=  0.46803515527699024\n",
        "\n",
        "Better result but huge overfitting!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36w9KIcGybGh",
        "colab_type": "code",
        "colab": {},
        "outputId": "8904dfec-9fe4-4fd9-99c3-e05449e6c183"
      },
      "source": [
        "dtrain = xgb.DMatrix(X_train.values, label=y_train.values) \n",
        "dval = xgb.DMatrix(X_val.values, label=y_val.values)\n",
        "params = {\n",
        "    'eta': 0.1,  \n",
        "    'n_estimators': 1000,\n",
        "    'silent': True,  # option for logging\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass tasks\n",
        "    'num_class': 9,  # number of classes to predic\n",
        "    'max_depth': 5,  # depth of the trees in the boosting process\n",
        "    'seed': 42,\n",
        "    'eval_metric': 'mlogloss', #cross-entropy is chosen as evaluation metric\n",
        "    'min_child_weight':4,\n",
        "    'gamma':0.0,\n",
        "    'subsample':0.9,\n",
        "    'colsample_bytree':0.8,\n",
        "    'nthread': 16,\n",
        "    #'scale_pos_weight':1,\n",
        "    'reg_alpha':0,\n",
        "    'max_delta_step':1\n",
        "    \n",
        "    }\n",
        "num_round = 50\n",
        "\n",
        "#training the model\n",
        "#model = xgb.train(params, dtrainR, num_round) #change dtrainR w/ dtrain\n",
        "\n",
        "num_boost_round = 999\n",
        "model = xgb.train(\n",
        "    params,\n",
        "    dtrain,#change dtrainR w/ dtrain\n",
        "    num_boost_round=num_boost_round,\n",
        "    evals=[(dtrain, \"Train\"),(dval, \"Test\")],\n",
        "    early_stopping_rounds=20\n",
        ")\n",
        "\n",
        "probsTrain = model.predict(dtrain)\n",
        "probsVal = model.predict(dval)\n",
        "print(\"Training Class Probabilities for First 5 Instances:\\n\",probsTrain[:5])\n",
        "print(\"Validation Class Probabilities for First 5 Instances:\\n\",probsVal[:5])\n",
        "best_predsTrain = np.asarray([np.argmax(line) for line in probsTrain])\n",
        "best_predsVal = np.asarray([np.argmax(line) for line in probsVal])\n",
        "\n",
        "print(\"Best Predictions for Train:\\n\", best_predsTrain+1)\n",
        "print(\"Best Predictions for Validation:\\n\", best_predsVal+1)\n",
        "#print(min(best_preds+1),max(best_preds+1))\n",
        "\n",
        "print(\"Precision Score of the Training Set= \",precision_score(y_train, best_predsTrain, average='macro'))#change y_train(y_val) to y_trainR(y_valR)\n",
        "print(\"Precision Score of the Validation Set= \",precision_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Recall Score of the Training Set= \",recall_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"Recall Score of the Validation Set= \",recall_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"F1 Score of the Training Set= \",f1_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"F1 Score of the Validation Set= \",f1_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Accuracy Score the Training Set= \", accuracy_score(y_train, best_predsTrain))\n",
        "print(\"Accuracy Score of the Validation Set= \", accuracy_score(y_val, best_predsVal))\n",
        "scoreTrain = log_loss(y_train, probsTrain)\n",
        "scoreVal = log_loss(y_val, probsVal)\n",
        "print(\"Logloss Score Training Set= \", scoreTrain)\n",
        "print(\"Logloss Score of the Validation Set= \", scoreVal)\n",
        "\n",
        "print(\"Confusion Matrix of the Training Set: \\n\")\n",
        "print(confusion_matrix(y_train, best_predsTrain))\n",
        "print(\"Confusion Matrix of the Validation Set: \\n\")\n",
        "print(confusion_matrix(y_val,best_predsVal))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\tTrain-mlogloss:2.10029\tTest-mlogloss:2.10174\n",
            "Multiple eval metrics have been passed: 'Test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until Test-mlogloss hasn't improved in 20 rounds.\n",
            "[1]\tTrain-mlogloss:2.0113\tTest-mlogloss:2.01411\n",
            "[2]\tTrain-mlogloss:1.92508\tTest-mlogloss:1.92923\n",
            "[3]\tTrain-mlogloss:1.84087\tTest-mlogloss:1.84607\n",
            "[4]\tTrain-mlogloss:1.75957\tTest-mlogloss:1.76543\n",
            "[5]\tTrain-mlogloss:1.68295\tTest-mlogloss:1.69039\n",
            "[6]\tTrain-mlogloss:1.60667\tTest-mlogloss:1.61517\n",
            "[7]\tTrain-mlogloss:1.53735\tTest-mlogloss:1.54696\n",
            "[8]\tTrain-mlogloss:1.47048\tTest-mlogloss:1.48126\n",
            "[9]\tTrain-mlogloss:1.40572\tTest-mlogloss:1.41749\n",
            "[10]\tTrain-mlogloss:1.34642\tTest-mlogloss:1.35987\n",
            "[11]\tTrain-mlogloss:1.29017\tTest-mlogloss:1.30422\n",
            "[12]\tTrain-mlogloss:1.24144\tTest-mlogloss:1.25619\n",
            "[13]\tTrain-mlogloss:1.19565\tTest-mlogloss:1.21146\n",
            "[14]\tTrain-mlogloss:1.15324\tTest-mlogloss:1.16955\n",
            "[15]\tTrain-mlogloss:1.11639\tTest-mlogloss:1.1339\n",
            "[16]\tTrain-mlogloss:1.0797\tTest-mlogloss:1.098\n",
            "[17]\tTrain-mlogloss:1.04633\tTest-mlogloss:1.06573\n",
            "[18]\tTrain-mlogloss:1.01407\tTest-mlogloss:1.03413\n",
            "[19]\tTrain-mlogloss:0.984935\tTest-mlogloss:1.00554\n",
            "[20]\tTrain-mlogloss:0.95833\tTest-mlogloss:0.979618\n",
            "[21]\tTrain-mlogloss:0.934665\tTest-mlogloss:0.956744\n",
            "[22]\tTrain-mlogloss:0.91365\tTest-mlogloss:0.936217\n",
            "[23]\tTrain-mlogloss:0.892974\tTest-mlogloss:0.916079\n",
            "[24]\tTrain-mlogloss:0.873901\tTest-mlogloss:0.89795\n",
            "[25]\tTrain-mlogloss:0.856004\tTest-mlogloss:0.880735\n",
            "[26]\tTrain-mlogloss:0.838964\tTest-mlogloss:0.864167\n",
            "[27]\tTrain-mlogloss:0.824044\tTest-mlogloss:0.849804\n",
            "[28]\tTrain-mlogloss:0.808629\tTest-mlogloss:0.834946\n",
            "[29]\tTrain-mlogloss:0.793996\tTest-mlogloss:0.820873\n",
            "[30]\tTrain-mlogloss:0.780753\tTest-mlogloss:0.808242\n",
            "[31]\tTrain-mlogloss:0.768059\tTest-mlogloss:0.796032\n",
            "[32]\tTrain-mlogloss:0.757457\tTest-mlogloss:0.786189\n",
            "[33]\tTrain-mlogloss:0.746732\tTest-mlogloss:0.775971\n",
            "[34]\tTrain-mlogloss:0.736105\tTest-mlogloss:0.765943\n",
            "[35]\tTrain-mlogloss:0.726019\tTest-mlogloss:0.756434\n",
            "[36]\tTrain-mlogloss:0.716926\tTest-mlogloss:0.748087\n",
            "[37]\tTrain-mlogloss:0.708394\tTest-mlogloss:0.740106\n",
            "[38]\tTrain-mlogloss:0.700559\tTest-mlogloss:0.732721\n",
            "[39]\tTrain-mlogloss:0.692408\tTest-mlogloss:0.725322\n",
            "[40]\tTrain-mlogloss:0.685207\tTest-mlogloss:0.718553\n",
            "[41]\tTrain-mlogloss:0.67818\tTest-mlogloss:0.71204\n",
            "[42]\tTrain-mlogloss:0.671291\tTest-mlogloss:0.705667\n",
            "[43]\tTrain-mlogloss:0.66466\tTest-mlogloss:0.69975\n",
            "[44]\tTrain-mlogloss:0.658643\tTest-mlogloss:0.694116\n",
            "[45]\tTrain-mlogloss:0.653088\tTest-mlogloss:0.689059\n",
            "[46]\tTrain-mlogloss:0.647729\tTest-mlogloss:0.684267\n",
            "[47]\tTrain-mlogloss:0.642164\tTest-mlogloss:0.679251\n",
            "[48]\tTrain-mlogloss:0.636661\tTest-mlogloss:0.674349\n",
            "[49]\tTrain-mlogloss:0.631567\tTest-mlogloss:0.669691\n",
            "[50]\tTrain-mlogloss:0.62685\tTest-mlogloss:0.665541\n",
            "[51]\tTrain-mlogloss:0.622064\tTest-mlogloss:0.661238\n",
            "[52]\tTrain-mlogloss:0.61811\tTest-mlogloss:0.657666\n",
            "[53]\tTrain-mlogloss:0.613477\tTest-mlogloss:0.653798\n",
            "[54]\tTrain-mlogloss:0.609236\tTest-mlogloss:0.65004\n",
            "[55]\tTrain-mlogloss:0.605188\tTest-mlogloss:0.646723\n",
            "[56]\tTrain-mlogloss:0.601636\tTest-mlogloss:0.643654\n",
            "[57]\tTrain-mlogloss:0.598307\tTest-mlogloss:0.640799\n",
            "[58]\tTrain-mlogloss:0.594597\tTest-mlogloss:0.637828\n",
            "[59]\tTrain-mlogloss:0.591139\tTest-mlogloss:0.634941\n",
            "[60]\tTrain-mlogloss:0.587473\tTest-mlogloss:0.631773\n",
            "[61]\tTrain-mlogloss:0.583947\tTest-mlogloss:0.628785\n",
            "[62]\tTrain-mlogloss:0.580651\tTest-mlogloss:0.625899\n",
            "[63]\tTrain-mlogloss:0.577528\tTest-mlogloss:0.623104\n",
            "[64]\tTrain-mlogloss:0.574873\tTest-mlogloss:0.620948\n",
            "[65]\tTrain-mlogloss:0.572065\tTest-mlogloss:0.618703\n",
            "[66]\tTrain-mlogloss:0.569322\tTest-mlogloss:0.616362\n",
            "[67]\tTrain-mlogloss:0.566758\tTest-mlogloss:0.614127\n",
            "[68]\tTrain-mlogloss:0.564369\tTest-mlogloss:0.61218\n",
            "[69]\tTrain-mlogloss:0.561833\tTest-mlogloss:0.610155\n",
            "[70]\tTrain-mlogloss:0.559357\tTest-mlogloss:0.608175\n",
            "[71]\tTrain-mlogloss:0.556601\tTest-mlogloss:0.605937\n",
            "[72]\tTrain-mlogloss:0.554127\tTest-mlogloss:0.603938\n",
            "[73]\tTrain-mlogloss:0.551824\tTest-mlogloss:0.602153\n",
            "[74]\tTrain-mlogloss:0.549607\tTest-mlogloss:0.600323\n",
            "[75]\tTrain-mlogloss:0.547458\tTest-mlogloss:0.598401\n",
            "[76]\tTrain-mlogloss:0.545326\tTest-mlogloss:0.596609\n",
            "[77]\tTrain-mlogloss:0.543137\tTest-mlogloss:0.594871\n",
            "[78]\tTrain-mlogloss:0.54141\tTest-mlogloss:0.59335\n",
            "[79]\tTrain-mlogloss:0.539382\tTest-mlogloss:0.591809\n",
            "[80]\tTrain-mlogloss:0.537621\tTest-mlogloss:0.590559\n",
            "[81]\tTrain-mlogloss:0.535772\tTest-mlogloss:0.589131\n",
            "[82]\tTrain-mlogloss:0.534144\tTest-mlogloss:0.587875\n",
            "[83]\tTrain-mlogloss:0.53244\tTest-mlogloss:0.586713\n",
            "[84]\tTrain-mlogloss:0.530682\tTest-mlogloss:0.585404\n",
            "[85]\tTrain-mlogloss:0.528942\tTest-mlogloss:0.584095\n",
            "[86]\tTrain-mlogloss:0.527313\tTest-mlogloss:0.582716\n",
            "[87]\tTrain-mlogloss:0.525759\tTest-mlogloss:0.581638\n",
            "[88]\tTrain-mlogloss:0.523621\tTest-mlogloss:0.580028\n",
            "[89]\tTrain-mlogloss:0.522135\tTest-mlogloss:0.578912\n",
            "[90]\tTrain-mlogloss:0.520783\tTest-mlogloss:0.577903\n",
            "[91]\tTrain-mlogloss:0.519014\tTest-mlogloss:0.57665\n",
            "[92]\tTrain-mlogloss:0.517441\tTest-mlogloss:0.575488\n",
            "[93]\tTrain-mlogloss:0.515877\tTest-mlogloss:0.574404\n",
            "[94]\tTrain-mlogloss:0.514531\tTest-mlogloss:0.573349\n",
            "[95]\tTrain-mlogloss:0.512982\tTest-mlogloss:0.572317\n",
            "[96]\tTrain-mlogloss:0.51148\tTest-mlogloss:0.571215\n",
            "[97]\tTrain-mlogloss:0.509901\tTest-mlogloss:0.570045\n",
            "[98]\tTrain-mlogloss:0.50864\tTest-mlogloss:0.569242\n",
            "[99]\tTrain-mlogloss:0.50708\tTest-mlogloss:0.568125\n",
            "[100]\tTrain-mlogloss:0.50573\tTest-mlogloss:0.567274\n",
            "[101]\tTrain-mlogloss:0.504151\tTest-mlogloss:0.566029\n",
            "[102]\tTrain-mlogloss:0.503007\tTest-mlogloss:0.565397\n",
            "[103]\tTrain-mlogloss:0.50178\tTest-mlogloss:0.564573\n",
            "[104]\tTrain-mlogloss:0.500633\tTest-mlogloss:0.563784\n",
            "[105]\tTrain-mlogloss:0.499278\tTest-mlogloss:0.562896\n",
            "[106]\tTrain-mlogloss:0.498006\tTest-mlogloss:0.562112\n",
            "[107]\tTrain-mlogloss:0.496728\tTest-mlogloss:0.561162\n",
            "[108]\tTrain-mlogloss:0.495379\tTest-mlogloss:0.560212\n",
            "[109]\tTrain-mlogloss:0.494078\tTest-mlogloss:0.559299\n",
            "[110]\tTrain-mlogloss:0.492943\tTest-mlogloss:0.558611\n",
            "[111]\tTrain-mlogloss:0.491893\tTest-mlogloss:0.557771\n",
            "[112]\tTrain-mlogloss:0.490645\tTest-mlogloss:0.556943\n",
            "[113]\tTrain-mlogloss:0.489395\tTest-mlogloss:0.556172\n",
            "[114]\tTrain-mlogloss:0.488435\tTest-mlogloss:0.555594\n",
            "[115]\tTrain-mlogloss:0.487399\tTest-mlogloss:0.554758\n",
            "[116]\tTrain-mlogloss:0.486016\tTest-mlogloss:0.553738\n",
            "[117]\tTrain-mlogloss:0.484824\tTest-mlogloss:0.55296\n",
            "[118]\tTrain-mlogloss:0.483451\tTest-mlogloss:0.552218\n",
            "[119]\tTrain-mlogloss:0.482311\tTest-mlogloss:0.551474\n",
            "[120]\tTrain-mlogloss:0.481152\tTest-mlogloss:0.550686\n",
            "[121]\tTrain-mlogloss:0.480156\tTest-mlogloss:0.550118\n",
            "[122]\tTrain-mlogloss:0.479157\tTest-mlogloss:0.549477\n",
            "[123]\tTrain-mlogloss:0.478426\tTest-mlogloss:0.549029\n",
            "[124]\tTrain-mlogloss:0.477354\tTest-mlogloss:0.548315\n",
            "[125]\tTrain-mlogloss:0.47627\tTest-mlogloss:0.54767\n",
            "[126]\tTrain-mlogloss:0.475212\tTest-mlogloss:0.547035\n",
            "[127]\tTrain-mlogloss:0.474127\tTest-mlogloss:0.546427\n",
            "[128]\tTrain-mlogloss:0.473139\tTest-mlogloss:0.545854\n",
            "[129]\tTrain-mlogloss:0.472097\tTest-mlogloss:0.545222\n",
            "[130]\tTrain-mlogloss:0.471055\tTest-mlogloss:0.544683\n",
            "[131]\tTrain-mlogloss:0.470025\tTest-mlogloss:0.54414\n",
            "[132]\tTrain-mlogloss:0.4692\tTest-mlogloss:0.543622\n",
            "[133]\tTrain-mlogloss:0.468275\tTest-mlogloss:0.543074\n",
            "[134]\tTrain-mlogloss:0.467259\tTest-mlogloss:0.542441\n",
            "[135]\tTrain-mlogloss:0.46623\tTest-mlogloss:0.541834\n",
            "[136]\tTrain-mlogloss:0.465306\tTest-mlogloss:0.541279\n",
            "[137]\tTrain-mlogloss:0.46457\tTest-mlogloss:0.540978\n",
            "[138]\tTrain-mlogloss:0.463638\tTest-mlogloss:0.540372\n",
            "[139]\tTrain-mlogloss:0.462625\tTest-mlogloss:0.539809\n",
            "[140]\tTrain-mlogloss:0.461781\tTest-mlogloss:0.539393\n",
            "[141]\tTrain-mlogloss:0.460759\tTest-mlogloss:0.53883\n",
            "[142]\tTrain-mlogloss:0.460092\tTest-mlogloss:0.538381\n",
            "[143]\tTrain-mlogloss:0.45913\tTest-mlogloss:0.53781\n",
            "[144]\tTrain-mlogloss:0.458241\tTest-mlogloss:0.537335\n",
            "[145]\tTrain-mlogloss:0.457276\tTest-mlogloss:0.536767\n",
            "[146]\tTrain-mlogloss:0.456301\tTest-mlogloss:0.536136\n",
            "[147]\tTrain-mlogloss:0.45538\tTest-mlogloss:0.535589\n",
            "[148]\tTrain-mlogloss:0.45448\tTest-mlogloss:0.535158\n",
            "[149]\tTrain-mlogloss:0.453621\tTest-mlogloss:0.534672\n",
            "[150]\tTrain-mlogloss:0.452793\tTest-mlogloss:0.534147\n",
            "[151]\tTrain-mlogloss:0.452106\tTest-mlogloss:0.533739\n",
            "[152]\tTrain-mlogloss:0.451269\tTest-mlogloss:0.533245\n",
            "[153]\tTrain-mlogloss:0.450583\tTest-mlogloss:0.532994\n",
            "[154]\tTrain-mlogloss:0.449903\tTest-mlogloss:0.53267\n",
            "[155]\tTrain-mlogloss:0.449152\tTest-mlogloss:0.532282\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[156]\tTrain-mlogloss:0.448565\tTest-mlogloss:0.531976\n",
            "[157]\tTrain-mlogloss:0.447863\tTest-mlogloss:0.531502\n",
            "[158]\tTrain-mlogloss:0.446921\tTest-mlogloss:0.530986\n",
            "[159]\tTrain-mlogloss:0.446064\tTest-mlogloss:0.530543\n",
            "[160]\tTrain-mlogloss:0.445333\tTest-mlogloss:0.530103\n",
            "[161]\tTrain-mlogloss:0.44462\tTest-mlogloss:0.529715\n",
            "[162]\tTrain-mlogloss:0.443999\tTest-mlogloss:0.529366\n",
            "[163]\tTrain-mlogloss:0.443359\tTest-mlogloss:0.529031\n",
            "[164]\tTrain-mlogloss:0.442541\tTest-mlogloss:0.528464\n",
            "[165]\tTrain-mlogloss:0.441584\tTest-mlogloss:0.527922\n",
            "[166]\tTrain-mlogloss:0.440688\tTest-mlogloss:0.527412\n",
            "[167]\tTrain-mlogloss:0.439862\tTest-mlogloss:0.526908\n",
            "[168]\tTrain-mlogloss:0.43916\tTest-mlogloss:0.526537\n",
            "[169]\tTrain-mlogloss:0.438293\tTest-mlogloss:0.526068\n",
            "[170]\tTrain-mlogloss:0.437418\tTest-mlogloss:0.525518\n",
            "[171]\tTrain-mlogloss:0.436831\tTest-mlogloss:0.525237\n",
            "[172]\tTrain-mlogloss:0.436082\tTest-mlogloss:0.524843\n",
            "[173]\tTrain-mlogloss:0.435438\tTest-mlogloss:0.524573\n",
            "[174]\tTrain-mlogloss:0.434844\tTest-mlogloss:0.524316\n",
            "[175]\tTrain-mlogloss:0.43425\tTest-mlogloss:0.523997\n",
            "[176]\tTrain-mlogloss:0.433402\tTest-mlogloss:0.523553\n",
            "[177]\tTrain-mlogloss:0.432873\tTest-mlogloss:0.523394\n",
            "[178]\tTrain-mlogloss:0.432331\tTest-mlogloss:0.523171\n",
            "[179]\tTrain-mlogloss:0.431565\tTest-mlogloss:0.522862\n",
            "[180]\tTrain-mlogloss:0.430754\tTest-mlogloss:0.522412\n",
            "[181]\tTrain-mlogloss:0.430042\tTest-mlogloss:0.522052\n",
            "[182]\tTrain-mlogloss:0.429357\tTest-mlogloss:0.521773\n",
            "[183]\tTrain-mlogloss:0.428693\tTest-mlogloss:0.521357\n",
            "[184]\tTrain-mlogloss:0.427986\tTest-mlogloss:0.520998\n",
            "[185]\tTrain-mlogloss:0.42739\tTest-mlogloss:0.520645\n",
            "[186]\tTrain-mlogloss:0.426508\tTest-mlogloss:0.520238\n",
            "[187]\tTrain-mlogloss:0.42584\tTest-mlogloss:0.519901\n",
            "[188]\tTrain-mlogloss:0.425146\tTest-mlogloss:0.519599\n",
            "[189]\tTrain-mlogloss:0.424398\tTest-mlogloss:0.519298\n",
            "[190]\tTrain-mlogloss:0.423785\tTest-mlogloss:0.519036\n",
            "[191]\tTrain-mlogloss:0.423092\tTest-mlogloss:0.518769\n",
            "[192]\tTrain-mlogloss:0.422399\tTest-mlogloss:0.518391\n",
            "[193]\tTrain-mlogloss:0.421756\tTest-mlogloss:0.518107\n",
            "[194]\tTrain-mlogloss:0.421244\tTest-mlogloss:0.517913\n",
            "[195]\tTrain-mlogloss:0.420505\tTest-mlogloss:0.517526\n",
            "[196]\tTrain-mlogloss:0.420031\tTest-mlogloss:0.517397\n",
            "[197]\tTrain-mlogloss:0.419269\tTest-mlogloss:0.517034\n",
            "[198]\tTrain-mlogloss:0.41853\tTest-mlogloss:0.516858\n",
            "[199]\tTrain-mlogloss:0.41787\tTest-mlogloss:0.516609\n",
            "[200]\tTrain-mlogloss:0.417312\tTest-mlogloss:0.5164\n",
            "[201]\tTrain-mlogloss:0.416575\tTest-mlogloss:0.515938\n",
            "[202]\tTrain-mlogloss:0.416128\tTest-mlogloss:0.515692\n",
            "[203]\tTrain-mlogloss:0.4155\tTest-mlogloss:0.515329\n",
            "[204]\tTrain-mlogloss:0.414942\tTest-mlogloss:0.515055\n",
            "[205]\tTrain-mlogloss:0.414403\tTest-mlogloss:0.514889\n",
            "[206]\tTrain-mlogloss:0.414048\tTest-mlogloss:0.514747\n",
            "[207]\tTrain-mlogloss:0.413519\tTest-mlogloss:0.514457\n",
            "[208]\tTrain-mlogloss:0.412853\tTest-mlogloss:0.514127\n",
            "[209]\tTrain-mlogloss:0.41232\tTest-mlogloss:0.513998\n",
            "[210]\tTrain-mlogloss:0.411597\tTest-mlogloss:0.513684\n",
            "[211]\tTrain-mlogloss:0.41112\tTest-mlogloss:0.513506\n",
            "[212]\tTrain-mlogloss:0.410463\tTest-mlogloss:0.513243\n",
            "[213]\tTrain-mlogloss:0.409793\tTest-mlogloss:0.512886\n",
            "[214]\tTrain-mlogloss:0.409303\tTest-mlogloss:0.512735\n",
            "[215]\tTrain-mlogloss:0.408634\tTest-mlogloss:0.512444\n",
            "[216]\tTrain-mlogloss:0.407918\tTest-mlogloss:0.512112\n",
            "[217]\tTrain-mlogloss:0.407399\tTest-mlogloss:0.511895\n",
            "[218]\tTrain-mlogloss:0.406701\tTest-mlogloss:0.511575\n",
            "[219]\tTrain-mlogloss:0.406279\tTest-mlogloss:0.511467\n",
            "[220]\tTrain-mlogloss:0.405743\tTest-mlogloss:0.511287\n",
            "[221]\tTrain-mlogloss:0.405151\tTest-mlogloss:0.51102\n",
            "[222]\tTrain-mlogloss:0.40462\tTest-mlogloss:0.5108\n",
            "[223]\tTrain-mlogloss:0.404084\tTest-mlogloss:0.510595\n",
            "[224]\tTrain-mlogloss:0.403574\tTest-mlogloss:0.510368\n",
            "[225]\tTrain-mlogloss:0.403018\tTest-mlogloss:0.510089\n",
            "[226]\tTrain-mlogloss:0.402488\tTest-mlogloss:0.509887\n",
            "[227]\tTrain-mlogloss:0.402081\tTest-mlogloss:0.509816\n",
            "[228]\tTrain-mlogloss:0.401383\tTest-mlogloss:0.509582\n",
            "[229]\tTrain-mlogloss:0.400816\tTest-mlogloss:0.509284\n",
            "[230]\tTrain-mlogloss:0.40035\tTest-mlogloss:0.509012\n",
            "[231]\tTrain-mlogloss:0.399707\tTest-mlogloss:0.5086\n",
            "[232]\tTrain-mlogloss:0.399228\tTest-mlogloss:0.508345\n",
            "[233]\tTrain-mlogloss:0.398591\tTest-mlogloss:0.508035\n",
            "[234]\tTrain-mlogloss:0.398029\tTest-mlogloss:0.507863\n",
            "[235]\tTrain-mlogloss:0.397418\tTest-mlogloss:0.507603\n",
            "[236]\tTrain-mlogloss:0.396767\tTest-mlogloss:0.50725\n",
            "[237]\tTrain-mlogloss:0.396122\tTest-mlogloss:0.507135\n",
            "[238]\tTrain-mlogloss:0.395647\tTest-mlogloss:0.506868\n",
            "[239]\tTrain-mlogloss:0.395192\tTest-mlogloss:0.506579\n",
            "[240]\tTrain-mlogloss:0.394771\tTest-mlogloss:0.506446\n",
            "[241]\tTrain-mlogloss:0.394386\tTest-mlogloss:0.506231\n",
            "[242]\tTrain-mlogloss:0.393794\tTest-mlogloss:0.505941\n",
            "[243]\tTrain-mlogloss:0.393433\tTest-mlogloss:0.505765\n",
            "[244]\tTrain-mlogloss:0.393067\tTest-mlogloss:0.505557\n",
            "[245]\tTrain-mlogloss:0.392415\tTest-mlogloss:0.505271\n",
            "[246]\tTrain-mlogloss:0.391699\tTest-mlogloss:0.505049\n",
            "[247]\tTrain-mlogloss:0.39132\tTest-mlogloss:0.504865\n",
            "[248]\tTrain-mlogloss:0.390934\tTest-mlogloss:0.504649\n",
            "[249]\tTrain-mlogloss:0.390298\tTest-mlogloss:0.504331\n",
            "[250]\tTrain-mlogloss:0.389871\tTest-mlogloss:0.504201\n",
            "[251]\tTrain-mlogloss:0.389239\tTest-mlogloss:0.504004\n",
            "[252]\tTrain-mlogloss:0.388719\tTest-mlogloss:0.503808\n",
            "[253]\tTrain-mlogloss:0.388165\tTest-mlogloss:0.503525\n",
            "[254]\tTrain-mlogloss:0.387702\tTest-mlogloss:0.503367\n",
            "[255]\tTrain-mlogloss:0.387192\tTest-mlogloss:0.503243\n",
            "[256]\tTrain-mlogloss:0.386708\tTest-mlogloss:0.503044\n",
            "[257]\tTrain-mlogloss:0.385904\tTest-mlogloss:0.50268\n",
            "[258]\tTrain-mlogloss:0.385305\tTest-mlogloss:0.502456\n",
            "[259]\tTrain-mlogloss:0.384813\tTest-mlogloss:0.50228\n",
            "[260]\tTrain-mlogloss:0.38421\tTest-mlogloss:0.501975\n",
            "[261]\tTrain-mlogloss:0.383711\tTest-mlogloss:0.501846\n",
            "[262]\tTrain-mlogloss:0.383087\tTest-mlogloss:0.501521\n",
            "[263]\tTrain-mlogloss:0.382646\tTest-mlogloss:0.501348\n",
            "[264]\tTrain-mlogloss:0.38231\tTest-mlogloss:0.501187\n",
            "[265]\tTrain-mlogloss:0.38173\tTest-mlogloss:0.50085\n",
            "[266]\tTrain-mlogloss:0.381352\tTest-mlogloss:0.500723\n",
            "[267]\tTrain-mlogloss:0.380964\tTest-mlogloss:0.500594\n",
            "[268]\tTrain-mlogloss:0.380565\tTest-mlogloss:0.500477\n",
            "[269]\tTrain-mlogloss:0.379868\tTest-mlogloss:0.500234\n",
            "[270]\tTrain-mlogloss:0.379556\tTest-mlogloss:0.500128\n",
            "[271]\tTrain-mlogloss:0.37896\tTest-mlogloss:0.499929\n",
            "[272]\tTrain-mlogloss:0.378497\tTest-mlogloss:0.4998\n",
            "[273]\tTrain-mlogloss:0.378034\tTest-mlogloss:0.499663\n",
            "[274]\tTrain-mlogloss:0.377559\tTest-mlogloss:0.499386\n",
            "[275]\tTrain-mlogloss:0.377111\tTest-mlogloss:0.499217\n",
            "[276]\tTrain-mlogloss:0.376707\tTest-mlogloss:0.499009\n",
            "[277]\tTrain-mlogloss:0.37618\tTest-mlogloss:0.498782\n",
            "[278]\tTrain-mlogloss:0.375754\tTest-mlogloss:0.498795\n",
            "[279]\tTrain-mlogloss:0.375273\tTest-mlogloss:0.498579\n",
            "[280]\tTrain-mlogloss:0.374751\tTest-mlogloss:0.498482\n",
            "[281]\tTrain-mlogloss:0.374307\tTest-mlogloss:0.498334\n",
            "[282]\tTrain-mlogloss:0.373988\tTest-mlogloss:0.498203\n",
            "[283]\tTrain-mlogloss:0.373595\tTest-mlogloss:0.498066\n",
            "[284]\tTrain-mlogloss:0.373225\tTest-mlogloss:0.498019\n",
            "[285]\tTrain-mlogloss:0.372741\tTest-mlogloss:0.49774\n",
            "[286]\tTrain-mlogloss:0.372337\tTest-mlogloss:0.497666\n",
            "[287]\tTrain-mlogloss:0.371865\tTest-mlogloss:0.497496\n",
            "[288]\tTrain-mlogloss:0.371404\tTest-mlogloss:0.497273\n",
            "[289]\tTrain-mlogloss:0.371031\tTest-mlogloss:0.497107\n",
            "[290]\tTrain-mlogloss:0.370697\tTest-mlogloss:0.496954\n",
            "[291]\tTrain-mlogloss:0.370318\tTest-mlogloss:0.496771\n",
            "[292]\tTrain-mlogloss:0.370013\tTest-mlogloss:0.496707\n",
            "[293]\tTrain-mlogloss:0.369725\tTest-mlogloss:0.496633\n",
            "[294]\tTrain-mlogloss:0.369214\tTest-mlogloss:0.496475\n",
            "[295]\tTrain-mlogloss:0.368825\tTest-mlogloss:0.496395\n",
            "[296]\tTrain-mlogloss:0.368518\tTest-mlogloss:0.496273\n",
            "[297]\tTrain-mlogloss:0.368076\tTest-mlogloss:0.496093\n",
            "[298]\tTrain-mlogloss:0.367712\tTest-mlogloss:0.495964\n",
            "[299]\tTrain-mlogloss:0.367291\tTest-mlogloss:0.495911\n",
            "[300]\tTrain-mlogloss:0.366806\tTest-mlogloss:0.495733\n",
            "[301]\tTrain-mlogloss:0.366436\tTest-mlogloss:0.495628\n",
            "[302]\tTrain-mlogloss:0.366073\tTest-mlogloss:0.495323\n",
            "[303]\tTrain-mlogloss:0.365716\tTest-mlogloss:0.495201\n",
            "[304]\tTrain-mlogloss:0.365263\tTest-mlogloss:0.49497\n",
            "[305]\tTrain-mlogloss:0.364851\tTest-mlogloss:0.494793\n",
            "[306]\tTrain-mlogloss:0.36452\tTest-mlogloss:0.494783\n",
            "[307]\tTrain-mlogloss:0.364168\tTest-mlogloss:0.49468\n",
            "[308]\tTrain-mlogloss:0.363803\tTest-mlogloss:0.494499\n",
            "[309]\tTrain-mlogloss:0.363399\tTest-mlogloss:0.494304\n",
            "[310]\tTrain-mlogloss:0.362891\tTest-mlogloss:0.494047\n",
            "[311]\tTrain-mlogloss:0.362518\tTest-mlogloss:0.493967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[312]\tTrain-mlogloss:0.362056\tTest-mlogloss:0.493846\n",
            "[313]\tTrain-mlogloss:0.361528\tTest-mlogloss:0.493676\n",
            "[314]\tTrain-mlogloss:0.361198\tTest-mlogloss:0.493551\n",
            "[315]\tTrain-mlogloss:0.36073\tTest-mlogloss:0.493539\n",
            "[316]\tTrain-mlogloss:0.360391\tTest-mlogloss:0.49346\n",
            "[317]\tTrain-mlogloss:0.359993\tTest-mlogloss:0.493399\n",
            "[318]\tTrain-mlogloss:0.359589\tTest-mlogloss:0.493224\n",
            "[319]\tTrain-mlogloss:0.359266\tTest-mlogloss:0.493123\n",
            "[320]\tTrain-mlogloss:0.358917\tTest-mlogloss:0.493068\n",
            "[321]\tTrain-mlogloss:0.358575\tTest-mlogloss:0.492889\n",
            "[322]\tTrain-mlogloss:0.358249\tTest-mlogloss:0.492747\n",
            "[323]\tTrain-mlogloss:0.358002\tTest-mlogloss:0.492677\n",
            "[324]\tTrain-mlogloss:0.357686\tTest-mlogloss:0.492613\n",
            "[325]\tTrain-mlogloss:0.35734\tTest-mlogloss:0.492611\n",
            "[326]\tTrain-mlogloss:0.356874\tTest-mlogloss:0.492479\n",
            "[327]\tTrain-mlogloss:0.35644\tTest-mlogloss:0.492316\n",
            "[328]\tTrain-mlogloss:0.355719\tTest-mlogloss:0.492014\n",
            "[329]\tTrain-mlogloss:0.355281\tTest-mlogloss:0.491941\n",
            "[330]\tTrain-mlogloss:0.354869\tTest-mlogloss:0.491724\n",
            "[331]\tTrain-mlogloss:0.354421\tTest-mlogloss:0.491549\n",
            "[332]\tTrain-mlogloss:0.353963\tTest-mlogloss:0.491369\n",
            "[333]\tTrain-mlogloss:0.35349\tTest-mlogloss:0.491191\n",
            "[334]\tTrain-mlogloss:0.35307\tTest-mlogloss:0.49103\n",
            "[335]\tTrain-mlogloss:0.352645\tTest-mlogloss:0.490854\n",
            "[336]\tTrain-mlogloss:0.352317\tTest-mlogloss:0.490761\n",
            "[337]\tTrain-mlogloss:0.35196\tTest-mlogloss:0.49065\n",
            "[338]\tTrain-mlogloss:0.351669\tTest-mlogloss:0.490523\n",
            "[339]\tTrain-mlogloss:0.351367\tTest-mlogloss:0.490465\n",
            "[340]\tTrain-mlogloss:0.351046\tTest-mlogloss:0.490375\n",
            "[341]\tTrain-mlogloss:0.350557\tTest-mlogloss:0.490246\n",
            "[342]\tTrain-mlogloss:0.350349\tTest-mlogloss:0.490229\n",
            "[343]\tTrain-mlogloss:0.349957\tTest-mlogloss:0.490095\n",
            "[344]\tTrain-mlogloss:0.349518\tTest-mlogloss:0.490002\n",
            "[345]\tTrain-mlogloss:0.349186\tTest-mlogloss:0.489938\n",
            "[346]\tTrain-mlogloss:0.348837\tTest-mlogloss:0.489854\n",
            "[347]\tTrain-mlogloss:0.348417\tTest-mlogloss:0.489729\n",
            "[348]\tTrain-mlogloss:0.348065\tTest-mlogloss:0.489638\n",
            "[349]\tTrain-mlogloss:0.347796\tTest-mlogloss:0.489618\n",
            "[350]\tTrain-mlogloss:0.347564\tTest-mlogloss:0.489571\n",
            "[351]\tTrain-mlogloss:0.347154\tTest-mlogloss:0.489442\n",
            "[352]\tTrain-mlogloss:0.346855\tTest-mlogloss:0.489323\n",
            "[353]\tTrain-mlogloss:0.346645\tTest-mlogloss:0.48927\n",
            "[354]\tTrain-mlogloss:0.346321\tTest-mlogloss:0.489199\n",
            "[355]\tTrain-mlogloss:0.345961\tTest-mlogloss:0.489019\n",
            "[356]\tTrain-mlogloss:0.345524\tTest-mlogloss:0.488833\n",
            "[357]\tTrain-mlogloss:0.345014\tTest-mlogloss:0.488688\n",
            "[358]\tTrain-mlogloss:0.344768\tTest-mlogloss:0.48867\n",
            "[359]\tTrain-mlogloss:0.344398\tTest-mlogloss:0.488568\n",
            "[360]\tTrain-mlogloss:0.344059\tTest-mlogloss:0.488355\n",
            "[361]\tTrain-mlogloss:0.343837\tTest-mlogloss:0.488309\n",
            "[362]\tTrain-mlogloss:0.343587\tTest-mlogloss:0.488189\n",
            "[363]\tTrain-mlogloss:0.343207\tTest-mlogloss:0.488086\n",
            "[364]\tTrain-mlogloss:0.342906\tTest-mlogloss:0.487988\n",
            "[365]\tTrain-mlogloss:0.342541\tTest-mlogloss:0.487848\n",
            "[366]\tTrain-mlogloss:0.342132\tTest-mlogloss:0.487775\n",
            "[367]\tTrain-mlogloss:0.341618\tTest-mlogloss:0.487594\n",
            "[368]\tTrain-mlogloss:0.341235\tTest-mlogloss:0.487476\n",
            "[369]\tTrain-mlogloss:0.340881\tTest-mlogloss:0.487359\n",
            "[370]\tTrain-mlogloss:0.340548\tTest-mlogloss:0.487303\n",
            "[371]\tTrain-mlogloss:0.340098\tTest-mlogloss:0.487127\n",
            "[372]\tTrain-mlogloss:0.339723\tTest-mlogloss:0.487019\n",
            "[373]\tTrain-mlogloss:0.339265\tTest-mlogloss:0.486873\n",
            "[374]\tTrain-mlogloss:0.338944\tTest-mlogloss:0.486762\n",
            "[375]\tTrain-mlogloss:0.338656\tTest-mlogloss:0.486636\n",
            "[376]\tTrain-mlogloss:0.338359\tTest-mlogloss:0.486536\n",
            "[377]\tTrain-mlogloss:0.337837\tTest-mlogloss:0.486457\n",
            "[378]\tTrain-mlogloss:0.337448\tTest-mlogloss:0.486406\n",
            "[379]\tTrain-mlogloss:0.336974\tTest-mlogloss:0.486257\n",
            "[380]\tTrain-mlogloss:0.336648\tTest-mlogloss:0.48611\n",
            "[381]\tTrain-mlogloss:0.336288\tTest-mlogloss:0.48603\n",
            "[382]\tTrain-mlogloss:0.335978\tTest-mlogloss:0.48594\n",
            "[383]\tTrain-mlogloss:0.335601\tTest-mlogloss:0.485754\n",
            "[384]\tTrain-mlogloss:0.335262\tTest-mlogloss:0.485633\n",
            "[385]\tTrain-mlogloss:0.334942\tTest-mlogloss:0.485561\n",
            "[386]\tTrain-mlogloss:0.33456\tTest-mlogloss:0.485498\n",
            "[387]\tTrain-mlogloss:0.334137\tTest-mlogloss:0.485411\n",
            "[388]\tTrain-mlogloss:0.333725\tTest-mlogloss:0.485286\n",
            "[389]\tTrain-mlogloss:0.333295\tTest-mlogloss:0.48504\n",
            "[390]\tTrain-mlogloss:0.332963\tTest-mlogloss:0.485053\n",
            "[391]\tTrain-mlogloss:0.332664\tTest-mlogloss:0.484953\n",
            "[392]\tTrain-mlogloss:0.332341\tTest-mlogloss:0.484863\n",
            "[393]\tTrain-mlogloss:0.332124\tTest-mlogloss:0.484784\n",
            "[394]\tTrain-mlogloss:0.331808\tTest-mlogloss:0.484661\n",
            "[395]\tTrain-mlogloss:0.331405\tTest-mlogloss:0.484634\n",
            "[396]\tTrain-mlogloss:0.331097\tTest-mlogloss:0.484561\n",
            "[397]\tTrain-mlogloss:0.330816\tTest-mlogloss:0.484445\n",
            "[398]\tTrain-mlogloss:0.330418\tTest-mlogloss:0.484251\n",
            "[399]\tTrain-mlogloss:0.329942\tTest-mlogloss:0.48414\n",
            "[400]\tTrain-mlogloss:0.329559\tTest-mlogloss:0.484085\n",
            "[401]\tTrain-mlogloss:0.32922\tTest-mlogloss:0.483923\n",
            "[402]\tTrain-mlogloss:0.328807\tTest-mlogloss:0.483824\n",
            "[403]\tTrain-mlogloss:0.328512\tTest-mlogloss:0.483781\n",
            "[404]\tTrain-mlogloss:0.328147\tTest-mlogloss:0.483645\n",
            "[405]\tTrain-mlogloss:0.327799\tTest-mlogloss:0.483597\n",
            "[406]\tTrain-mlogloss:0.32746\tTest-mlogloss:0.483489\n",
            "[407]\tTrain-mlogloss:0.327077\tTest-mlogloss:0.483387\n",
            "[408]\tTrain-mlogloss:0.326633\tTest-mlogloss:0.48328\n",
            "[409]\tTrain-mlogloss:0.326324\tTest-mlogloss:0.483254\n",
            "[410]\tTrain-mlogloss:0.326118\tTest-mlogloss:0.483246\n",
            "[411]\tTrain-mlogloss:0.325832\tTest-mlogloss:0.483169\n",
            "[412]\tTrain-mlogloss:0.325431\tTest-mlogloss:0.483124\n",
            "[413]\tTrain-mlogloss:0.325084\tTest-mlogloss:0.482985\n",
            "[414]\tTrain-mlogloss:0.324757\tTest-mlogloss:0.48288\n",
            "[415]\tTrain-mlogloss:0.324494\tTest-mlogloss:0.482824\n",
            "[416]\tTrain-mlogloss:0.324108\tTest-mlogloss:0.482694\n",
            "[417]\tTrain-mlogloss:0.323799\tTest-mlogloss:0.482592\n",
            "[418]\tTrain-mlogloss:0.323355\tTest-mlogloss:0.482487\n",
            "[419]\tTrain-mlogloss:0.322945\tTest-mlogloss:0.482392\n",
            "[420]\tTrain-mlogloss:0.322687\tTest-mlogloss:0.482401\n",
            "[421]\tTrain-mlogloss:0.322356\tTest-mlogloss:0.482312\n",
            "[422]\tTrain-mlogloss:0.321936\tTest-mlogloss:0.482268\n",
            "[423]\tTrain-mlogloss:0.321665\tTest-mlogloss:0.482168\n",
            "[424]\tTrain-mlogloss:0.321354\tTest-mlogloss:0.482118\n",
            "[425]\tTrain-mlogloss:0.321009\tTest-mlogloss:0.482079\n",
            "[426]\tTrain-mlogloss:0.320764\tTest-mlogloss:0.481991\n",
            "[427]\tTrain-mlogloss:0.320279\tTest-mlogloss:0.481804\n",
            "[428]\tTrain-mlogloss:0.320007\tTest-mlogloss:0.481724\n",
            "[429]\tTrain-mlogloss:0.319695\tTest-mlogloss:0.481694\n",
            "[430]\tTrain-mlogloss:0.319343\tTest-mlogloss:0.481653\n",
            "[431]\tTrain-mlogloss:0.31904\tTest-mlogloss:0.481641\n",
            "[432]\tTrain-mlogloss:0.318762\tTest-mlogloss:0.481602\n",
            "[433]\tTrain-mlogloss:0.318526\tTest-mlogloss:0.481547\n",
            "[434]\tTrain-mlogloss:0.31827\tTest-mlogloss:0.481528\n",
            "[435]\tTrain-mlogloss:0.318033\tTest-mlogloss:0.481472\n",
            "[436]\tTrain-mlogloss:0.317689\tTest-mlogloss:0.481387\n",
            "[437]\tTrain-mlogloss:0.317317\tTest-mlogloss:0.481365\n",
            "[438]\tTrain-mlogloss:0.317017\tTest-mlogloss:0.481389\n",
            "[439]\tTrain-mlogloss:0.316723\tTest-mlogloss:0.481342\n",
            "[440]\tTrain-mlogloss:0.316428\tTest-mlogloss:0.48127\n",
            "[441]\tTrain-mlogloss:0.316173\tTest-mlogloss:0.481232\n",
            "[442]\tTrain-mlogloss:0.315928\tTest-mlogloss:0.481154\n",
            "[443]\tTrain-mlogloss:0.315485\tTest-mlogloss:0.481106\n",
            "[444]\tTrain-mlogloss:0.315182\tTest-mlogloss:0.481015\n",
            "[445]\tTrain-mlogloss:0.314771\tTest-mlogloss:0.480998\n",
            "[446]\tTrain-mlogloss:0.314496\tTest-mlogloss:0.480881\n",
            "[447]\tTrain-mlogloss:0.314095\tTest-mlogloss:0.480798\n",
            "[448]\tTrain-mlogloss:0.313649\tTest-mlogloss:0.480589\n",
            "[449]\tTrain-mlogloss:0.313429\tTest-mlogloss:0.480569\n",
            "[450]\tTrain-mlogloss:0.313202\tTest-mlogloss:0.480437\n",
            "[451]\tTrain-mlogloss:0.312913\tTest-mlogloss:0.480341\n",
            "[452]\tTrain-mlogloss:0.312607\tTest-mlogloss:0.480275\n",
            "[453]\tTrain-mlogloss:0.312336\tTest-mlogloss:0.480192\n",
            "[454]\tTrain-mlogloss:0.312041\tTest-mlogloss:0.480093\n",
            "[455]\tTrain-mlogloss:0.311612\tTest-mlogloss:0.480129\n",
            "[456]\tTrain-mlogloss:0.311199\tTest-mlogloss:0.48004\n",
            "[457]\tTrain-mlogloss:0.310923\tTest-mlogloss:0.479998\n",
            "[458]\tTrain-mlogloss:0.31058\tTest-mlogloss:0.479846\n",
            "[459]\tTrain-mlogloss:0.310362\tTest-mlogloss:0.479837\n",
            "[460]\tTrain-mlogloss:0.31003\tTest-mlogloss:0.479751\n",
            "[461]\tTrain-mlogloss:0.309723\tTest-mlogloss:0.4796\n",
            "[462]\tTrain-mlogloss:0.309482\tTest-mlogloss:0.47958\n",
            "[463]\tTrain-mlogloss:0.309359\tTest-mlogloss:0.479564\n",
            "[464]\tTrain-mlogloss:0.309123\tTest-mlogloss:0.479491\n",
            "[465]\tTrain-mlogloss:0.30887\tTest-mlogloss:0.479464\n",
            "[466]\tTrain-mlogloss:0.308567\tTest-mlogloss:0.479363\n",
            "[467]\tTrain-mlogloss:0.308281\tTest-mlogloss:0.479241\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[468]\tTrain-mlogloss:0.307941\tTest-mlogloss:0.479126\n",
            "[469]\tTrain-mlogloss:0.307654\tTest-mlogloss:0.479027\n",
            "[470]\tTrain-mlogloss:0.307348\tTest-mlogloss:0.478912\n",
            "[471]\tTrain-mlogloss:0.306996\tTest-mlogloss:0.478816\n",
            "[472]\tTrain-mlogloss:0.306749\tTest-mlogloss:0.478836\n",
            "[473]\tTrain-mlogloss:0.306362\tTest-mlogloss:0.478743\n",
            "[474]\tTrain-mlogloss:0.306\tTest-mlogloss:0.478577\n",
            "[475]\tTrain-mlogloss:0.305733\tTest-mlogloss:0.478507\n",
            "[476]\tTrain-mlogloss:0.305325\tTest-mlogloss:0.478405\n",
            "[477]\tTrain-mlogloss:0.305106\tTest-mlogloss:0.478426\n",
            "[478]\tTrain-mlogloss:0.304836\tTest-mlogloss:0.478383\n",
            "[479]\tTrain-mlogloss:0.304543\tTest-mlogloss:0.478333\n",
            "[480]\tTrain-mlogloss:0.304219\tTest-mlogloss:0.478272\n",
            "[481]\tTrain-mlogloss:0.303807\tTest-mlogloss:0.47818\n",
            "[482]\tTrain-mlogloss:0.303503\tTest-mlogloss:0.478161\n",
            "[483]\tTrain-mlogloss:0.303237\tTest-mlogloss:0.478043\n",
            "[484]\tTrain-mlogloss:0.302789\tTest-mlogloss:0.477906\n",
            "[485]\tTrain-mlogloss:0.30246\tTest-mlogloss:0.477874\n",
            "[486]\tTrain-mlogloss:0.302064\tTest-mlogloss:0.477713\n",
            "[487]\tTrain-mlogloss:0.301799\tTest-mlogloss:0.477619\n",
            "[488]\tTrain-mlogloss:0.301485\tTest-mlogloss:0.477582\n",
            "[489]\tTrain-mlogloss:0.301185\tTest-mlogloss:0.477556\n",
            "[490]\tTrain-mlogloss:0.300953\tTest-mlogloss:0.477555\n",
            "[491]\tTrain-mlogloss:0.300563\tTest-mlogloss:0.477474\n",
            "[492]\tTrain-mlogloss:0.300262\tTest-mlogloss:0.477363\n",
            "[493]\tTrain-mlogloss:0.29998\tTest-mlogloss:0.477283\n",
            "[494]\tTrain-mlogloss:0.299705\tTest-mlogloss:0.477226\n",
            "[495]\tTrain-mlogloss:0.299374\tTest-mlogloss:0.477147\n",
            "[496]\tTrain-mlogloss:0.299055\tTest-mlogloss:0.47702\n",
            "[497]\tTrain-mlogloss:0.298683\tTest-mlogloss:0.47697\n",
            "[498]\tTrain-mlogloss:0.298431\tTest-mlogloss:0.476911\n",
            "[499]\tTrain-mlogloss:0.298065\tTest-mlogloss:0.476809\n",
            "[500]\tTrain-mlogloss:0.297847\tTest-mlogloss:0.476793\n",
            "[501]\tTrain-mlogloss:0.297617\tTest-mlogloss:0.476755\n",
            "[502]\tTrain-mlogloss:0.29724\tTest-mlogloss:0.476651\n",
            "[503]\tTrain-mlogloss:0.296962\tTest-mlogloss:0.476607\n",
            "[504]\tTrain-mlogloss:0.296531\tTest-mlogloss:0.476461\n",
            "[505]\tTrain-mlogloss:0.296282\tTest-mlogloss:0.476406\n",
            "[506]\tTrain-mlogloss:0.296002\tTest-mlogloss:0.476384\n",
            "[507]\tTrain-mlogloss:0.295734\tTest-mlogloss:0.476387\n",
            "[508]\tTrain-mlogloss:0.295452\tTest-mlogloss:0.476386\n",
            "[509]\tTrain-mlogloss:0.295247\tTest-mlogloss:0.476324\n",
            "[510]\tTrain-mlogloss:0.294877\tTest-mlogloss:0.476182\n",
            "[511]\tTrain-mlogloss:0.294641\tTest-mlogloss:0.476162\n",
            "[512]\tTrain-mlogloss:0.294342\tTest-mlogloss:0.476085\n",
            "[513]\tTrain-mlogloss:0.293918\tTest-mlogloss:0.476071\n",
            "[514]\tTrain-mlogloss:0.293629\tTest-mlogloss:0.476053\n",
            "[515]\tTrain-mlogloss:0.293277\tTest-mlogloss:0.476013\n",
            "[516]\tTrain-mlogloss:0.292996\tTest-mlogloss:0.476009\n",
            "[517]\tTrain-mlogloss:0.292674\tTest-mlogloss:0.475924\n",
            "[518]\tTrain-mlogloss:0.292426\tTest-mlogloss:0.475902\n",
            "[519]\tTrain-mlogloss:0.292162\tTest-mlogloss:0.475824\n",
            "[520]\tTrain-mlogloss:0.291865\tTest-mlogloss:0.475716\n",
            "[521]\tTrain-mlogloss:0.291646\tTest-mlogloss:0.475697\n",
            "[522]\tTrain-mlogloss:0.291275\tTest-mlogloss:0.475635\n",
            "[523]\tTrain-mlogloss:0.291052\tTest-mlogloss:0.475562\n",
            "[524]\tTrain-mlogloss:0.290723\tTest-mlogloss:0.475518\n",
            "[525]\tTrain-mlogloss:0.290407\tTest-mlogloss:0.475365\n",
            "[526]\tTrain-mlogloss:0.290121\tTest-mlogloss:0.475341\n",
            "[527]\tTrain-mlogloss:0.289887\tTest-mlogloss:0.475319\n",
            "[528]\tTrain-mlogloss:0.289711\tTest-mlogloss:0.475299\n",
            "[529]\tTrain-mlogloss:0.289426\tTest-mlogloss:0.475227\n",
            "[530]\tTrain-mlogloss:0.289211\tTest-mlogloss:0.475218\n",
            "[531]\tTrain-mlogloss:0.289012\tTest-mlogloss:0.475182\n",
            "[532]\tTrain-mlogloss:0.288737\tTest-mlogloss:0.475014\n",
            "[533]\tTrain-mlogloss:0.288433\tTest-mlogloss:0.474978\n",
            "[534]\tTrain-mlogloss:0.288062\tTest-mlogloss:0.474908\n",
            "[535]\tTrain-mlogloss:0.287761\tTest-mlogloss:0.474821\n",
            "[536]\tTrain-mlogloss:0.287478\tTest-mlogloss:0.474791\n",
            "[537]\tTrain-mlogloss:0.287223\tTest-mlogloss:0.474699\n",
            "[538]\tTrain-mlogloss:0.286936\tTest-mlogloss:0.47459\n",
            "[539]\tTrain-mlogloss:0.286691\tTest-mlogloss:0.47461\n",
            "[540]\tTrain-mlogloss:0.286488\tTest-mlogloss:0.474579\n",
            "[541]\tTrain-mlogloss:0.286232\tTest-mlogloss:0.474481\n",
            "[542]\tTrain-mlogloss:0.285894\tTest-mlogloss:0.474329\n",
            "[543]\tTrain-mlogloss:0.285611\tTest-mlogloss:0.474252\n",
            "[544]\tTrain-mlogloss:0.285329\tTest-mlogloss:0.474264\n",
            "[545]\tTrain-mlogloss:0.285103\tTest-mlogloss:0.474247\n",
            "[546]\tTrain-mlogloss:0.284791\tTest-mlogloss:0.474193\n",
            "[547]\tTrain-mlogloss:0.284522\tTest-mlogloss:0.47416\n",
            "[548]\tTrain-mlogloss:0.284265\tTest-mlogloss:0.474109\n",
            "[549]\tTrain-mlogloss:0.284011\tTest-mlogloss:0.474033\n",
            "[550]\tTrain-mlogloss:0.283779\tTest-mlogloss:0.474015\n",
            "[551]\tTrain-mlogloss:0.283546\tTest-mlogloss:0.47395\n",
            "[552]\tTrain-mlogloss:0.28316\tTest-mlogloss:0.473952\n",
            "[553]\tTrain-mlogloss:0.282954\tTest-mlogloss:0.474008\n",
            "[554]\tTrain-mlogloss:0.282711\tTest-mlogloss:0.474026\n",
            "[555]\tTrain-mlogloss:0.282483\tTest-mlogloss:0.473959\n",
            "[556]\tTrain-mlogloss:0.282264\tTest-mlogloss:0.473924\n",
            "[557]\tTrain-mlogloss:0.282024\tTest-mlogloss:0.473948\n",
            "[558]\tTrain-mlogloss:0.281769\tTest-mlogloss:0.473929\n",
            "[559]\tTrain-mlogloss:0.281555\tTest-mlogloss:0.473816\n",
            "[560]\tTrain-mlogloss:0.281257\tTest-mlogloss:0.473803\n",
            "[561]\tTrain-mlogloss:0.280996\tTest-mlogloss:0.473689\n",
            "[562]\tTrain-mlogloss:0.280722\tTest-mlogloss:0.473607\n",
            "[563]\tTrain-mlogloss:0.280471\tTest-mlogloss:0.473561\n",
            "[564]\tTrain-mlogloss:0.280238\tTest-mlogloss:0.473527\n",
            "[565]\tTrain-mlogloss:0.279954\tTest-mlogloss:0.473484\n",
            "[566]\tTrain-mlogloss:0.279571\tTest-mlogloss:0.473447\n",
            "[567]\tTrain-mlogloss:0.279302\tTest-mlogloss:0.47343\n",
            "[568]\tTrain-mlogloss:0.279111\tTest-mlogloss:0.473425\n",
            "[569]\tTrain-mlogloss:0.278871\tTest-mlogloss:0.473415\n",
            "[570]\tTrain-mlogloss:0.278588\tTest-mlogloss:0.473356\n",
            "[571]\tTrain-mlogloss:0.278344\tTest-mlogloss:0.473327\n",
            "[572]\tTrain-mlogloss:0.278079\tTest-mlogloss:0.473295\n",
            "[573]\tTrain-mlogloss:0.277837\tTest-mlogloss:0.473262\n",
            "[574]\tTrain-mlogloss:0.277526\tTest-mlogloss:0.473148\n",
            "[575]\tTrain-mlogloss:0.277195\tTest-mlogloss:0.473067\n",
            "[576]\tTrain-mlogloss:0.276936\tTest-mlogloss:0.472958\n",
            "[577]\tTrain-mlogloss:0.276777\tTest-mlogloss:0.47299\n",
            "[578]\tTrain-mlogloss:0.276509\tTest-mlogloss:0.472926\n",
            "[579]\tTrain-mlogloss:0.276209\tTest-mlogloss:0.472854\n",
            "[580]\tTrain-mlogloss:0.275979\tTest-mlogloss:0.472845\n",
            "[581]\tTrain-mlogloss:0.275779\tTest-mlogloss:0.472847\n",
            "[582]\tTrain-mlogloss:0.275494\tTest-mlogloss:0.472709\n",
            "[583]\tTrain-mlogloss:0.275255\tTest-mlogloss:0.472705\n",
            "[584]\tTrain-mlogloss:0.27502\tTest-mlogloss:0.472746\n",
            "[585]\tTrain-mlogloss:0.274795\tTest-mlogloss:0.472698\n",
            "[586]\tTrain-mlogloss:0.27454\tTest-mlogloss:0.472641\n",
            "[587]\tTrain-mlogloss:0.274269\tTest-mlogloss:0.472647\n",
            "[588]\tTrain-mlogloss:0.274021\tTest-mlogloss:0.472604\n",
            "[589]\tTrain-mlogloss:0.273736\tTest-mlogloss:0.472627\n",
            "[590]\tTrain-mlogloss:0.273457\tTest-mlogloss:0.472603\n",
            "[591]\tTrain-mlogloss:0.273163\tTest-mlogloss:0.472547\n",
            "[592]\tTrain-mlogloss:0.27293\tTest-mlogloss:0.472509\n",
            "[593]\tTrain-mlogloss:0.272712\tTest-mlogloss:0.472444\n",
            "[594]\tTrain-mlogloss:0.272512\tTest-mlogloss:0.472529\n",
            "[595]\tTrain-mlogloss:0.272305\tTest-mlogloss:0.472502\n",
            "[596]\tTrain-mlogloss:0.272116\tTest-mlogloss:0.472478\n",
            "[597]\tTrain-mlogloss:0.271921\tTest-mlogloss:0.472476\n",
            "[598]\tTrain-mlogloss:0.271643\tTest-mlogloss:0.472422\n",
            "[599]\tTrain-mlogloss:0.271347\tTest-mlogloss:0.472373\n",
            "[600]\tTrain-mlogloss:0.27109\tTest-mlogloss:0.472318\n",
            "[601]\tTrain-mlogloss:0.270853\tTest-mlogloss:0.47238\n",
            "[602]\tTrain-mlogloss:0.270578\tTest-mlogloss:0.472341\n",
            "[603]\tTrain-mlogloss:0.270337\tTest-mlogloss:0.472247\n",
            "[604]\tTrain-mlogloss:0.269982\tTest-mlogloss:0.472189\n",
            "[605]\tTrain-mlogloss:0.269773\tTest-mlogloss:0.472189\n",
            "[606]\tTrain-mlogloss:0.269466\tTest-mlogloss:0.472171\n",
            "[607]\tTrain-mlogloss:0.269302\tTest-mlogloss:0.472156\n",
            "[608]\tTrain-mlogloss:0.269062\tTest-mlogloss:0.472084\n",
            "[609]\tTrain-mlogloss:0.268721\tTest-mlogloss:0.472035\n",
            "[610]\tTrain-mlogloss:0.2685\tTest-mlogloss:0.471942\n",
            "[611]\tTrain-mlogloss:0.268219\tTest-mlogloss:0.471901\n",
            "[612]\tTrain-mlogloss:0.267982\tTest-mlogloss:0.471868\n",
            "[613]\tTrain-mlogloss:0.267764\tTest-mlogloss:0.471892\n",
            "[614]\tTrain-mlogloss:0.267574\tTest-mlogloss:0.471913\n",
            "[615]\tTrain-mlogloss:0.267405\tTest-mlogloss:0.471895\n",
            "[616]\tTrain-mlogloss:0.267181\tTest-mlogloss:0.471878\n",
            "[617]\tTrain-mlogloss:0.266938\tTest-mlogloss:0.47186\n",
            "[618]\tTrain-mlogloss:0.266665\tTest-mlogloss:0.471755\n",
            "[619]\tTrain-mlogloss:0.266419\tTest-mlogloss:0.471711\n",
            "[620]\tTrain-mlogloss:0.266046\tTest-mlogloss:0.471606\n",
            "[621]\tTrain-mlogloss:0.265855\tTest-mlogloss:0.471589\n",
            "[622]\tTrain-mlogloss:0.265664\tTest-mlogloss:0.471602\n",
            "[623]\tTrain-mlogloss:0.265412\tTest-mlogloss:0.471523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[624]\tTrain-mlogloss:0.265207\tTest-mlogloss:0.471529\n",
            "[625]\tTrain-mlogloss:0.264936\tTest-mlogloss:0.471577\n",
            "[626]\tTrain-mlogloss:0.264696\tTest-mlogloss:0.471605\n",
            "[627]\tTrain-mlogloss:0.264336\tTest-mlogloss:0.471589\n",
            "[628]\tTrain-mlogloss:0.264178\tTest-mlogloss:0.471604\n",
            "[629]\tTrain-mlogloss:0.263917\tTest-mlogloss:0.47164\n",
            "[630]\tTrain-mlogloss:0.263681\tTest-mlogloss:0.471522\n",
            "[631]\tTrain-mlogloss:0.263468\tTest-mlogloss:0.471445\n",
            "[632]\tTrain-mlogloss:0.263259\tTest-mlogloss:0.471445\n",
            "[633]\tTrain-mlogloss:0.262982\tTest-mlogloss:0.471445\n",
            "[634]\tTrain-mlogloss:0.262733\tTest-mlogloss:0.471401\n",
            "[635]\tTrain-mlogloss:0.262502\tTest-mlogloss:0.47137\n",
            "[636]\tTrain-mlogloss:0.262257\tTest-mlogloss:0.471389\n",
            "[637]\tTrain-mlogloss:0.261994\tTest-mlogloss:0.471347\n",
            "[638]\tTrain-mlogloss:0.261803\tTest-mlogloss:0.47133\n",
            "[639]\tTrain-mlogloss:0.261591\tTest-mlogloss:0.471285\n",
            "[640]\tTrain-mlogloss:0.261323\tTest-mlogloss:0.471162\n",
            "[641]\tTrain-mlogloss:0.261145\tTest-mlogloss:0.471171\n",
            "[642]\tTrain-mlogloss:0.260907\tTest-mlogloss:0.471143\n",
            "[643]\tTrain-mlogloss:0.260674\tTest-mlogloss:0.471072\n",
            "[644]\tTrain-mlogloss:0.260352\tTest-mlogloss:0.471021\n",
            "[645]\tTrain-mlogloss:0.260169\tTest-mlogloss:0.471038\n",
            "[646]\tTrain-mlogloss:0.259951\tTest-mlogloss:0.471049\n",
            "[647]\tTrain-mlogloss:0.259778\tTest-mlogloss:0.471012\n",
            "[648]\tTrain-mlogloss:0.259617\tTest-mlogloss:0.470972\n",
            "[649]\tTrain-mlogloss:0.259328\tTest-mlogloss:0.470884\n",
            "[650]\tTrain-mlogloss:0.259131\tTest-mlogloss:0.470943\n",
            "[651]\tTrain-mlogloss:0.258925\tTest-mlogloss:0.47096\n",
            "[652]\tTrain-mlogloss:0.258734\tTest-mlogloss:0.470929\n",
            "[653]\tTrain-mlogloss:0.258442\tTest-mlogloss:0.470907\n",
            "[654]\tTrain-mlogloss:0.25819\tTest-mlogloss:0.470899\n",
            "[655]\tTrain-mlogloss:0.257949\tTest-mlogloss:0.47093\n",
            "[656]\tTrain-mlogloss:0.257711\tTest-mlogloss:0.470875\n",
            "[657]\tTrain-mlogloss:0.25748\tTest-mlogloss:0.470834\n",
            "[658]\tTrain-mlogloss:0.257195\tTest-mlogloss:0.470792\n",
            "[659]\tTrain-mlogloss:0.256976\tTest-mlogloss:0.470716\n",
            "[660]\tTrain-mlogloss:0.256708\tTest-mlogloss:0.470562\n",
            "[661]\tTrain-mlogloss:0.256445\tTest-mlogloss:0.4706\n",
            "[662]\tTrain-mlogloss:0.256252\tTest-mlogloss:0.470611\n",
            "[663]\tTrain-mlogloss:0.255992\tTest-mlogloss:0.470448\n",
            "[664]\tTrain-mlogloss:0.255723\tTest-mlogloss:0.470385\n",
            "[665]\tTrain-mlogloss:0.25555\tTest-mlogloss:0.470369\n",
            "[666]\tTrain-mlogloss:0.255303\tTest-mlogloss:0.47033\n",
            "[667]\tTrain-mlogloss:0.255117\tTest-mlogloss:0.470287\n",
            "[668]\tTrain-mlogloss:0.254836\tTest-mlogloss:0.470167\n",
            "[669]\tTrain-mlogloss:0.25456\tTest-mlogloss:0.470084\n",
            "[670]\tTrain-mlogloss:0.254275\tTest-mlogloss:0.469979\n",
            "[671]\tTrain-mlogloss:0.254052\tTest-mlogloss:0.469984\n",
            "[672]\tTrain-mlogloss:0.253763\tTest-mlogloss:0.469962\n",
            "[673]\tTrain-mlogloss:0.253547\tTest-mlogloss:0.469893\n",
            "[674]\tTrain-mlogloss:0.253318\tTest-mlogloss:0.469824\n",
            "[675]\tTrain-mlogloss:0.253145\tTest-mlogloss:0.469808\n",
            "[676]\tTrain-mlogloss:0.252921\tTest-mlogloss:0.469763\n",
            "[677]\tTrain-mlogloss:0.252727\tTest-mlogloss:0.469789\n",
            "[678]\tTrain-mlogloss:0.252527\tTest-mlogloss:0.469777\n",
            "[679]\tTrain-mlogloss:0.25229\tTest-mlogloss:0.469706\n",
            "[680]\tTrain-mlogloss:0.25209\tTest-mlogloss:0.469663\n",
            "[681]\tTrain-mlogloss:0.251873\tTest-mlogloss:0.46961\n",
            "[682]\tTrain-mlogloss:0.251645\tTest-mlogloss:0.469663\n",
            "[683]\tTrain-mlogloss:0.251444\tTest-mlogloss:0.469656\n",
            "[684]\tTrain-mlogloss:0.251309\tTest-mlogloss:0.469668\n",
            "[685]\tTrain-mlogloss:0.251144\tTest-mlogloss:0.469665\n",
            "[686]\tTrain-mlogloss:0.250964\tTest-mlogloss:0.469637\n",
            "[687]\tTrain-mlogloss:0.250754\tTest-mlogloss:0.469621\n",
            "[688]\tTrain-mlogloss:0.250564\tTest-mlogloss:0.46962\n",
            "[689]\tTrain-mlogloss:0.250425\tTest-mlogloss:0.469589\n",
            "[690]\tTrain-mlogloss:0.250262\tTest-mlogloss:0.469611\n",
            "[691]\tTrain-mlogloss:0.250037\tTest-mlogloss:0.469582\n",
            "[692]\tTrain-mlogloss:0.249824\tTest-mlogloss:0.469513\n",
            "[693]\tTrain-mlogloss:0.249586\tTest-mlogloss:0.469503\n",
            "[694]\tTrain-mlogloss:0.24932\tTest-mlogloss:0.469437\n",
            "[695]\tTrain-mlogloss:0.249106\tTest-mlogloss:0.469418\n",
            "[696]\tTrain-mlogloss:0.24897\tTest-mlogloss:0.469427\n",
            "[697]\tTrain-mlogloss:0.248829\tTest-mlogloss:0.469462\n",
            "[698]\tTrain-mlogloss:0.248629\tTest-mlogloss:0.469441\n",
            "[699]\tTrain-mlogloss:0.248453\tTest-mlogloss:0.469413\n",
            "[700]\tTrain-mlogloss:0.248322\tTest-mlogloss:0.469416\n",
            "[701]\tTrain-mlogloss:0.248117\tTest-mlogloss:0.469453\n",
            "[702]\tTrain-mlogloss:0.248016\tTest-mlogloss:0.469454\n",
            "[703]\tTrain-mlogloss:0.24778\tTest-mlogloss:0.469438\n",
            "[704]\tTrain-mlogloss:0.247554\tTest-mlogloss:0.469342\n",
            "[705]\tTrain-mlogloss:0.247409\tTest-mlogloss:0.469284\n",
            "[706]\tTrain-mlogloss:0.247179\tTest-mlogloss:0.469242\n",
            "[707]\tTrain-mlogloss:0.247015\tTest-mlogloss:0.469246\n",
            "[708]\tTrain-mlogloss:0.246791\tTest-mlogloss:0.469257\n",
            "[709]\tTrain-mlogloss:0.246647\tTest-mlogloss:0.469302\n",
            "[710]\tTrain-mlogloss:0.246373\tTest-mlogloss:0.469305\n",
            "[711]\tTrain-mlogloss:0.246105\tTest-mlogloss:0.469299\n",
            "[712]\tTrain-mlogloss:0.245856\tTest-mlogloss:0.469246\n",
            "[713]\tTrain-mlogloss:0.245653\tTest-mlogloss:0.469293\n",
            "[714]\tTrain-mlogloss:0.245479\tTest-mlogloss:0.469291\n",
            "[715]\tTrain-mlogloss:0.245251\tTest-mlogloss:0.469135\n",
            "[716]\tTrain-mlogloss:0.245026\tTest-mlogloss:0.469055\n",
            "[717]\tTrain-mlogloss:0.244771\tTest-mlogloss:0.469075\n",
            "[718]\tTrain-mlogloss:0.244635\tTest-mlogloss:0.469052\n",
            "[719]\tTrain-mlogloss:0.244519\tTest-mlogloss:0.469034\n",
            "[720]\tTrain-mlogloss:0.244317\tTest-mlogloss:0.469049\n",
            "[721]\tTrain-mlogloss:0.244024\tTest-mlogloss:0.469035\n",
            "[722]\tTrain-mlogloss:0.24379\tTest-mlogloss:0.469016\n",
            "[723]\tTrain-mlogloss:0.243585\tTest-mlogloss:0.469027\n",
            "[724]\tTrain-mlogloss:0.243391\tTest-mlogloss:0.468988\n",
            "[725]\tTrain-mlogloss:0.243227\tTest-mlogloss:0.469051\n",
            "[726]\tTrain-mlogloss:0.243023\tTest-mlogloss:0.468957\n",
            "[727]\tTrain-mlogloss:0.24284\tTest-mlogloss:0.468993\n",
            "[728]\tTrain-mlogloss:0.242628\tTest-mlogloss:0.469018\n",
            "[729]\tTrain-mlogloss:0.242452\tTest-mlogloss:0.468996\n",
            "[730]\tTrain-mlogloss:0.242232\tTest-mlogloss:0.469005\n",
            "[731]\tTrain-mlogloss:0.241994\tTest-mlogloss:0.468959\n",
            "[732]\tTrain-mlogloss:0.241775\tTest-mlogloss:0.468881\n",
            "[733]\tTrain-mlogloss:0.241526\tTest-mlogloss:0.46888\n",
            "[734]\tTrain-mlogloss:0.24134\tTest-mlogloss:0.468898\n",
            "[735]\tTrain-mlogloss:0.241162\tTest-mlogloss:0.468856\n",
            "[736]\tTrain-mlogloss:0.240987\tTest-mlogloss:0.46881\n",
            "[737]\tTrain-mlogloss:0.240733\tTest-mlogloss:0.468804\n",
            "[738]\tTrain-mlogloss:0.240551\tTest-mlogloss:0.468804\n",
            "[739]\tTrain-mlogloss:0.240352\tTest-mlogloss:0.468769\n",
            "[740]\tTrain-mlogloss:0.240111\tTest-mlogloss:0.468828\n",
            "[741]\tTrain-mlogloss:0.239908\tTest-mlogloss:0.468771\n",
            "[742]\tTrain-mlogloss:0.239741\tTest-mlogloss:0.468788\n",
            "[743]\tTrain-mlogloss:0.239569\tTest-mlogloss:0.468869\n",
            "[744]\tTrain-mlogloss:0.239256\tTest-mlogloss:0.468889\n",
            "[745]\tTrain-mlogloss:0.239034\tTest-mlogloss:0.468824\n",
            "[746]\tTrain-mlogloss:0.238883\tTest-mlogloss:0.468797\n",
            "[747]\tTrain-mlogloss:0.238638\tTest-mlogloss:0.468811\n",
            "[748]\tTrain-mlogloss:0.238379\tTest-mlogloss:0.468782\n",
            "[749]\tTrain-mlogloss:0.23818\tTest-mlogloss:0.46882\n",
            "[750]\tTrain-mlogloss:0.237964\tTest-mlogloss:0.468804\n",
            "[751]\tTrain-mlogloss:0.237694\tTest-mlogloss:0.468804\n",
            "[752]\tTrain-mlogloss:0.237589\tTest-mlogloss:0.468847\n",
            "[753]\tTrain-mlogloss:0.237374\tTest-mlogloss:0.468822\n",
            "[754]\tTrain-mlogloss:0.23722\tTest-mlogloss:0.468825\n",
            "[755]\tTrain-mlogloss:0.237073\tTest-mlogloss:0.468783\n",
            "[756]\tTrain-mlogloss:0.236847\tTest-mlogloss:0.468794\n",
            "[757]\tTrain-mlogloss:0.236586\tTest-mlogloss:0.468756\n",
            "[758]\tTrain-mlogloss:0.236313\tTest-mlogloss:0.468678\n",
            "[759]\tTrain-mlogloss:0.23607\tTest-mlogloss:0.468721\n",
            "[760]\tTrain-mlogloss:0.235851\tTest-mlogloss:0.468723\n",
            "[761]\tTrain-mlogloss:0.235642\tTest-mlogloss:0.468709\n",
            "[762]\tTrain-mlogloss:0.235446\tTest-mlogloss:0.468733\n",
            "[763]\tTrain-mlogloss:0.235215\tTest-mlogloss:0.468674\n",
            "[764]\tTrain-mlogloss:0.235059\tTest-mlogloss:0.468665\n",
            "[765]\tTrain-mlogloss:0.234948\tTest-mlogloss:0.46867\n",
            "[766]\tTrain-mlogloss:0.234791\tTest-mlogloss:0.468712\n",
            "[767]\tTrain-mlogloss:0.234631\tTest-mlogloss:0.468735\n",
            "[768]\tTrain-mlogloss:0.234394\tTest-mlogloss:0.468672\n",
            "[769]\tTrain-mlogloss:0.234179\tTest-mlogloss:0.46864\n",
            "[770]\tTrain-mlogloss:0.233976\tTest-mlogloss:0.468695\n",
            "[771]\tTrain-mlogloss:0.233835\tTest-mlogloss:0.468771\n",
            "[772]\tTrain-mlogloss:0.233618\tTest-mlogloss:0.468724\n",
            "[773]\tTrain-mlogloss:0.233395\tTest-mlogloss:0.46877\n",
            "[774]\tTrain-mlogloss:0.233286\tTest-mlogloss:0.468807\n",
            "[775]\tTrain-mlogloss:0.233124\tTest-mlogloss:0.468853\n",
            "[776]\tTrain-mlogloss:0.232914\tTest-mlogloss:0.468872\n",
            "[777]\tTrain-mlogloss:0.232697\tTest-mlogloss:0.468837\n",
            "[778]\tTrain-mlogloss:0.23255\tTest-mlogloss:0.468809\n",
            "[779]\tTrain-mlogloss:0.232368\tTest-mlogloss:0.468819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[780]\tTrain-mlogloss:0.23223\tTest-mlogloss:0.468753\n",
            "[781]\tTrain-mlogloss:0.232037\tTest-mlogloss:0.468723\n",
            "[782]\tTrain-mlogloss:0.231793\tTest-mlogloss:0.468556\n",
            "[783]\tTrain-mlogloss:0.231661\tTest-mlogloss:0.468581\n",
            "[784]\tTrain-mlogloss:0.231504\tTest-mlogloss:0.468542\n",
            "[785]\tTrain-mlogloss:0.231348\tTest-mlogloss:0.46856\n",
            "[786]\tTrain-mlogloss:0.231168\tTest-mlogloss:0.468621\n",
            "[787]\tTrain-mlogloss:0.231\tTest-mlogloss:0.46857\n",
            "[788]\tTrain-mlogloss:0.2308\tTest-mlogloss:0.468538\n",
            "[789]\tTrain-mlogloss:0.230583\tTest-mlogloss:0.468521\n",
            "[790]\tTrain-mlogloss:0.230373\tTest-mlogloss:0.468436\n",
            "[791]\tTrain-mlogloss:0.230175\tTest-mlogloss:0.468418\n",
            "[792]\tTrain-mlogloss:0.230024\tTest-mlogloss:0.468417\n",
            "[793]\tTrain-mlogloss:0.229809\tTest-mlogloss:0.468455\n",
            "[794]\tTrain-mlogloss:0.229608\tTest-mlogloss:0.468417\n",
            "[795]\tTrain-mlogloss:0.229412\tTest-mlogloss:0.468406\n",
            "[796]\tTrain-mlogloss:0.229226\tTest-mlogloss:0.468417\n",
            "[797]\tTrain-mlogloss:0.22911\tTest-mlogloss:0.468393\n",
            "[798]\tTrain-mlogloss:0.228894\tTest-mlogloss:0.468344\n",
            "[799]\tTrain-mlogloss:0.228741\tTest-mlogloss:0.468354\n",
            "[800]\tTrain-mlogloss:0.228555\tTest-mlogloss:0.468264\n",
            "[801]\tTrain-mlogloss:0.228371\tTest-mlogloss:0.468288\n",
            "[802]\tTrain-mlogloss:0.228213\tTest-mlogloss:0.468304\n",
            "[803]\tTrain-mlogloss:0.228044\tTest-mlogloss:0.468286\n",
            "[804]\tTrain-mlogloss:0.227801\tTest-mlogloss:0.468231\n",
            "[805]\tTrain-mlogloss:0.227605\tTest-mlogloss:0.468167\n",
            "[806]\tTrain-mlogloss:0.227457\tTest-mlogloss:0.468131\n",
            "[807]\tTrain-mlogloss:0.22724\tTest-mlogloss:0.468134\n",
            "[808]\tTrain-mlogloss:0.227089\tTest-mlogloss:0.468121\n",
            "[809]\tTrain-mlogloss:0.226979\tTest-mlogloss:0.468136\n",
            "[810]\tTrain-mlogloss:0.226835\tTest-mlogloss:0.468101\n",
            "[811]\tTrain-mlogloss:0.226636\tTest-mlogloss:0.468082\n",
            "[812]\tTrain-mlogloss:0.226432\tTest-mlogloss:0.468069\n",
            "[813]\tTrain-mlogloss:0.226271\tTest-mlogloss:0.468039\n",
            "[814]\tTrain-mlogloss:0.226109\tTest-mlogloss:0.468047\n",
            "[815]\tTrain-mlogloss:0.225808\tTest-mlogloss:0.468065\n",
            "[816]\tTrain-mlogloss:0.225631\tTest-mlogloss:0.468097\n",
            "[817]\tTrain-mlogloss:0.225384\tTest-mlogloss:0.468054\n",
            "[818]\tTrain-mlogloss:0.225164\tTest-mlogloss:0.467979\n",
            "[819]\tTrain-mlogloss:0.22497\tTest-mlogloss:0.467937\n",
            "[820]\tTrain-mlogloss:0.224776\tTest-mlogloss:0.468002\n",
            "[821]\tTrain-mlogloss:0.224605\tTest-mlogloss:0.468009\n",
            "[822]\tTrain-mlogloss:0.224437\tTest-mlogloss:0.468084\n",
            "[823]\tTrain-mlogloss:0.224285\tTest-mlogloss:0.468087\n",
            "[824]\tTrain-mlogloss:0.224055\tTest-mlogloss:0.468054\n",
            "[825]\tTrain-mlogloss:0.223851\tTest-mlogloss:0.468125\n",
            "[826]\tTrain-mlogloss:0.223608\tTest-mlogloss:0.468157\n",
            "[827]\tTrain-mlogloss:0.223439\tTest-mlogloss:0.468131\n",
            "[828]\tTrain-mlogloss:0.22321\tTest-mlogloss:0.468174\n",
            "[829]\tTrain-mlogloss:0.222954\tTest-mlogloss:0.468129\n",
            "[830]\tTrain-mlogloss:0.22271\tTest-mlogloss:0.468135\n",
            "[831]\tTrain-mlogloss:0.222563\tTest-mlogloss:0.468121\n",
            "[832]\tTrain-mlogloss:0.222405\tTest-mlogloss:0.468036\n",
            "[833]\tTrain-mlogloss:0.222205\tTest-mlogloss:0.468037\n",
            "[834]\tTrain-mlogloss:0.22209\tTest-mlogloss:0.468062\n",
            "[835]\tTrain-mlogloss:0.221901\tTest-mlogloss:0.468098\n",
            "[836]\tTrain-mlogloss:0.221666\tTest-mlogloss:0.468029\n",
            "[837]\tTrain-mlogloss:0.221477\tTest-mlogloss:0.46808\n",
            "[838]\tTrain-mlogloss:0.221293\tTest-mlogloss:0.468064\n",
            "[839]\tTrain-mlogloss:0.221064\tTest-mlogloss:0.468035\n",
            "Stopping. Best iteration:\n",
            "[819]\tTrain-mlogloss:0.22497\tTest-mlogloss:0.467937\n",
            "\n",
            "Training Class Probabilities for First 5 Instances:\n",
            " [[4.9844184e-05 6.9372177e-02 8.5674280e-01 8.6777434e-03 1.0010292e-06\n",
            "  2.8729174e-04 6.4836718e-02 2.3504343e-05 8.8882243e-06]\n",
            " [3.1389707e-05 8.8631797e-01 1.0836417e-01 4.9758391e-03 3.6361993e-07\n",
            "  4.7017700e-05 2.1747377e-04 3.7665020e-05 8.1120725e-06]\n",
            " [4.9069218e-05 7.3243850e-01 2.6268178e-01 1.8789043e-03 4.1920401e-04\n",
            "  1.4515994e-04 1.2019466e-04 2.0674316e-03 1.9975394e-04]\n",
            " [6.7047938e-03 5.4456745e-03 4.0798704e-03 3.5606921e-03 1.9896209e-04\n",
            "  9.3619955e-01 4.2241827e-02 9.5457648e-04 6.1407266e-04]\n",
            " [1.0348459e-03 7.4048882e-07 8.2638854e-07 3.1749260e-05 1.8552701e-08\n",
            "  9.9859911e-01 6.6823115e-05 1.7528633e-04 9.0634836e-05]]\n",
            "Validation Class Probabilities for First 5 Instances:\n",
            " [[7.2848506e-04 8.9173287e-01 9.3639968e-03 8.6400948e-02 1.1589239e-05\n",
            "  3.2631191e-03 9.0438809e-04 1.2422038e-03 6.3523571e-03]\n",
            " [1.0593581e-06 2.7923760e-04 1.8005558e-05 6.5093848e-04 9.9903476e-01\n",
            "  2.5785930e-06 3.8228964e-06 7.2202788e-06 2.3744601e-06]\n",
            " [6.8782327e-05 3.9982647e-01 2.3272060e-01 3.6253268e-01 1.2162398e-06\n",
            "  4.3401698e-04 3.3715577e-03 5.3852604e-04 5.0618843e-04]\n",
            " [9.6280667e-07 9.8677617e-01 9.1692135e-03 4.0273303e-03 1.0159204e-06\n",
            "  1.0095715e-05 1.3714976e-05 8.2723409e-07 6.3683956e-07]\n",
            " [3.0424712e-02 5.7978858e-03 2.4987208e-03 3.3988126e-04 2.0563092e-04\n",
            "  6.6049960e-03 2.9065943e-04 3.9346125e-03 9.4990295e-01]]\n",
            "Best Predictions for Train:\n",
            " [3 2 2 ... 2 6 6]\n",
            "Best Predictions for Validation:\n",
            " [2 5 2 ... 2 2 2]\n",
            "Precision Score of the Training Set=  0.947052452046258\n",
            "Precision Score of the Validation Set=  0.8098757825236489\n",
            "Recall Score of the Training Set=  0.9184636160533136\n",
            "Recall Score of the Validation Set=  0.7686112919660354\n",
            "F1 Score of the Training Set=  0.9311314351938879\n",
            "F1 Score of the Validation Set=  0.7852967471088035\n",
            "Accuracy Score the Training Set=  0.9329320027473638\n",
            "Accuracy Score of the Validation Set=  0.8209437621202327\n",
            "Logloss Score Training Set=  0.22106428668160558\n",
            "Logloss Score of the Validation Set=  0.46803515527699024\n",
            "Confusion Matrix of the Training Set: \n",
            "\n",
            "[[ 1414    13     2     0     0    18    13    38    45]\n",
            " [    3 12230   577    39     3     5    26     8     7]\n",
            " [    2  1409  4924    28     0     3    30     6     1]\n",
            " [    2   323   127  1672     2    16     9     2     0]\n",
            " [    1     1     0     0  2188     0     1     0     0]\n",
            " [   21    24     5     3     0 11167    22    48    18]\n",
            " [   10    98    44     7     2    20  2074    13     3]\n",
            " [   12    28    12     0     0    29    20  6651    19]\n",
            " [   15    31     2     1     0    12     8    33  3862]]\n",
            "Confusion Matrix of the Validation Set: \n",
            "\n",
            "[[ 214   10    1    3    1   28   10   56   63]\n",
            " [   3 2733  390   42    5   11   24    4   12]\n",
            " [   0  663  857   26    0    6   40    5    4]\n",
            " [   0  146   69  301    1   10   10    0    1]\n",
            " [   1    6    0    0  539    1    1    0    0]\n",
            " [  15   15    5    5    0 2685   28   41   33]\n",
            " [  10   50   40    9    3   33  387   30    6]\n",
            " [  24   16    3    0    0   38   11 1578   23]\n",
            " [  37   10    0    1    2   22    5   48  866]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7_cOFHQybGt",
        "colab_type": "text"
      },
      "source": [
        "## Retune maxdepth and min child weight\n",
        "converges reaaly slow when eta=0.1\n",
        "mlogloss 0.4833872 for 998 rounds for (4,2) so eta changed to 0.3 back"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LniFxmkpybGu",
        "colab_type": "code",
        "colab": {},
        "outputId": "e86e8f22-a744-43a2-83e7-5f875ec1708a"
      },
      "source": [
        "gridsearch_params = [\n",
        "    (max_depth, min_child_weight)\n",
        "    for max_depth in range(4,7)\n",
        "    for min_child_weight in range(2,6)\n",
        "]\n",
        "params = {\n",
        "    'eta': 0.3,  \n",
        "    'n_estimators': 1000,\n",
        "    'silent': True,  # option for logging\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass tasks\n",
        "    'num_class': 9,  # number of classes to predic\n",
        "    'max_depth': 6,  # depth of the trees in the boosting process\n",
        "    'seed': 42,\n",
        "    'eval_metric': 'mlogloss', #cross-entropy is chosen as evaluation metric\n",
        "    'min_child_weight':1,\n",
        "    'gamma':0.0,\n",
        "    'subsample':0.9,\n",
        "    'colsample_bytree':0.8,\n",
        "    'nthread': 16,\n",
        "    #'scale_pos_weight':1,\n",
        "    'reg_alpha':0,\n",
        "    'max_delta_step':1\n",
        "    }\n",
        "# Define initial best params and mlogloss\n",
        "min_mlogloss = float(\"Inf\")\n",
        "best_params = None\n",
        "for max_depth, min_child_weight in gridsearch_params:\n",
        "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
        "                             max_depth,\n",
        "                             min_child_weight))\n",
        "\n",
        "    # Update our parameters\n",
        "    params['max_depth'] = max_depth\n",
        "    params['min_child_weight'] = min_child_weight\n",
        "\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=num_boost_round,\n",
        "        seed=42,\n",
        "        nfold=5,\n",
        "        metrics={'mlogloss'},\n",
        "        early_stopping_rounds=20,\n",
        "        #fpreproc = fpreproc, #use custom fn to update weights\n",
        "        stratified =True,\n",
        "        verbose_eval =10\n",
        "    )\n",
        "\n",
        "    # Update best mlogloss\n",
        "    mean_mlogloss = cv_results['test-mlogloss-mean'].min()\n",
        "    boost_rounds = cv_results['test-mlogloss-mean'].argmin()\n",
        "    print(\"\\tmlogloss {} for {} rounds\".format(mean_mlogloss, boost_rounds))\n",
        "    if mean_mlogloss < min_mlogloss:\n",
        "        min_mlogloss = mean_mlogloss\n",
        "        best_params = (max_depth,min_child_weight)\n",
        "\n",
        "print(\"Best params: {}, {}, mlogloss: {}\".format(best_params[0], best_params[1], min_mlogloss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV with max_depth=4, min_child_weight=2\n",
            "[0]\ttrain-mlogloss:1.93308+0.00238313\ttest-mlogloss:1.93568+0.0027791\n",
            "[10]\ttrain-mlogloss:0.810071+0.00458795\ttest-mlogloss:0.830687+0.00449577\n",
            "[20]\ttrain-mlogloss:0.634943+0.00278677\ttest-mlogloss:0.669269+0.00479685\n",
            "[30]\ttrain-mlogloss:0.567823+0.0023498\ttest-mlogloss:0.614136+0.00488411\n",
            "[40]\ttrain-mlogloss:0.528076+0.0018457\ttest-mlogloss:0.585504+0.00504872\n",
            "[50]\ttrain-mlogloss:0.499512+0.00141127\ttest-mlogloss:0.567331+0.00537546\n",
            "[60]\ttrain-mlogloss:0.476848+0.00161506\ttest-mlogloss:0.554411+0.00552743\n",
            "[70]\ttrain-mlogloss:0.457791+0.00143614\ttest-mlogloss:0.54404+0.00496098\n",
            "[80]\ttrain-mlogloss:0.440992+0.00159007\ttest-mlogloss:0.535971+0.0045596\n",
            "[90]\ttrain-mlogloss:0.426245+0.0015917\ttest-mlogloss:0.529611+0.00466163\n",
            "[100]\ttrain-mlogloss:0.413077+0.00165618\ttest-mlogloss:0.524263+0.00511038\n",
            "[110]\ttrain-mlogloss:0.400875+0.0018221\ttest-mlogloss:0.520155+0.00507315\n",
            "[120]\ttrain-mlogloss:0.389662+0.00143179\ttest-mlogloss:0.516461+0.0053661\n",
            "[130]\ttrain-mlogloss:0.379541+0.00122591\ttest-mlogloss:0.513174+0.0056613\n",
            "[140]\ttrain-mlogloss:0.369628+0.00103128\ttest-mlogloss:0.510265+0.00548415\n",
            "[150]\ttrain-mlogloss:0.360199+0.00127533\ttest-mlogloss:0.507244+0.00555324\n",
            "[160]\ttrain-mlogloss:0.351283+0.00142744\ttest-mlogloss:0.505319+0.00554146\n",
            "[170]\ttrain-mlogloss:0.342797+0.00119255\ttest-mlogloss:0.503226+0.00575646\n",
            "[180]\ttrain-mlogloss:0.334932+0.00115146\ttest-mlogloss:0.501546+0.00569993\n",
            "[190]\ttrain-mlogloss:0.327254+0.00141648\ttest-mlogloss:0.500329+0.00580464\n",
            "[200]\ttrain-mlogloss:0.319759+0.00166058\ttest-mlogloss:0.499085+0.00578159\n",
            "[210]\ttrain-mlogloss:0.312773+0.00179541\ttest-mlogloss:0.497677+0.00578411\n",
            "[220]\ttrain-mlogloss:0.306315+0.00196397\ttest-mlogloss:0.497057+0.00589861\n",
            "[230]\ttrain-mlogloss:0.299846+0.00182647\ttest-mlogloss:0.495999+0.00615316\n",
            "[240]\ttrain-mlogloss:0.293359+0.00207829\ttest-mlogloss:0.495234+0.00622851\n",
            "[250]\ttrain-mlogloss:0.28744+0.00213725\ttest-mlogloss:0.494658+0.00662277\n",
            "[260]\ttrain-mlogloss:0.281583+0.00188847\ttest-mlogloss:0.494133+0.00711635\n",
            "[270]\ttrain-mlogloss:0.27598+0.00200514\ttest-mlogloss:0.493985+0.0071\n",
            "[280]\ttrain-mlogloss:0.270768+0.00183592\ttest-mlogloss:0.494074+0.00709087\n",
            "[290]\ttrain-mlogloss:0.265608+0.00181706\ttest-mlogloss:0.493821+0.00708653\n",
            "[300]\ttrain-mlogloss:0.26063+0.00170479\ttest-mlogloss:0.493773+0.00704651\n",
            "[310]\ttrain-mlogloss:0.255795+0.00169262\ttest-mlogloss:0.49353+0.00707415\n",
            "[320]\ttrain-mlogloss:0.250908+0.00162684\ttest-mlogloss:0.493535+0.00717937\n",
            "[330]\ttrain-mlogloss:0.246126+0.00161902\ttest-mlogloss:0.493292+0.00737301\n",
            "[340]\ttrain-mlogloss:0.241629+0.0014409\ttest-mlogloss:0.493356+0.00757182\n",
            "[350]\ttrain-mlogloss:0.237463+0.00141281\ttest-mlogloss:0.493444+0.0076583\n",
            "[360]\ttrain-mlogloss:0.233235+0.0012452\ttest-mlogloss:0.49334+0.00751929\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tmlogloss 0.4932514 for 347 rounds\n",
            "CV with max_depth=4, min_child_weight=3\n",
            "[0]\ttrain-mlogloss:1.93315+0.00237476\ttest-mlogloss:1.9357+0.00271275\n",
            "[10]\ttrain-mlogloss:0.81037+0.0038138\ttest-mlogloss:0.830672+0.00416233\n",
            "[20]\ttrain-mlogloss:0.635642+0.00239641\ttest-mlogloss:0.670028+0.00523897\n",
            "[30]\ttrain-mlogloss:0.568205+0.00209274\ttest-mlogloss:0.614674+0.00549501\n",
            "[40]\ttrain-mlogloss:0.528879+0.00181827\ttest-mlogloss:0.586289+0.00573856\n",
            "[50]\ttrain-mlogloss:0.50055+0.00182441\ttest-mlogloss:0.567846+0.00580858\n",
            "[60]\ttrain-mlogloss:0.478429+0.00189634\ttest-mlogloss:0.55477+0.00578257\n",
            "[70]\ttrain-mlogloss:0.459897+0.00193584\ttest-mlogloss:0.544849+0.00473739\n",
            "[80]\ttrain-mlogloss:0.443472+0.00190731\ttest-mlogloss:0.536928+0.00446738\n",
            "[90]\ttrain-mlogloss:0.42872+0.00223237\ttest-mlogloss:0.530147+0.0040279\n",
            "[100]\ttrain-mlogloss:0.415641+0.00219977\ttest-mlogloss:0.524661+0.00397007\n",
            "[110]\ttrain-mlogloss:0.404008+0.00225345\ttest-mlogloss:0.520217+0.00402055\n",
            "[120]\ttrain-mlogloss:0.392831+0.00232044\ttest-mlogloss:0.516629+0.00406412\n",
            "[130]\ttrain-mlogloss:0.382837+0.00193778\ttest-mlogloss:0.513451+0.0044735\n",
            "[140]\ttrain-mlogloss:0.373408+0.00177367\ttest-mlogloss:0.510688+0.0046378\n",
            "[150]\ttrain-mlogloss:0.364223+0.00178823\ttest-mlogloss:0.508008+0.00457887\n",
            "[160]\ttrain-mlogloss:0.355606+0.00199412\ttest-mlogloss:0.50627+0.00466128\n",
            "[170]\ttrain-mlogloss:0.347045+0.00178865\ttest-mlogloss:0.504275+0.00539755\n",
            "[180]\ttrain-mlogloss:0.339256+0.0021176\ttest-mlogloss:0.502495+0.00518809\n",
            "[190]\ttrain-mlogloss:0.331811+0.00208382\ttest-mlogloss:0.501342+0.00524215\n",
            "[200]\ttrain-mlogloss:0.324559+0.00227887\ttest-mlogloss:0.499923+0.00524126\n",
            "[210]\ttrain-mlogloss:0.317866+0.00227661\ttest-mlogloss:0.498524+0.00555341\n",
            "[220]\ttrain-mlogloss:0.311294+0.00218661\ttest-mlogloss:0.497384+0.00585712\n",
            "[230]\ttrain-mlogloss:0.30526+0.00226761\ttest-mlogloss:0.49665+0.00590349\n",
            "[240]\ttrain-mlogloss:0.298822+0.00213513\ttest-mlogloss:0.496042+0.00626458\n",
            "[250]\ttrain-mlogloss:0.292923+0.00220606\ttest-mlogloss:0.495228+0.00596243\n",
            "[260]\ttrain-mlogloss:0.287307+0.00243625\ttest-mlogloss:0.494912+0.00572224\n",
            "[270]\ttrain-mlogloss:0.281887+0.00244586\ttest-mlogloss:0.494385+0.00577024\n",
            "[280]\ttrain-mlogloss:0.276826+0.00255291\ttest-mlogloss:0.493942+0.00582087\n",
            "[290]\ttrain-mlogloss:0.271537+0.00239615\ttest-mlogloss:0.493513+0.00553413\n",
            "[300]\ttrain-mlogloss:0.266706+0.00224955\ttest-mlogloss:0.493473+0.00560161\n",
            "[310]\ttrain-mlogloss:0.262001+0.0021154\ttest-mlogloss:0.492915+0.00572914\n",
            "[320]\ttrain-mlogloss:0.257403+0.00204228\ttest-mlogloss:0.49288+0.00552728\n",
            "[330]\ttrain-mlogloss:0.252916+0.00186465\ttest-mlogloss:0.493158+0.00539195\n",
            "\tmlogloss 0.4928578 for 315 rounds\n",
            "CV with max_depth=4, min_child_weight=4\n",
            "[0]\ttrain-mlogloss:1.93319+0.00238959\ttest-mlogloss:1.93567+0.00270199\n",
            "[10]\ttrain-mlogloss:0.810022+0.00378559\ttest-mlogloss:0.830312+0.00334312\n",
            "[20]\ttrain-mlogloss:0.635328+0.00298513\ttest-mlogloss:0.668865+0.00415551\n",
            "[30]\ttrain-mlogloss:0.569696+0.00314868\ttest-mlogloss:0.614802+0.00397134\n",
            "[40]\ttrain-mlogloss:0.531237+0.00254302\ttest-mlogloss:0.586885+0.00464801\n",
            "[50]\ttrain-mlogloss:0.503272+0.00186501\ttest-mlogloss:0.568144+0.00485201\n",
            "[60]\ttrain-mlogloss:0.48179+0.0018303\ttest-mlogloss:0.555678+0.00456956\n",
            "[70]\ttrain-mlogloss:0.462801+0.00150205\ttest-mlogloss:0.545168+0.00453582\n",
            "[80]\ttrain-mlogloss:0.446469+0.00179083\ttest-mlogloss:0.537796+0.00460651\n",
            "[90]\ttrain-mlogloss:0.431901+0.00188898\ttest-mlogloss:0.531052+0.00421625\n",
            "[100]\ttrain-mlogloss:0.419371+0.00180115\ttest-mlogloss:0.525605+0.00428855\n",
            "[110]\ttrain-mlogloss:0.408053+0.00228814\ttest-mlogloss:0.521214+0.00414066\n",
            "[120]\ttrain-mlogloss:0.397076+0.00234148\ttest-mlogloss:0.516995+0.00419347\n",
            "[130]\ttrain-mlogloss:0.387082+0.00220134\ttest-mlogloss:0.513686+0.00446445\n",
            "[140]\ttrain-mlogloss:0.37766+0.00225045\ttest-mlogloss:0.510921+0.00409571\n",
            "[150]\ttrain-mlogloss:0.368743+0.00232421\ttest-mlogloss:0.508539+0.00402943\n",
            "[160]\ttrain-mlogloss:0.360119+0.00251173\ttest-mlogloss:0.506549+0.00415661\n",
            "[170]\ttrain-mlogloss:0.351927+0.00202884\ttest-mlogloss:0.504782+0.00428969\n",
            "[180]\ttrain-mlogloss:0.344379+0.0020834\ttest-mlogloss:0.503298+0.00417944\n",
            "[190]\ttrain-mlogloss:0.337219+0.00216928\ttest-mlogloss:0.501749+0.00432039\n",
            "[200]\ttrain-mlogloss:0.330033+0.00212043\ttest-mlogloss:0.500336+0.00432951\n",
            "[210]\ttrain-mlogloss:0.323212+0.00225676\ttest-mlogloss:0.49922+0.00450922\n",
            "[220]\ttrain-mlogloss:0.316716+0.00225862\ttest-mlogloss:0.498251+0.00467899\n",
            "[230]\ttrain-mlogloss:0.31031+0.00213119\ttest-mlogloss:0.497193+0.00466498\n",
            "[240]\ttrain-mlogloss:0.304071+0.0023573\ttest-mlogloss:0.496824+0.00513092\n",
            "[250]\ttrain-mlogloss:0.298523+0.00245546\ttest-mlogloss:0.4959+0.00513527\n",
            "[260]\ttrain-mlogloss:0.292818+0.00251468\ttest-mlogloss:0.495041+0.00534102\n",
            "[270]\ttrain-mlogloss:0.287453+0.00253411\ttest-mlogloss:0.494643+0.00539788\n",
            "[280]\ttrain-mlogloss:0.281922+0.00243688\ttest-mlogloss:0.494186+0.00540114\n",
            "[290]\ttrain-mlogloss:0.277078+0.00233314\ttest-mlogloss:0.494014+0.00539227\n",
            "[300]\ttrain-mlogloss:0.271953+0.00224259\ttest-mlogloss:0.493615+0.00584431\n",
            "[310]\ttrain-mlogloss:0.267322+0.00212944\ttest-mlogloss:0.493619+0.00579049\n",
            "[320]\ttrain-mlogloss:0.262845+0.00198248\ttest-mlogloss:0.493279+0.00591992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-145-ad0b83aeb54f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;31m#fpreproc = fpreproc, #use custom fn to update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mstratified\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mverbose_eval\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m     )\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mcv\u001b[1;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks, shuffle)\u001b[0m\n\u001b[0;32m    404\u001b[0m                            evaluation_result_list=None))\n\u001b[0;32m    405\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mfold\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, iteration, fobj)\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m    892\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[1;32m--> 894\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m    895\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOUWSwbmybG6",
        "colab_type": "text"
      },
      "source": [
        "## Oversampling Revisited"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm7umeygybG7",
        "colab_type": "text"
      },
      "source": [
        "To solve class imbalance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prj5dw69ybG_",
        "colab_type": "code",
        "colab": {},
        "outputId": "641ce536-5279-497f-c054-0957e7bd8ff9"
      },
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install --upgrade pip\n",
        "!{sys.executable} -m pip install imbalanced-learn"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pip in d:\\programdata\\anaconda3\\lib\\site-packages (18.0)\n",
            "Requirement already satisfied: imbalanced-learn in d:\\programdata\\anaconda3\\lib\\site-packages (0.3.3)\n",
            "Requirement already satisfied: scipy in d:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.0.0)\n",
            "Requirement already satisfied: numpy in d:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.14.3)\n",
            "Requirement already satisfied: scikit-learn in d:\\programdata\\anaconda3\\lib\\site-packages (from imbalanced-learn) (0.19.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpgvC2OVybHG",
        "colab_type": "code",
        "colab": {},
        "outputId": "70c1a2ae-6d36-46b9-c306-f5d75344e957"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state = 42)\n",
        "X_resampled , y_resampled =  sm.fit_sample(X_trans2,y)\n",
        "headers = X_trans2.keys()\n",
        "X_resampled = pd.DataFrame(X_resampled)\n",
        "print(\"set size:\", len(X_resampled))\n",
        "# work with preprocessed data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_resampled,y_resampled,test_size = 0.20,stratify=y_resampled, random_state=42)\n",
        "#Dmatrix\n",
        "#Preprocessed data:\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dval = xgb.DMatrix(X_val, label=y_val)    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "set size: 145098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZnuiHZbybHM",
        "colab_type": "text"
      },
      "source": [
        "Results are too good to be true when evaluation set is given\n",
        "Stopping. Best iteration:\n",
        "[243]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000191\n",
        "Precision Score of the Training Set=  1.0\n",
        "Precision Score of the Validation Set=  1.0\n",
        "Recall Score of the Training Set=  1.0\n",
        "Recall Score of the Validation Set=  1.0\n",
        "F1 Score of the Training Set=  1.0\n",
        "F1 Score of the Validation Set=  1.0\n",
        "Accuracy Score the Training Set=  1.0\n",
        "Accuracy Score of the Validation Set=  1.0\n",
        "Logloss Score Training Set=  0.00019188346988216327\n",
        "Logloss Score of the Validation Set=  0.00019054292580559802"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOud527aybHN",
        "colab_type": "code",
        "colab": {},
        "outputId": "f2dfbbba-249d-4ca5-dcff-eb458dca2d99"
      },
      "source": [
        "params = {\n",
        "    'eta': 0.1,  \n",
        "    'n_estimators': 1000,\n",
        "    'silent': True,  # option for logging\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass tasks\n",
        "    'num_class': 9,  # number of classes to predic\n",
        "    'max_depth': 5,  # depth of the trees in the boosting process\n",
        "    'seed': 42,\n",
        "    'eval_metric': 'mlogloss', #cross-entropy is chosen as evaluation metric\n",
        "    'min_child_weight':4,\n",
        "    'gamma':0.0,\n",
        "    'subsample':0.9,\n",
        "    'colsample_bytree':0.8,\n",
        "    'nthread': 16,\n",
        "    #'scale_pos_weight':1,\n",
        "    'reg_alpha':0,\n",
        "    'max_delta_step':1\n",
        "    \n",
        "    }\n",
        "num_round = 50\n",
        "\n",
        "#training the model\n",
        "#model = xgb.train(params, dtrainR, num_round) #change dtrainR w/ dtrain\n",
        "\n",
        "num_boost_round = 999\n",
        "model = xgb.train(\n",
        "    params,\n",
        "    dtrain,#change dtrainR w/ dtrain\n",
        "    num_boost_round=num_boost_round,\n",
        "    evals=[(dtrain, \"Train\"),(dval, \"Test\")],\n",
        "    early_stopping_rounds=20\n",
        ")\n",
        "\n",
        "probsTrain = model.predict(dtrain)\n",
        "probsVal = model.predict(dval)\n",
        "print(\"Training Class Probabilities for First 5 Instances:\\n\",probsTrain[:5])\n",
        "print(\"Validation Class Probabilities for First 5 Instances:\\n\",probsVal[:5])\n",
        "best_predsTrain = np.asarray([np.argmax(line) for line in probsTrain])\n",
        "best_predsVal = np.asarray([np.argmax(line) for line in probsVal])\n",
        "\n",
        "print(\"Best Predictions for Train:\\n\", best_predsTrain+1)\n",
        "print(\"Best Predictions for Validation:\\n\", best_predsVal+1)\n",
        "#print(min(best_preds+1),max(best_preds+1))\n",
        "\n",
        "print(\"Precision Score of the Training Set= \",precision_score(y_train, best_predsTrain, average='macro'))#change y_train(y_val) to y_trainR(y_valR)\n",
        "print(\"Precision Score of the Validation Set= \",precision_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Recall Score of the Training Set= \",recall_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"Recall Score of the Validation Set= \",recall_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"F1 Score of the Training Set= \",f1_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"F1 Score of the Validation Set= \",f1_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Accuracy Score the Training Set= \", accuracy_score(y_train, best_predsTrain))\n",
        "print(\"Accuracy Score of the Validation Set= \", accuracy_score(y_val, best_predsVal))\n",
        "scoreTrain = log_loss(y_train, probsTrain)\n",
        "scoreVal = log_loss(y_val, probsVal)\n",
        "print(\"Logloss Score Training Set= \", scoreTrain)\n",
        "print(\"Logloss Score of the Validation Set= \", scoreVal)\n",
        "\n",
        "print(\"Confusion Matrix of the Training Set: \\n\")\n",
        "print(confusion_matrix(y_train, best_predsTrain))\n",
        "print(\"Confusion Matrix of the Validation Set: \\n\")\n",
        "print(confusion_matrix(y_val,best_predsVal))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\tTrain-mlogloss:2.07141\tTest-mlogloss:2.07163\n",
            "Multiple eval metrics have been passed: 'Test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until Test-mlogloss hasn't improved in 20 rounds.\n",
            "[1]\tTrain-mlogloss:1.94018\tTest-mlogloss:1.94039\n",
            "[2]\tTrain-mlogloss:1.81343\tTest-mlogloss:1.81374\n",
            "[3]\tTrain-mlogloss:1.69803\tTest-mlogloss:1.6986\n",
            "[4]\tTrain-mlogloss:1.5847\tTest-mlogloss:1.58521\n",
            "[5]\tTrain-mlogloss:1.47379\tTest-mlogloss:1.47413\n",
            "[6]\tTrain-mlogloss:1.36298\tTest-mlogloss:1.36343\n",
            "[7]\tTrain-mlogloss:1.25928\tTest-mlogloss:1.25965\n",
            "[8]\tTrain-mlogloss:1.15566\tTest-mlogloss:1.15602\n",
            "[9]\tTrain-mlogloss:1.06799\tTest-mlogloss:1.0684\n",
            "[10]\tTrain-mlogloss:0.96961\tTest-mlogloss:0.969993\n",
            "[11]\tTrain-mlogloss:0.87698\tTest-mlogloss:0.87734\n",
            "[12]\tTrain-mlogloss:0.80535\tTest-mlogloss:0.805767\n",
            "[13]\tTrain-mlogloss:0.737218\tTest-mlogloss:0.737682\n",
            "[14]\tTrain-mlogloss:0.665797\tTest-mlogloss:0.666266\n",
            "[15]\tTrain-mlogloss:0.596326\tTest-mlogloss:0.596772\n",
            "[16]\tTrain-mlogloss:0.536362\tTest-mlogloss:0.536844\n",
            "[17]\tTrain-mlogloss:0.488792\tTest-mlogloss:0.489253\n",
            "[18]\tTrain-mlogloss:0.439247\tTest-mlogloss:0.439666\n",
            "[19]\tTrain-mlogloss:0.40054\tTest-mlogloss:0.400925\n",
            "[20]\tTrain-mlogloss:0.36551\tTest-mlogloss:0.365926\n",
            "[21]\tTrain-mlogloss:0.334041\tTest-mlogloss:0.334496\n",
            "[22]\tTrain-mlogloss:0.304755\tTest-mlogloss:0.305165\n",
            "[23]\tTrain-mlogloss:0.27667\tTest-mlogloss:0.277077\n",
            "[24]\tTrain-mlogloss:0.255039\tTest-mlogloss:0.255471\n",
            "[25]\tTrain-mlogloss:0.231951\tTest-mlogloss:0.232352\n",
            "[26]\tTrain-mlogloss:0.214245\tTest-mlogloss:0.21465\n",
            "[27]\tTrain-mlogloss:0.195924\tTest-mlogloss:0.196305\n",
            "[28]\tTrain-mlogloss:0.178247\tTest-mlogloss:0.1786\n",
            "[29]\tTrain-mlogloss:0.16486\tTest-mlogloss:0.165187\n",
            "[30]\tTrain-mlogloss:0.150902\tTest-mlogloss:0.151203\n",
            "[31]\tTrain-mlogloss:0.138434\tTest-mlogloss:0.138712\n",
            "[32]\tTrain-mlogloss:0.12601\tTest-mlogloss:0.126266\n",
            "[33]\tTrain-mlogloss:0.114722\tTest-mlogloss:0.114967\n",
            "[34]\tTrain-mlogloss:0.103683\tTest-mlogloss:0.103906\n",
            "[35]\tTrain-mlogloss:0.094139\tTest-mlogloss:0.094353\n",
            "[36]\tTrain-mlogloss:0.085811\tTest-mlogloss:0.086008\n",
            "[37]\tTrain-mlogloss:0.078244\tTest-mlogloss:0.078433\n",
            "[38]\tTrain-mlogloss:0.071807\tTest-mlogloss:0.071996\n",
            "[39]\tTrain-mlogloss:0.065866\tTest-mlogloss:0.066045\n",
            "[40]\tTrain-mlogloss:0.059984\tTest-mlogloss:0.060152\n",
            "[41]\tTrain-mlogloss:0.055045\tTest-mlogloss:0.055207\n",
            "[42]\tTrain-mlogloss:0.050713\tTest-mlogloss:0.05088\n",
            "[43]\tTrain-mlogloss:0.045869\tTest-mlogloss:0.046021\n",
            "[44]\tTrain-mlogloss:0.042027\tTest-mlogloss:0.042177\n",
            "[45]\tTrain-mlogloss:0.039301\tTest-mlogloss:0.039444\n",
            "[46]\tTrain-mlogloss:0.035552\tTest-mlogloss:0.035682\n",
            "[47]\tTrain-mlogloss:0.032669\tTest-mlogloss:0.032791\n",
            "[48]\tTrain-mlogloss:0.029999\tTest-mlogloss:0.030114\n",
            "[49]\tTrain-mlogloss:0.027414\tTest-mlogloss:0.02752\n",
            "[50]\tTrain-mlogloss:0.025203\tTest-mlogloss:0.025304\n",
            "[51]\tTrain-mlogloss:0.022805\tTest-mlogloss:0.022896\n",
            "[52]\tTrain-mlogloss:0.020638\tTest-mlogloss:0.02072\n",
            "[53]\tTrain-mlogloss:0.019219\tTest-mlogloss:0.019301\n",
            "[54]\tTrain-mlogloss:0.017395\tTest-mlogloss:0.017469\n",
            "[55]\tTrain-mlogloss:0.015746\tTest-mlogloss:0.015812\n",
            "[56]\tTrain-mlogloss:0.01443\tTest-mlogloss:0.014492\n",
            "[57]\tTrain-mlogloss:0.013192\tTest-mlogloss:0.013251\n",
            "[58]\tTrain-mlogloss:0.012115\tTest-mlogloss:0.012167\n",
            "[59]\tTrain-mlogloss:0.011199\tTest-mlogloss:0.011247\n",
            "[60]\tTrain-mlogloss:0.010241\tTest-mlogloss:0.010285\n",
            "[61]\tTrain-mlogloss:0.009281\tTest-mlogloss:0.009321\n",
            "[62]\tTrain-mlogloss:0.008405\tTest-mlogloss:0.008441\n",
            "[63]\tTrain-mlogloss:0.007676\tTest-mlogloss:0.007709\n",
            "[64]\tTrain-mlogloss:0.007043\tTest-mlogloss:0.007074\n",
            "[65]\tTrain-mlogloss:0.006426\tTest-mlogloss:0.006455\n",
            "[66]\tTrain-mlogloss:0.00582\tTest-mlogloss:0.005846\n",
            "[67]\tTrain-mlogloss:0.005304\tTest-mlogloss:0.005328\n",
            "[68]\tTrain-mlogloss:0.004842\tTest-mlogloss:0.004863\n",
            "[69]\tTrain-mlogloss:0.004392\tTest-mlogloss:0.004411\n",
            "[70]\tTrain-mlogloss:0.004058\tTest-mlogloss:0.004076\n",
            "[71]\tTrain-mlogloss:0.003747\tTest-mlogloss:0.003763\n",
            "[72]\tTrain-mlogloss:0.0034\tTest-mlogloss:0.003415\n",
            "[73]\tTrain-mlogloss:0.003177\tTest-mlogloss:0.003191\n",
            "[74]\tTrain-mlogloss:0.002903\tTest-mlogloss:0.002915\n",
            "[75]\tTrain-mlogloss:0.002635\tTest-mlogloss:0.002645\n",
            "[76]\tTrain-mlogloss:0.002436\tTest-mlogloss:0.002446\n",
            "[77]\tTrain-mlogloss:0.002225\tTest-mlogloss:0.002234\n",
            "[78]\tTrain-mlogloss:0.002074\tTest-mlogloss:0.002082\n",
            "[79]\tTrain-mlogloss:0.001902\tTest-mlogloss:0.001909\n",
            "[80]\tTrain-mlogloss:0.001763\tTest-mlogloss:0.00177\n",
            "[81]\tTrain-mlogloss:0.001622\tTest-mlogloss:0.001629\n",
            "[82]\tTrain-mlogloss:0.00149\tTest-mlogloss:0.001496\n",
            "[83]\tTrain-mlogloss:0.001395\tTest-mlogloss:0.001399\n",
            "[84]\tTrain-mlogloss:0.001293\tTest-mlogloss:0.001298\n",
            "[85]\tTrain-mlogloss:0.001182\tTest-mlogloss:0.001187\n",
            "[86]\tTrain-mlogloss:0.001098\tTest-mlogloss:0.001102\n",
            "[87]\tTrain-mlogloss:0.001018\tTest-mlogloss:0.001021\n",
            "[88]\tTrain-mlogloss:0.00093\tTest-mlogloss:0.000933\n",
            "[89]\tTrain-mlogloss:0.000874\tTest-mlogloss:0.000876\n",
            "[90]\tTrain-mlogloss:0.000821\tTest-mlogloss:0.000824\n",
            "[91]\tTrain-mlogloss:0.000759\tTest-mlogloss:0.000761\n",
            "[92]\tTrain-mlogloss:0.000705\tTest-mlogloss:0.000707\n",
            "[93]\tTrain-mlogloss:0.000655\tTest-mlogloss:0.000657\n",
            "[94]\tTrain-mlogloss:0.000618\tTest-mlogloss:0.000619\n",
            "[95]\tTrain-mlogloss:0.000569\tTest-mlogloss:0.00057\n",
            "[96]\tTrain-mlogloss:0.000534\tTest-mlogloss:0.000534\n",
            "[97]\tTrain-mlogloss:0.000498\tTest-mlogloss:0.000498\n",
            "[98]\tTrain-mlogloss:0.000466\tTest-mlogloss:0.000467\n",
            "[99]\tTrain-mlogloss:0.000437\tTest-mlogloss:0.000437\n",
            "[100]\tTrain-mlogloss:0.000416\tTest-mlogloss:0.000416\n",
            "[101]\tTrain-mlogloss:0.000391\tTest-mlogloss:0.000391\n",
            "[102]\tTrain-mlogloss:0.000369\tTest-mlogloss:0.000369\n",
            "[103]\tTrain-mlogloss:0.000351\tTest-mlogloss:0.000351\n",
            "[104]\tTrain-mlogloss:0.000335\tTest-mlogloss:0.000334\n",
            "[105]\tTrain-mlogloss:0.00032\tTest-mlogloss:0.000319\n",
            "[106]\tTrain-mlogloss:0.000308\tTest-mlogloss:0.000308\n",
            "[107]\tTrain-mlogloss:0.000296\tTest-mlogloss:0.000295\n",
            "[108]\tTrain-mlogloss:0.000285\tTest-mlogloss:0.000284\n",
            "[109]\tTrain-mlogloss:0.000275\tTest-mlogloss:0.000274\n",
            "[110]\tTrain-mlogloss:0.000268\tTest-mlogloss:0.000267\n",
            "[111]\tTrain-mlogloss:0.000263\tTest-mlogloss:0.000263\n",
            "[112]\tTrain-mlogloss:0.000258\tTest-mlogloss:0.000257\n",
            "[113]\tTrain-mlogloss:0.000251\tTest-mlogloss:0.00025\n",
            "[114]\tTrain-mlogloss:0.000244\tTest-mlogloss:0.000243\n",
            "[115]\tTrain-mlogloss:0.000238\tTest-mlogloss:0.000238\n",
            "[116]\tTrain-mlogloss:0.000235\tTest-mlogloss:0.000234\n",
            "[117]\tTrain-mlogloss:0.00023\tTest-mlogloss:0.00023\n",
            "[118]\tTrain-mlogloss:0.000227\tTest-mlogloss:0.000227\n",
            "[119]\tTrain-mlogloss:0.000223\tTest-mlogloss:0.000222\n",
            "[120]\tTrain-mlogloss:0.000219\tTest-mlogloss:0.000218\n",
            "[121]\tTrain-mlogloss:0.000218\tTest-mlogloss:0.000217\n",
            "[122]\tTrain-mlogloss:0.000217\tTest-mlogloss:0.000216\n",
            "[123]\tTrain-mlogloss:0.000216\tTest-mlogloss:0.000215\n",
            "[124]\tTrain-mlogloss:0.000215\tTest-mlogloss:0.000214\n",
            "[125]\tTrain-mlogloss:0.000215\tTest-mlogloss:0.000214\n",
            "[126]\tTrain-mlogloss:0.000214\tTest-mlogloss:0.000213\n",
            "[127]\tTrain-mlogloss:0.000213\tTest-mlogloss:0.000212\n",
            "[128]\tTrain-mlogloss:0.000212\tTest-mlogloss:0.000211\n",
            "[129]\tTrain-mlogloss:0.000212\tTest-mlogloss:0.000211\n",
            "[130]\tTrain-mlogloss:0.000211\tTest-mlogloss:0.00021\n",
            "[131]\tTrain-mlogloss:0.000211\tTest-mlogloss:0.00021\n",
            "[132]\tTrain-mlogloss:0.00021\tTest-mlogloss:0.000209\n",
            "[133]\tTrain-mlogloss:0.00021\tTest-mlogloss:0.000209\n",
            "[134]\tTrain-mlogloss:0.000209\tTest-mlogloss:0.000208\n",
            "[135]\tTrain-mlogloss:0.000209\tTest-mlogloss:0.000208\n",
            "[136]\tTrain-mlogloss:0.000208\tTest-mlogloss:0.000207\n",
            "[137]\tTrain-mlogloss:0.000208\tTest-mlogloss:0.000207\n",
            "[138]\tTrain-mlogloss:0.000208\tTest-mlogloss:0.000207\n",
            "[139]\tTrain-mlogloss:0.000207\tTest-mlogloss:0.000206\n",
            "[140]\tTrain-mlogloss:0.000207\tTest-mlogloss:0.000206\n",
            "[141]\tTrain-mlogloss:0.000207\tTest-mlogloss:0.000205\n",
            "[142]\tTrain-mlogloss:0.000206\tTest-mlogloss:0.000205\n",
            "[143]\tTrain-mlogloss:0.000206\tTest-mlogloss:0.000205\n",
            "[144]\tTrain-mlogloss:0.000206\tTest-mlogloss:0.000204\n",
            "[145]\tTrain-mlogloss:0.000205\tTest-mlogloss:0.000204\n",
            "[146]\tTrain-mlogloss:0.000205\tTest-mlogloss:0.000204\n",
            "[147]\tTrain-mlogloss:0.000205\tTest-mlogloss:0.000204\n",
            "[148]\tTrain-mlogloss:0.000204\tTest-mlogloss:0.000203\n",
            "[149]\tTrain-mlogloss:0.000204\tTest-mlogloss:0.000203\n",
            "[150]\tTrain-mlogloss:0.000204\tTest-mlogloss:0.000203\n",
            "[151]\tTrain-mlogloss:0.000204\tTest-mlogloss:0.000202\n",
            "[152]\tTrain-mlogloss:0.000203\tTest-mlogloss:0.000202\n",
            "[153]\tTrain-mlogloss:0.000203\tTest-mlogloss:0.000202\n",
            "[154]\tTrain-mlogloss:0.000203\tTest-mlogloss:0.000202\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[155]\tTrain-mlogloss:0.000203\tTest-mlogloss:0.000201\n",
            "[156]\tTrain-mlogloss:0.000202\tTest-mlogloss:0.000201\n",
            "[157]\tTrain-mlogloss:0.000202\tTest-mlogloss:0.000201\n",
            "[158]\tTrain-mlogloss:0.000202\tTest-mlogloss:0.000201\n",
            "[159]\tTrain-mlogloss:0.000202\tTest-mlogloss:0.000201\n",
            "[160]\tTrain-mlogloss:0.000202\tTest-mlogloss:0.0002\n",
            "[161]\tTrain-mlogloss:0.000201\tTest-mlogloss:0.0002\n",
            "[162]\tTrain-mlogloss:0.000201\tTest-mlogloss:0.0002\n",
            "[163]\tTrain-mlogloss:0.000201\tTest-mlogloss:0.0002\n",
            "[164]\tTrain-mlogloss:0.000201\tTest-mlogloss:0.0002\n",
            "[165]\tTrain-mlogloss:0.0002\tTest-mlogloss:0.000199\n",
            "[166]\tTrain-mlogloss:0.0002\tTest-mlogloss:0.000199\n",
            "[167]\tTrain-mlogloss:0.0002\tTest-mlogloss:0.000199\n",
            "[168]\tTrain-mlogloss:0.0002\tTest-mlogloss:0.000199\n",
            "[169]\tTrain-mlogloss:0.0002\tTest-mlogloss:0.000199\n",
            "[170]\tTrain-mlogloss:0.0002\tTest-mlogloss:0.000198\n",
            "[171]\tTrain-mlogloss:0.000199\tTest-mlogloss:0.000198\n",
            "[172]\tTrain-mlogloss:0.000199\tTest-mlogloss:0.000198\n",
            "[173]\tTrain-mlogloss:0.000199\tTest-mlogloss:0.000198\n",
            "[174]\tTrain-mlogloss:0.000199\tTest-mlogloss:0.000198\n",
            "[175]\tTrain-mlogloss:0.000199\tTest-mlogloss:0.000198\n",
            "[176]\tTrain-mlogloss:0.000199\tTest-mlogloss:0.000197\n",
            "[177]\tTrain-mlogloss:0.000199\tTest-mlogloss:0.000197\n",
            "[178]\tTrain-mlogloss:0.000198\tTest-mlogloss:0.000197\n",
            "[179]\tTrain-mlogloss:0.000198\tTest-mlogloss:0.000197\n",
            "[180]\tTrain-mlogloss:0.000198\tTest-mlogloss:0.000197\n",
            "[181]\tTrain-mlogloss:0.000198\tTest-mlogloss:0.000197\n",
            "[182]\tTrain-mlogloss:0.000198\tTest-mlogloss:0.000197\n",
            "[183]\tTrain-mlogloss:0.000198\tTest-mlogloss:0.000196\n",
            "[184]\tTrain-mlogloss:0.000198\tTest-mlogloss:0.000196\n",
            "[185]\tTrain-mlogloss:0.000197\tTest-mlogloss:0.000196\n",
            "[186]\tTrain-mlogloss:0.000197\tTest-mlogloss:0.000196\n",
            "[187]\tTrain-mlogloss:0.000197\tTest-mlogloss:0.000196\n",
            "[188]\tTrain-mlogloss:0.000197\tTest-mlogloss:0.000196\n",
            "[189]\tTrain-mlogloss:0.000197\tTest-mlogloss:0.000196\n",
            "[190]\tTrain-mlogloss:0.000197\tTest-mlogloss:0.000196\n",
            "[191]\tTrain-mlogloss:0.000197\tTest-mlogloss:0.000195\n",
            "[192]\tTrain-mlogloss:0.000197\tTest-mlogloss:0.000195\n",
            "[193]\tTrain-mlogloss:0.000197\tTest-mlogloss:0.000195\n",
            "[194]\tTrain-mlogloss:0.000196\tTest-mlogloss:0.000195\n",
            "[195]\tTrain-mlogloss:0.000196\tTest-mlogloss:0.000195\n",
            "[196]\tTrain-mlogloss:0.000196\tTest-mlogloss:0.000195\n",
            "[197]\tTrain-mlogloss:0.000196\tTest-mlogloss:0.000195\n",
            "[198]\tTrain-mlogloss:0.000196\tTest-mlogloss:0.000195\n",
            "[199]\tTrain-mlogloss:0.000196\tTest-mlogloss:0.000195\n",
            "[200]\tTrain-mlogloss:0.000196\tTest-mlogloss:0.000195\n",
            "[201]\tTrain-mlogloss:0.000196\tTest-mlogloss:0.000194\n",
            "[202]\tTrain-mlogloss:0.000196\tTest-mlogloss:0.000194\n",
            "[203]\tTrain-mlogloss:0.000196\tTest-mlogloss:0.000194\n",
            "[204]\tTrain-mlogloss:0.000195\tTest-mlogloss:0.000194\n",
            "[205]\tTrain-mlogloss:0.000195\tTest-mlogloss:0.000194\n",
            "[206]\tTrain-mlogloss:0.000195\tTest-mlogloss:0.000194\n",
            "[207]\tTrain-mlogloss:0.000195\tTest-mlogloss:0.000194\n",
            "[208]\tTrain-mlogloss:0.000195\tTest-mlogloss:0.000194\n",
            "[209]\tTrain-mlogloss:0.000195\tTest-mlogloss:0.000194\n",
            "[210]\tTrain-mlogloss:0.000195\tTest-mlogloss:0.000194\n",
            "[211]\tTrain-mlogloss:0.000195\tTest-mlogloss:0.000194\n",
            "[212]\tTrain-mlogloss:0.000195\tTest-mlogloss:0.000193\n",
            "[213]\tTrain-mlogloss:0.000195\tTest-mlogloss:0.000193\n",
            "[214]\tTrain-mlogloss:0.000195\tTest-mlogloss:0.000193\n",
            "[215]\tTrain-mlogloss:0.000195\tTest-mlogloss:0.000193\n",
            "[216]\tTrain-mlogloss:0.000195\tTest-mlogloss:0.000193\n",
            "[217]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000193\n",
            "[218]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000193\n",
            "[219]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000193\n",
            "[220]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000193\n",
            "[221]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000193\n",
            "[222]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000193\n",
            "[223]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000193\n",
            "[224]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000193\n",
            "[225]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000193\n",
            "[226]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000193\n",
            "[227]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000192\n",
            "[228]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000192\n",
            "[229]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000192\n",
            "[230]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000192\n",
            "[231]\tTrain-mlogloss:0.000194\tTest-mlogloss:0.000192\n",
            "[232]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000192\n",
            "[233]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000192\n",
            "[234]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000192\n",
            "[235]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000192\n",
            "[236]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000192\n",
            "[237]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000192\n",
            "[238]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000192\n",
            "[239]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000192\n",
            "[240]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000192\n",
            "[241]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000192\n",
            "[242]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000192\n",
            "[243]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000191\n",
            "[244]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000191\n",
            "[245]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000191\n",
            "[246]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000191\n",
            "[247]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000191\n",
            "[248]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000191\n",
            "[249]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000191\n",
            "[250]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "[251]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "[252]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "[253]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "[254]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "[255]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "[256]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "[257]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "[258]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "[259]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "[260]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "[261]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "[262]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "[263]\tTrain-mlogloss:0.000192\tTest-mlogloss:0.000191\n",
            "Stopping. Best iteration:\n",
            "[243]\tTrain-mlogloss:0.000193\tTest-mlogloss:0.000191\n",
            "\n",
            "Training Class Probabilities for First 5 Instances:\n",
            " [[4.9995178e-06 2.9947528e-06 3.6532247e-06 3.1103577e-06 2.5672507e-06\n",
            "  9.9993706e-01 6.7674055e-06 3.4329132e-05 4.5740539e-06]\n",
            " [6.3459242e-06 9.9981242e-01 6.9320056e-05 1.6131396e-05 4.0953259e-06\n",
            "  2.6894497e-05 7.6757124e-06 2.0944457e-05 3.6175250e-05]\n",
            " [6.4811202e-06 2.2842300e-05 9.9988639e-01 4.7180954e-05 5.3913664e-06\n",
            "  4.7637759e-06 1.1243669e-05 9.6123595e-06 6.1124997e-06]\n",
            " [1.3236905e-05 3.5296864e-05 9.9985719e-01 3.6348123e-05 4.5428460e-06\n",
            "  5.3565136e-06 1.3235164e-05 6.3275224e-06 2.8495000e-05]\n",
            " [8.5049860e-06 5.0526342e-05 4.8527265e-05 9.9983799e-01 1.0754519e-05\n",
            "  5.5631449e-06 1.9263402e-05 8.0372674e-06 1.0747599e-05]]\n",
            "Validation Class Probabilities for First 5 Instances:\n",
            " [[9.39850906e-06 4.11449473e-05 9.99852419e-01 3.19621067e-05\n",
            "  7.17548755e-06 1.57673949e-05 1.22238089e-05 1.00108173e-05\n",
            "  1.99275401e-05]\n",
            " [1.76543763e-05 9.16470071e-06 6.60289334e-06 1.09317325e-05\n",
            "  9.99888182e-01 9.72787439e-06 1.40989005e-05 1.96084638e-05\n",
            "  2.39705041e-05]\n",
            " [1.30768813e-05 1.09050006e-05 1.00745274e-05 1.21195853e-05\n",
            "  6.72988390e-05 1.42640965e-05 9.99834538e-01 1.18795060e-05\n",
            "  2.57844458e-05]\n",
            " [1.60930340e-05 2.47529752e-05 9.99649763e-01 1.15065932e-05\n",
            "  2.01638803e-04 1.06474727e-05 2.54556562e-05 2.11935749e-05\n",
            "  3.89174311e-05]\n",
            " [2.86607756e-05 2.02004812e-05 2.49363711e-05 1.75683017e-05\n",
            "  1.32114628e-05 2.68093409e-05 2.00831564e-05 9.99823511e-01\n",
            "  2.50183839e-05]]\n",
            "Best Predictions for Train:\n",
            " [6 2 3 ... 5 3 7]\n",
            "Best Predictions for Validation:\n",
            " [3 5 7 ... 1 5 3]\n",
            "Precision Score of the Training Set=  1.0\n",
            "Precision Score of the Validation Set=  1.0\n",
            "Recall Score of the Training Set=  1.0\n",
            "Recall Score of the Validation Set=  1.0\n",
            "F1 Score of the Training Set=  1.0\n",
            "F1 Score of the Validation Set=  1.0\n",
            "Accuracy Score the Training Set=  1.0\n",
            "Accuracy Score of the Validation Set=  1.0\n",
            "Logloss Score Training Set=  0.00019188346988216327\n",
            "Logloss Score of the Validation Set=  0.00019054292580559802\n",
            "Confusion Matrix of the Training Set: \n",
            "\n",
            "[[12898     0     0     0     0     0     0     0     0]\n",
            " [    0 12898     0     0     0     0     0     0     0]\n",
            " [    0     0 12897     0     0     0     0     0     0]\n",
            " [    0     0     0 12897     0     0     0     0     0]\n",
            " [    0     0     0     0 12897     0     0     0     0]\n",
            " [    0     0     0     0     0 12898     0     0     0]\n",
            " [    0     0     0     0     0     0 12897     0     0]\n",
            " [    0     0     0     0     0     0     0 12898     0]\n",
            " [    0     0     0     0     0     0     0     0 12898]]\n",
            "Confusion Matrix of the Validation Set: \n",
            "\n",
            "[[3224    0    0    0    0    0    0    0    0]\n",
            " [   0 3224    0    0    0    0    0    0    0]\n",
            " [   0    0 3225    0    0    0    0    0    0]\n",
            " [   0    0    0 3225    0    0    0    0    0]\n",
            " [   0    0    0    0 3225    0    0    0    0]\n",
            " [   0    0    0    0    0 3224    0    0    0]\n",
            " [   0    0    0    0    0    0 3225    0    0]\n",
            " [   0    0    0    0    0    0    0 3224    0]\n",
            " [   0    0    0    0    0    0    0    0 3224]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY23rsOLybHR",
        "colab_type": "text"
      },
      "source": [
        "## Tune Gamma\n",
        "\n",
        "*gamma = 0 => mlogloss 0.48930560000000006 for 218 rounds*\n",
        "gamma =0.1 => mlogloss 0.49001900000000004 for 218 rounds\n",
        "gamma =0.2 => mlogloss 0.48938500000000007 for 230 rounds\n",
        "gamma = 0.3 => mlogloss 0.4895237999999999 for 248 rounds\n",
        "gamma =0.4 => mlogloss 0.4896344 for 227 rounds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mge5AbDybHR",
        "colab_type": "code",
        "colab": {},
        "outputId": "09caafcd-a580-4ffe-ae09-3cd18fc2cd9a"
      },
      "source": [
        "gridsearch_params = [\n",
        "    (gamma)\n",
        "    for gamma in [i/10. for i in range(0,5)]\n",
        "    \n",
        "]\n",
        "params = {\n",
        "    'eta': 0.3,  #increased for Gridsearch only\n",
        "    'n_estimators': 1000,\n",
        "    'silent': True,  # option for logging\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass tasks\n",
        "    'num_class': 9,  # number of classes to predic\n",
        "    'max_depth': 5,  # depth of the trees in the boosting process\n",
        "    'seed': 42,\n",
        "    'eval_metric': 'mlogloss', #cross-entropy is chosen as evaluation metric\n",
        "    'min_child_weight':4,\n",
        "    'gamma':0.0,\n",
        "    'subsample':0.9,\n",
        "    'colsample_bytree':0.8,\n",
        "    'nthread': 16,\n",
        "    #'scale_pos_weight':1,\n",
        "    'reg_alpha':0,\n",
        "    'max_delta_step':1\n",
        "    }\n",
        "# Define initial best params and mlogloss\n",
        "min_mlogloss = float(\"Inf\")\n",
        "best_params = None\n",
        "for gamma in gridsearch_params:\n",
        "    print(\"CV with gamma={}\".format(\n",
        "                             gamma))\n",
        "\n",
        "    # Update our parameters\n",
        "    params['gamma'] = gamma\n",
        "    \n",
        "\n",
        "    # Run CV\n",
        "    cv_results = xgb.cv(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=num_boost_round,\n",
        "        seed=42,\n",
        "        nfold=5,\n",
        "        metrics={'mlogloss'},\n",
        "        early_stopping_rounds=20,\n",
        "        #fpreproc = fpreproc, #use custom fn to update weights\n",
        "        stratified =True,\n",
        "        verbose_eval =10\n",
        "    )\n",
        "\n",
        "    # Update best mlogloss\n",
        "    mean_mlogloss = cv_results['test-mlogloss-mean'].min()\n",
        "    boost_rounds = cv_results['test-mlogloss-mean'].argmin()\n",
        "    print(\"\\tmlogloss {} for {} rounds\".format(mean_mlogloss, boost_rounds))\n",
        "    if mean_mlogloss < min_mlogloss:\n",
        "        min_mlogloss = mean_mlogloss\n",
        "        best_params = (gamma)\n",
        "\n",
        "print(\"Best params: {}, {}, mlogloss: {}\".format(best_params, min_mlogloss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CV with gamma=0.0\n",
            "[0]\ttrain-mlogloss:1.92221+0.00289063\ttest-mlogloss:1.92617+0.00326339\n",
            "[10]\ttrain-mlogloss:0.749833+0.00407823\ttest-mlogloss:0.78386+0.00511257\n",
            "[20]\ttrain-mlogloss:0.576543+0.00118897\ttest-mlogloss:0.63127+0.00601702\n",
            "[30]\ttrain-mlogloss:0.510457+0.00195453\ttest-mlogloss:0.582491+0.00573138\n",
            "[40]\ttrain-mlogloss:0.470605+0.00205347\ttest-mlogloss:0.557552+0.00515311\n",
            "[50]\ttrain-mlogloss:0.441826+0.00242437\ttest-mlogloss:0.542+0.00520644\n",
            "[60]\ttrain-mlogloss:0.41918+0.00246886\ttest-mlogloss:0.531212+0.00511459\n",
            "[70]\ttrain-mlogloss:0.399618+0.00205475\ttest-mlogloss:0.522983+0.00518999\n",
            "[80]\ttrain-mlogloss:0.380899+0.00201626\ttest-mlogloss:0.516103+0.00512336\n",
            "[90]\ttrain-mlogloss:0.365123+0.00194188\ttest-mlogloss:0.510828+0.0051596\n",
            "[100]\ttrain-mlogloss:0.351437+0.0018664\ttest-mlogloss:0.506808+0.00508529\n",
            "[110]\ttrain-mlogloss:0.338759+0.00160385\ttest-mlogloss:0.503212+0.0050477\n",
            "[120]\ttrain-mlogloss:0.326387+0.00172242\ttest-mlogloss:0.500771+0.00533158\n",
            "[130]\ttrain-mlogloss:0.314932+0.00184963\ttest-mlogloss:0.498411+0.0055818\n",
            "[140]\ttrain-mlogloss:0.304434+0.00169184\ttest-mlogloss:0.496322+0.00544035\n",
            "[150]\ttrain-mlogloss:0.294005+0.00161856\ttest-mlogloss:0.494283+0.00571446\n",
            "[160]\ttrain-mlogloss:0.284018+0.00175922\ttest-mlogloss:0.493047+0.00560632\n",
            "[170]\ttrain-mlogloss:0.274912+0.00162358\ttest-mlogloss:0.491636+0.00568678\n",
            "[180]\ttrain-mlogloss:0.266316+0.00158998\ttest-mlogloss:0.490927+0.00582458\n",
            "[190]\ttrain-mlogloss:0.258479+0.00184451\ttest-mlogloss:0.490465+0.00583736\n",
            "[200]\ttrain-mlogloss:0.250534+0.0019243\ttest-mlogloss:0.490067+0.00600924\n",
            "[210]\ttrain-mlogloss:0.242924+0.00177343\ttest-mlogloss:0.489645+0.00606514\n",
            "[220]\ttrain-mlogloss:0.236264+0.0020724\ttest-mlogloss:0.489511+0.00607459\n",
            "[230]\ttrain-mlogloss:0.229646+0.0020684\ttest-mlogloss:0.489625+0.00663018\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:51: FutureWarning: 'argmin' is deprecated. Use 'idxmin' instead. The behavior of 'argmin' will be corrected to return the positional minimum in the future. Use 'series.values.argmin' to get the position of the minimum now.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\tmlogloss 0.48930560000000006 for 218 rounds\n",
            "CV with gamma=0.1\n",
            "[0]\ttrain-mlogloss:1.92221+0.00289153\ttest-mlogloss:1.92617+0.00325975\n",
            "[10]\ttrain-mlogloss:0.749703+0.00376208\ttest-mlogloss:0.783597+0.005104\n",
            "[20]\ttrain-mlogloss:0.57625+0.000979299\ttest-mlogloss:0.631246+0.00582987\n",
            "[30]\ttrain-mlogloss:0.510159+0.0017701\ttest-mlogloss:0.582354+0.00592956\n",
            "[40]\ttrain-mlogloss:0.470486+0.00195305\ttest-mlogloss:0.55832+0.00519835\n",
            "[50]\ttrain-mlogloss:0.441835+0.00178875\ttest-mlogloss:0.54274+0.005046\n",
            "[60]\ttrain-mlogloss:0.418906+0.0020217\ttest-mlogloss:0.531734+0.00447016\n",
            "[70]\ttrain-mlogloss:0.398796+0.0018658\ttest-mlogloss:0.523546+0.00447775\n",
            "[80]\ttrain-mlogloss:0.381647+0.00234943\ttest-mlogloss:0.517175+0.00369438\n",
            "[90]\ttrain-mlogloss:0.365972+0.00253088\ttest-mlogloss:0.511898+0.00372455\n",
            "[100]\ttrain-mlogloss:0.352294+0.00241402\ttest-mlogloss:0.507752+0.00388886\n",
            "[110]\ttrain-mlogloss:0.339786+0.00252652\ttest-mlogloss:0.504272+0.00370099\n",
            "[120]\ttrain-mlogloss:0.327481+0.00257573\ttest-mlogloss:0.501146+0.00344963\n",
            "[130]\ttrain-mlogloss:0.315765+0.00275747\ttest-mlogloss:0.499121+0.00359957\n",
            "[140]\ttrain-mlogloss:0.304972+0.0025752\ttest-mlogloss:0.496726+0.00384125\n",
            "[150]\ttrain-mlogloss:0.295033+0.00246126\ttest-mlogloss:0.495142+0.00394185\n",
            "[160]\ttrain-mlogloss:0.284927+0.00274704\ttest-mlogloss:0.493812+0.00430548\n",
            "[170]\ttrain-mlogloss:0.275764+0.00240331\ttest-mlogloss:0.492715+0.00457621\n",
            "[180]\ttrain-mlogloss:0.267041+0.00259793\ttest-mlogloss:0.491448+0.00436503\n",
            "[190]\ttrain-mlogloss:0.259051+0.00268546\ttest-mlogloss:0.491188+0.00443182\n",
            "[200]\ttrain-mlogloss:0.251125+0.00290983\ttest-mlogloss:0.490846+0.0047812\n",
            "[210]\ttrain-mlogloss:0.24364+0.00277451\ttest-mlogloss:0.490369+0.00517014\n",
            "[220]\ttrain-mlogloss:0.23673+0.00287189\ttest-mlogloss:0.490119+0.00506077\n",
            "[230]\ttrain-mlogloss:0.230317+0.00257994\ttest-mlogloss:0.490481+0.00544717\n",
            "\tmlogloss 0.49001900000000004 for 218 rounds\n",
            "CV with gamma=0.2\n",
            "[0]\ttrain-mlogloss:1.92221+0.00288695\ttest-mlogloss:1.92616+0.00325621\n",
            "[10]\ttrain-mlogloss:0.749725+0.00374738\ttest-mlogloss:0.783661+0.00515442\n",
            "[20]\ttrain-mlogloss:0.576574+0.00173852\ttest-mlogloss:0.631784+0.00570289\n",
            "[30]\ttrain-mlogloss:0.510885+0.00245844\ttest-mlogloss:0.58274+0.00564681\n",
            "[40]\ttrain-mlogloss:0.470174+0.0014741\ttest-mlogloss:0.55759+0.00580965\n",
            "[50]\ttrain-mlogloss:0.441704+0.00200474\ttest-mlogloss:0.542813+0.00539762\n",
            "[60]\ttrain-mlogloss:0.418907+0.0018573\ttest-mlogloss:0.532221+0.00555573\n",
            "[70]\ttrain-mlogloss:0.399187+0.00184529\ttest-mlogloss:0.523374+0.00528071\n",
            "[80]\ttrain-mlogloss:0.381026+0.00191282\ttest-mlogloss:0.516511+0.00495118\n",
            "[90]\ttrain-mlogloss:0.365549+0.00173925\ttest-mlogloss:0.511597+0.00490852\n",
            "[100]\ttrain-mlogloss:0.351991+0.00147919\ttest-mlogloss:0.507825+0.00490648\n",
            "[110]\ttrain-mlogloss:0.339042+0.00114237\ttest-mlogloss:0.503817+0.00461309\n",
            "[120]\ttrain-mlogloss:0.326606+0.000959324\ttest-mlogloss:0.50107+0.00503065\n",
            "[130]\ttrain-mlogloss:0.315336+0.000949944\ttest-mlogloss:0.498947+0.00517415\n",
            "[140]\ttrain-mlogloss:0.30458+0.00112697\ttest-mlogloss:0.496855+0.00548104\n",
            "[150]\ttrain-mlogloss:0.29486+0.00135539\ttest-mlogloss:0.495222+0.00559003\n",
            "[160]\ttrain-mlogloss:0.284875+0.00171334\ttest-mlogloss:0.493866+0.00545737\n",
            "[170]\ttrain-mlogloss:0.275811+0.00178014\ttest-mlogloss:0.492637+0.00565469\n",
            "[180]\ttrain-mlogloss:0.267055+0.00205934\ttest-mlogloss:0.491664+0.00579098\n",
            "[190]\ttrain-mlogloss:0.259006+0.00239023\ttest-mlogloss:0.491289+0.00560031\n",
            "[200]\ttrain-mlogloss:0.251092+0.00230073\ttest-mlogloss:0.490694+0.00548755\n",
            "[210]\ttrain-mlogloss:0.243577+0.00227768\ttest-mlogloss:0.489984+0.00589421\n",
            "[220]\ttrain-mlogloss:0.236654+0.00253211\ttest-mlogloss:0.489694+0.00589822\n",
            "[230]\ttrain-mlogloss:0.229861+0.00209174\ttest-mlogloss:0.489385+0.00594636\n",
            "[240]\ttrain-mlogloss:0.223387+0.0024059\ttest-mlogloss:0.489909+0.00602492\n",
            "\tmlogloss 0.48938500000000007 for 230 rounds\n",
            "CV with gamma=0.3\n",
            "[0]\ttrain-mlogloss:1.92221+0.00288634\ttest-mlogloss:1.92616+0.0032591\n",
            "[10]\ttrain-mlogloss:0.749761+0.00370142\ttest-mlogloss:0.783537+0.00520984\n",
            "[20]\ttrain-mlogloss:0.576178+0.0021556\ttest-mlogloss:0.63098+0.00486461\n",
            "[30]\ttrain-mlogloss:0.510428+0.00295881\ttest-mlogloss:0.582134+0.00480209\n",
            "[40]\ttrain-mlogloss:0.470914+0.00199957\ttest-mlogloss:0.557614+0.00465205\n",
            "[50]\ttrain-mlogloss:0.441926+0.00238877\ttest-mlogloss:0.542333+0.00479296\n",
            "[60]\ttrain-mlogloss:0.419417+0.00216459\ttest-mlogloss:0.531238+0.00519748\n",
            "[70]\ttrain-mlogloss:0.400164+0.00188435\ttest-mlogloss:0.522683+0.00476291\n",
            "[80]\ttrain-mlogloss:0.38204+0.00206478\ttest-mlogloss:0.516167+0.00420736\n",
            "[90]\ttrain-mlogloss:0.366231+0.00217015\ttest-mlogloss:0.510928+0.00427875\n",
            "[100]\ttrain-mlogloss:0.352117+0.00192277\ttest-mlogloss:0.506942+0.00420631\n",
            "[110]\ttrain-mlogloss:0.339328+0.00205557\ttest-mlogloss:0.503577+0.00391759\n",
            "[120]\ttrain-mlogloss:0.327062+0.00210091\ttest-mlogloss:0.500772+0.00416796\n",
            "[130]\ttrain-mlogloss:0.315473+0.00225212\ttest-mlogloss:0.498462+0.00418417\n",
            "[140]\ttrain-mlogloss:0.304851+0.00261308\ttest-mlogloss:0.496521+0.00448365\n",
            "[150]\ttrain-mlogloss:0.29484+0.00304523\ttest-mlogloss:0.494693+0.00427096\n",
            "[160]\ttrain-mlogloss:0.285404+0.00295369\ttest-mlogloss:0.493502+0.00427978\n",
            "[170]\ttrain-mlogloss:0.276215+0.0027949\ttest-mlogloss:0.492198+0.00449685\n",
            "[180]\ttrain-mlogloss:0.267374+0.00307913\ttest-mlogloss:0.491174+0.00461686\n",
            "[190]\ttrain-mlogloss:0.259267+0.00320257\ttest-mlogloss:0.490811+0.00478232\n",
            "[200]\ttrain-mlogloss:0.251532+0.00319255\ttest-mlogloss:0.490236+0.00501218\n",
            "[210]\ttrain-mlogloss:0.244379+0.00331505\ttest-mlogloss:0.489922+0.00496105\n",
            "[220]\ttrain-mlogloss:0.237742+0.00345504\ttest-mlogloss:0.48975+0.0050812\n",
            "[230]\ttrain-mlogloss:0.230931+0.00332961\ttest-mlogloss:0.489596+0.00534861\n",
            "[240]\ttrain-mlogloss:0.224541+0.00301301\ttest-mlogloss:0.489774+0.00549432\n",
            "[250]\ttrain-mlogloss:0.218449+0.00320217\ttest-mlogloss:0.489627+0.0055589\n",
            "[260]\ttrain-mlogloss:0.212812+0.00320053\ttest-mlogloss:0.489736+0.00557785\n",
            "\tmlogloss 0.4895237999999999 for 248 rounds\n",
            "CV with gamma=0.4\n",
            "[0]\ttrain-mlogloss:1.92221+0.00288628\ttest-mlogloss:1.92616+0.00325841\n",
            "[10]\ttrain-mlogloss:0.74973+0.00374724\ttest-mlogloss:0.783544+0.00518026\n",
            "[20]\ttrain-mlogloss:0.576624+0.00237195\ttest-mlogloss:0.631884+0.00523798\n",
            "[30]\ttrain-mlogloss:0.511345+0.00217444\ttest-mlogloss:0.583189+0.00537169\n",
            "[40]\ttrain-mlogloss:0.470799+0.00157792\ttest-mlogloss:0.55845+0.00492991\n",
            "[50]\ttrain-mlogloss:0.442762+0.00203131\ttest-mlogloss:0.543005+0.00482342\n",
            "[60]\ttrain-mlogloss:0.419624+0.00193688\ttest-mlogloss:0.532301+0.00497189\n",
            "[70]\ttrain-mlogloss:0.399778+0.00139376\ttest-mlogloss:0.523739+0.00466124\n",
            "[80]\ttrain-mlogloss:0.381973+0.00176891\ttest-mlogloss:0.516841+0.00422622\n",
            "[90]\ttrain-mlogloss:0.366599+0.00176336\ttest-mlogloss:0.511391+0.00442384\n",
            "[100]\ttrain-mlogloss:0.352646+0.00205845\ttest-mlogloss:0.507492+0.00418218\n",
            "[110]\ttrain-mlogloss:0.339585+0.00232725\ttest-mlogloss:0.503808+0.00451295\n",
            "[120]\ttrain-mlogloss:0.326963+0.0019225\ttest-mlogloss:0.50107+0.0049014\n",
            "[130]\ttrain-mlogloss:0.315958+0.00143108\ttest-mlogloss:0.498792+0.00511749\n",
            "[140]\ttrain-mlogloss:0.305828+0.00169379\ttest-mlogloss:0.496776+0.00554899\n",
            "[150]\ttrain-mlogloss:0.295921+0.0021521\ttest-mlogloss:0.495267+0.00585688\n",
            "[160]\ttrain-mlogloss:0.285908+0.00230637\ttest-mlogloss:0.494074+0.00593762\n",
            "[170]\ttrain-mlogloss:0.276712+0.0023648\ttest-mlogloss:0.492968+0.00627141\n",
            "[180]\ttrain-mlogloss:0.268538+0.00210116\ttest-mlogloss:0.491962+0.00658297\n",
            "[190]\ttrain-mlogloss:0.260529+0.00173013\ttest-mlogloss:0.491004+0.00707586\n",
            "[200]\ttrain-mlogloss:0.252824+0.0016433\ttest-mlogloss:0.490338+0.00719927\n",
            "[210]\ttrain-mlogloss:0.245534+0.00163699\ttest-mlogloss:0.490017+0.00709498\n",
            "[220]\ttrain-mlogloss:0.239033+0.00168481\ttest-mlogloss:0.489917+0.00737855\n",
            "[230]\ttrain-mlogloss:0.232498+0.00141789\ttest-mlogloss:0.489726+0.00782905\n",
            "[240]\ttrain-mlogloss:0.226573+0.00143208\ttest-mlogloss:0.489935+0.00791565\n",
            "\tmlogloss 0.4896344 for 227 rounds\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-185-9d22aa7ca277>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mbest_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmin_child_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Best params: {}, {}, mlogloss: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_mlogloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNq3YjYJybHb",
        "colab_type": "text"
      },
      "source": [
        "## Results\n",
        "Just with data preprocess. No oversampling and no instance weight initialization/rescaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsW1lzgBybHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocess test set \n",
        "train = pd.read_csv(\"trainData.csv\")\n",
        "test = pd.read_csv(\"testData.csv\")\n",
        "y_train = train.target\n",
        "X_train = train.drop('target', axis=1)\n",
        "# preprocess train/test set\n",
        "Xtrain_trans =BCtrans(X_train)\n",
        "test_trans = BCtrans(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKQl7XOjybHf",
        "colab_type": "code",
        "colab": {},
        "outputId": "e942cc53-3764-48e0-813e-2acf23f15550"
      },
      "source": [
        "#Xtrain_trans = BCtrans(X_train)\n",
        "tic = time.clock() #initialize timer\n",
        "Xtrain_trans2 =  Xtrain_trans\n",
        "\n",
        "for v in Xtrain_trans2.keys():\n",
        "    MAD_L,MAD_R = doubleMAD(Xtrain_trans2[v])\n",
        "    if MAD_L != 0 and MAD_R != 0: #check only the columns w/ non-zero MADs\n",
        "        #print(f\"calculating for {v}\")\n",
        "        Xtrain_trans2[v] = handleOutliers(Xtrain_trans2[v])    \n",
        "        \n",
        "toc = time.clock() \n",
        "deltaT = toc - tic #calculate time passed\n",
        "print(\"Time passed:\", deltaT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time passed: 8.006247935038118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5F3ZdxcuybHk",
        "colab_type": "code",
        "colab": {},
        "outputId": "23394d0d-0757-4518-ad54-df628c516bbc"
      },
      "source": [
        "type(Xtrain_trans2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 179
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fx0RUf7ybHq",
        "colab_type": "code",
        "colab": {},
        "outputId": "0beb29f0-6187-418b-9687-08f073d21392"
      },
      "source": [
        "X_trainNEW, X_valNEW, y_train, y_val = train_test_split(Xtrain_trans2,y,test_size = 0.20,stratify=y, random_state=42)\n",
        "dtrain = xgb.DMatrix(X_trainNEW.values, label=y_train.values) \n",
        "dval = xgb.DMatrix(X_valNEW.values, label=y_val.values)\n",
        "params = {\n",
        "    'eta': 0.1,  \n",
        "    'n_estimators': 1000,\n",
        "    'silent': True,  # option for logging\n",
        "    'objective': 'multi:softprob',  # error evaluation for multiclass tasks\n",
        "    'num_class': 9,  # number of classes to predic\n",
        "    'max_depth': 5,  # depth of the trees in the boosting process\n",
        "    'seed': 42,\n",
        "    'eval_metric': 'mlogloss', #cross-entropy is chosen as evaluation metric\n",
        "    'min_child_weight':4,\n",
        "    'gamma':0.0,\n",
        "    'subsample':0.9,\n",
        "    'colsample_bytree':0.8,\n",
        "    'nthread': 16,\n",
        "    #'scale_pos_weight':1,\n",
        "    'reg_alpha':0,\n",
        "    'max_delta_step':1\n",
        "    \n",
        "    }\n",
        "num_round = 50\n",
        "\n",
        "#training the model\n",
        "#model = xgb.train(params, dtrainR, num_round) #change dtrainR w/ dtrain\n",
        "\n",
        "num_boost_round = 999\n",
        "model = xgb.train(\n",
        "    params,\n",
        "    dtrain,#change dtrainR w/ dtrain\n",
        "    num_boost_round=num_boost_round,\n",
        "    evals=[(dtrain, \"Train\"),(dval, \"Test\")],\n",
        "    early_stopping_rounds=20\n",
        ")\n",
        "\n",
        "probsTrain = model.predict(dtrain)\n",
        "probsVal = model.predict(dval)\n",
        "print(\"Training Class Probabilities for First 5 Instances:\\n\",probsTrain[:5])\n",
        "print(\"Validation Class Probabilities for First 5 Instances:\\n\",probsVal[:5])\n",
        "best_predsTrain = np.asarray([np.argmax(line) for line in probsTrain])\n",
        "best_predsVal = np.asarray([np.argmax(line) for line in probsVal])\n",
        "\n",
        "print(\"Best Predictions for Train:\\n\", best_predsTrain+1)\n",
        "print(\"Best Predictions for Validation:\\n\", best_predsVal+1)\n",
        "#print(min(best_preds+1),max(best_preds+1))\n",
        "\n",
        "print(\"Precision Score of the Training Set= \",precision_score(y_train, best_predsTrain, average='macro'))#change y_train(y_val) to y_trainR(y_valR)\n",
        "print(\"Precision Score of the Validation Set= \",precision_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Recall Score of the Training Set= \",recall_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"Recall Score of the Validation Set= \",recall_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"F1 Score of the Training Set= \",f1_score(y_train, best_predsTrain, average='macro'))\n",
        "print(\"F1 Score of the Validation Set= \",f1_score(y_val, best_predsVal, average='macro'))\n",
        "print(\"Accuracy Score the Training Set= \", accuracy_score(y_train, best_predsTrain))\n",
        "print(\"Accuracy Score of the Validation Set= \", accuracy_score(y_val, best_predsVal))\n",
        "scoreTrain = log_loss(y_train, probsTrain)\n",
        "scoreVal = log_loss(y_val, probsVal)\n",
        "print(\"Logloss Score Training Set= \", scoreTrain)\n",
        "print(\"Logloss Score of the Validation Set= \", scoreVal)\n",
        "\n",
        "print(\"Confusion Matrix of the Training Set: \\n\")\n",
        "print(confusion_matrix(y_train, best_predsTrain))\n",
        "print(\"Confusion Matrix of the Validation Set: \\n\")\n",
        "print(confusion_matrix(y_val,best_predsVal))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\tTrain-mlogloss:2.10029\tTest-mlogloss:2.10174\n",
            "Multiple eval metrics have been passed: 'Test-mlogloss' will be used for early stopping.\n",
            "\n",
            "Will train until Test-mlogloss hasn't improved in 20 rounds.\n",
            "[1]\tTrain-mlogloss:2.0113\tTest-mlogloss:2.01411\n",
            "[2]\tTrain-mlogloss:1.92508\tTest-mlogloss:1.92923\n",
            "[3]\tTrain-mlogloss:1.84087\tTest-mlogloss:1.84607\n",
            "[4]\tTrain-mlogloss:1.75957\tTest-mlogloss:1.76543\n",
            "[5]\tTrain-mlogloss:1.68295\tTest-mlogloss:1.69039\n",
            "[6]\tTrain-mlogloss:1.60667\tTest-mlogloss:1.61517\n",
            "[7]\tTrain-mlogloss:1.53735\tTest-mlogloss:1.54696\n",
            "[8]\tTrain-mlogloss:1.47048\tTest-mlogloss:1.48126\n",
            "[9]\tTrain-mlogloss:1.40572\tTest-mlogloss:1.41749\n",
            "[10]\tTrain-mlogloss:1.34642\tTest-mlogloss:1.35987\n",
            "[11]\tTrain-mlogloss:1.29017\tTest-mlogloss:1.30422\n",
            "[12]\tTrain-mlogloss:1.24144\tTest-mlogloss:1.25619\n",
            "[13]\tTrain-mlogloss:1.19565\tTest-mlogloss:1.21146\n",
            "[14]\tTrain-mlogloss:1.15324\tTest-mlogloss:1.16955\n",
            "[15]\tTrain-mlogloss:1.11639\tTest-mlogloss:1.1339\n",
            "[16]\tTrain-mlogloss:1.0797\tTest-mlogloss:1.098\n",
            "[17]\tTrain-mlogloss:1.04633\tTest-mlogloss:1.06573\n",
            "[18]\tTrain-mlogloss:1.01407\tTest-mlogloss:1.03413\n",
            "[19]\tTrain-mlogloss:0.984935\tTest-mlogloss:1.00554\n",
            "[20]\tTrain-mlogloss:0.95833\tTest-mlogloss:0.979618\n",
            "[21]\tTrain-mlogloss:0.934665\tTest-mlogloss:0.956744\n",
            "[22]\tTrain-mlogloss:0.91365\tTest-mlogloss:0.936217\n",
            "[23]\tTrain-mlogloss:0.892974\tTest-mlogloss:0.916079\n",
            "[24]\tTrain-mlogloss:0.873901\tTest-mlogloss:0.89795\n",
            "[25]\tTrain-mlogloss:0.856004\tTest-mlogloss:0.880735\n",
            "[26]\tTrain-mlogloss:0.838964\tTest-mlogloss:0.864167\n",
            "[27]\tTrain-mlogloss:0.824044\tTest-mlogloss:0.849804\n",
            "[28]\tTrain-mlogloss:0.808629\tTest-mlogloss:0.834946\n",
            "[29]\tTrain-mlogloss:0.793996\tTest-mlogloss:0.820873\n",
            "[30]\tTrain-mlogloss:0.780753\tTest-mlogloss:0.808242\n",
            "[31]\tTrain-mlogloss:0.768059\tTest-mlogloss:0.796032\n",
            "[32]\tTrain-mlogloss:0.757457\tTest-mlogloss:0.786189\n",
            "[33]\tTrain-mlogloss:0.746732\tTest-mlogloss:0.775971\n",
            "[34]\tTrain-mlogloss:0.736105\tTest-mlogloss:0.765943\n",
            "[35]\tTrain-mlogloss:0.726019\tTest-mlogloss:0.756434\n",
            "[36]\tTrain-mlogloss:0.716926\tTest-mlogloss:0.748087\n",
            "[37]\tTrain-mlogloss:0.708394\tTest-mlogloss:0.740106\n",
            "[38]\tTrain-mlogloss:0.700559\tTest-mlogloss:0.732721\n",
            "[39]\tTrain-mlogloss:0.692408\tTest-mlogloss:0.725322\n",
            "[40]\tTrain-mlogloss:0.685207\tTest-mlogloss:0.718553\n",
            "[41]\tTrain-mlogloss:0.67818\tTest-mlogloss:0.71204\n",
            "[42]\tTrain-mlogloss:0.671291\tTest-mlogloss:0.705667\n",
            "[43]\tTrain-mlogloss:0.66466\tTest-mlogloss:0.69975\n",
            "[44]\tTrain-mlogloss:0.658643\tTest-mlogloss:0.694116\n",
            "[45]\tTrain-mlogloss:0.653088\tTest-mlogloss:0.689059\n",
            "[46]\tTrain-mlogloss:0.647729\tTest-mlogloss:0.684267\n",
            "[47]\tTrain-mlogloss:0.642164\tTest-mlogloss:0.679251\n",
            "[48]\tTrain-mlogloss:0.636661\tTest-mlogloss:0.674349\n",
            "[49]\tTrain-mlogloss:0.631567\tTest-mlogloss:0.669691\n",
            "[50]\tTrain-mlogloss:0.62685\tTest-mlogloss:0.665541\n",
            "[51]\tTrain-mlogloss:0.622064\tTest-mlogloss:0.661238\n",
            "[52]\tTrain-mlogloss:0.61811\tTest-mlogloss:0.657666\n",
            "[53]\tTrain-mlogloss:0.613477\tTest-mlogloss:0.653798\n",
            "[54]\tTrain-mlogloss:0.609236\tTest-mlogloss:0.65004\n",
            "[55]\tTrain-mlogloss:0.605188\tTest-mlogloss:0.646723\n",
            "[56]\tTrain-mlogloss:0.601636\tTest-mlogloss:0.643654\n",
            "[57]\tTrain-mlogloss:0.598307\tTest-mlogloss:0.640799\n",
            "[58]\tTrain-mlogloss:0.594597\tTest-mlogloss:0.637828\n",
            "[59]\tTrain-mlogloss:0.591139\tTest-mlogloss:0.634941\n",
            "[60]\tTrain-mlogloss:0.587473\tTest-mlogloss:0.631773\n",
            "[61]\tTrain-mlogloss:0.583947\tTest-mlogloss:0.628785\n",
            "[62]\tTrain-mlogloss:0.580651\tTest-mlogloss:0.625899\n",
            "[63]\tTrain-mlogloss:0.577528\tTest-mlogloss:0.623104\n",
            "[64]\tTrain-mlogloss:0.574873\tTest-mlogloss:0.620948\n",
            "[65]\tTrain-mlogloss:0.572065\tTest-mlogloss:0.618703\n",
            "[66]\tTrain-mlogloss:0.569322\tTest-mlogloss:0.616362\n",
            "[67]\tTrain-mlogloss:0.566758\tTest-mlogloss:0.614127\n",
            "[68]\tTrain-mlogloss:0.564369\tTest-mlogloss:0.61218\n",
            "[69]\tTrain-mlogloss:0.561833\tTest-mlogloss:0.610155\n",
            "[70]\tTrain-mlogloss:0.559357\tTest-mlogloss:0.608175\n",
            "[71]\tTrain-mlogloss:0.556601\tTest-mlogloss:0.605937\n",
            "[72]\tTrain-mlogloss:0.554127\tTest-mlogloss:0.603938\n",
            "[73]\tTrain-mlogloss:0.551824\tTest-mlogloss:0.602153\n",
            "[74]\tTrain-mlogloss:0.549607\tTest-mlogloss:0.600323\n",
            "[75]\tTrain-mlogloss:0.547458\tTest-mlogloss:0.598401\n",
            "[76]\tTrain-mlogloss:0.545326\tTest-mlogloss:0.596609\n",
            "[77]\tTrain-mlogloss:0.543137\tTest-mlogloss:0.594871\n",
            "[78]\tTrain-mlogloss:0.54141\tTest-mlogloss:0.59335\n",
            "[79]\tTrain-mlogloss:0.539382\tTest-mlogloss:0.591809\n",
            "[80]\tTrain-mlogloss:0.537621\tTest-mlogloss:0.590559\n",
            "[81]\tTrain-mlogloss:0.535772\tTest-mlogloss:0.589131\n",
            "[82]\tTrain-mlogloss:0.534144\tTest-mlogloss:0.587875\n",
            "[83]\tTrain-mlogloss:0.53244\tTest-mlogloss:0.586713\n",
            "[84]\tTrain-mlogloss:0.530682\tTest-mlogloss:0.585404\n",
            "[85]\tTrain-mlogloss:0.528942\tTest-mlogloss:0.584095\n",
            "[86]\tTrain-mlogloss:0.527313\tTest-mlogloss:0.582716\n",
            "[87]\tTrain-mlogloss:0.525759\tTest-mlogloss:0.581638\n",
            "[88]\tTrain-mlogloss:0.523621\tTest-mlogloss:0.580028\n",
            "[89]\tTrain-mlogloss:0.522135\tTest-mlogloss:0.578912\n",
            "[90]\tTrain-mlogloss:0.520783\tTest-mlogloss:0.577903\n",
            "[91]\tTrain-mlogloss:0.519014\tTest-mlogloss:0.57665\n",
            "[92]\tTrain-mlogloss:0.517441\tTest-mlogloss:0.575488\n",
            "[93]\tTrain-mlogloss:0.515877\tTest-mlogloss:0.574404\n",
            "[94]\tTrain-mlogloss:0.514531\tTest-mlogloss:0.573349\n",
            "[95]\tTrain-mlogloss:0.512982\tTest-mlogloss:0.572317\n",
            "[96]\tTrain-mlogloss:0.51148\tTest-mlogloss:0.571215\n",
            "[97]\tTrain-mlogloss:0.509901\tTest-mlogloss:0.570045\n",
            "[98]\tTrain-mlogloss:0.50864\tTest-mlogloss:0.569242\n",
            "[99]\tTrain-mlogloss:0.50708\tTest-mlogloss:0.568125\n",
            "[100]\tTrain-mlogloss:0.50573\tTest-mlogloss:0.567274\n",
            "[101]\tTrain-mlogloss:0.504151\tTest-mlogloss:0.566029\n",
            "[102]\tTrain-mlogloss:0.503007\tTest-mlogloss:0.565397\n",
            "[103]\tTrain-mlogloss:0.50178\tTest-mlogloss:0.564573\n",
            "[104]\tTrain-mlogloss:0.500633\tTest-mlogloss:0.563784\n",
            "[105]\tTrain-mlogloss:0.499278\tTest-mlogloss:0.562896\n",
            "[106]\tTrain-mlogloss:0.498006\tTest-mlogloss:0.562112\n",
            "[107]\tTrain-mlogloss:0.496728\tTest-mlogloss:0.561162\n",
            "[108]\tTrain-mlogloss:0.495379\tTest-mlogloss:0.560212\n",
            "[109]\tTrain-mlogloss:0.494078\tTest-mlogloss:0.559299\n",
            "[110]\tTrain-mlogloss:0.492943\tTest-mlogloss:0.558611\n",
            "[111]\tTrain-mlogloss:0.491893\tTest-mlogloss:0.557771\n",
            "[112]\tTrain-mlogloss:0.490645\tTest-mlogloss:0.556943\n",
            "[113]\tTrain-mlogloss:0.489395\tTest-mlogloss:0.556172\n",
            "[114]\tTrain-mlogloss:0.488435\tTest-mlogloss:0.555594\n",
            "[115]\tTrain-mlogloss:0.487399\tTest-mlogloss:0.554758\n",
            "[116]\tTrain-mlogloss:0.486016\tTest-mlogloss:0.553738\n",
            "[117]\tTrain-mlogloss:0.484824\tTest-mlogloss:0.55296\n",
            "[118]\tTrain-mlogloss:0.483451\tTest-mlogloss:0.552218\n",
            "[119]\tTrain-mlogloss:0.482311\tTest-mlogloss:0.551474\n",
            "[120]\tTrain-mlogloss:0.481152\tTest-mlogloss:0.550686\n",
            "[121]\tTrain-mlogloss:0.480156\tTest-mlogloss:0.550118\n",
            "[122]\tTrain-mlogloss:0.479157\tTest-mlogloss:0.549477\n",
            "[123]\tTrain-mlogloss:0.478426\tTest-mlogloss:0.549029\n",
            "[124]\tTrain-mlogloss:0.477354\tTest-mlogloss:0.548315\n",
            "[125]\tTrain-mlogloss:0.47627\tTest-mlogloss:0.54767\n",
            "[126]\tTrain-mlogloss:0.475212\tTest-mlogloss:0.547035\n",
            "[127]\tTrain-mlogloss:0.474127\tTest-mlogloss:0.546427\n",
            "[128]\tTrain-mlogloss:0.473139\tTest-mlogloss:0.545854\n",
            "[129]\tTrain-mlogloss:0.472097\tTest-mlogloss:0.545222\n",
            "[130]\tTrain-mlogloss:0.471055\tTest-mlogloss:0.544683\n",
            "[131]\tTrain-mlogloss:0.470025\tTest-mlogloss:0.54414\n",
            "[132]\tTrain-mlogloss:0.4692\tTest-mlogloss:0.543622\n",
            "[133]\tTrain-mlogloss:0.468275\tTest-mlogloss:0.543074\n",
            "[134]\tTrain-mlogloss:0.467259\tTest-mlogloss:0.542441\n",
            "[135]\tTrain-mlogloss:0.46623\tTest-mlogloss:0.541834\n",
            "[136]\tTrain-mlogloss:0.465306\tTest-mlogloss:0.541279\n",
            "[137]\tTrain-mlogloss:0.46457\tTest-mlogloss:0.540978\n",
            "[138]\tTrain-mlogloss:0.463638\tTest-mlogloss:0.540372\n",
            "[139]\tTrain-mlogloss:0.462625\tTest-mlogloss:0.539809\n",
            "[140]\tTrain-mlogloss:0.461781\tTest-mlogloss:0.539393\n",
            "[141]\tTrain-mlogloss:0.460759\tTest-mlogloss:0.53883\n",
            "[142]\tTrain-mlogloss:0.460092\tTest-mlogloss:0.538381\n",
            "[143]\tTrain-mlogloss:0.45913\tTest-mlogloss:0.53781\n",
            "[144]\tTrain-mlogloss:0.458241\tTest-mlogloss:0.537335\n",
            "[145]\tTrain-mlogloss:0.457276\tTest-mlogloss:0.536767\n",
            "[146]\tTrain-mlogloss:0.456301\tTest-mlogloss:0.536136\n",
            "[147]\tTrain-mlogloss:0.45538\tTest-mlogloss:0.535589\n",
            "[148]\tTrain-mlogloss:0.45448\tTest-mlogloss:0.535158\n",
            "[149]\tTrain-mlogloss:0.453621\tTest-mlogloss:0.534672\n",
            "[150]\tTrain-mlogloss:0.452793\tTest-mlogloss:0.534147\n",
            "[151]\tTrain-mlogloss:0.452106\tTest-mlogloss:0.533739\n",
            "[152]\tTrain-mlogloss:0.451269\tTest-mlogloss:0.533245\n",
            "[153]\tTrain-mlogloss:0.450583\tTest-mlogloss:0.532994\n",
            "[154]\tTrain-mlogloss:0.449903\tTest-mlogloss:0.53267\n",
            "[155]\tTrain-mlogloss:0.449152\tTest-mlogloss:0.532282\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[156]\tTrain-mlogloss:0.448565\tTest-mlogloss:0.531976\n",
            "[157]\tTrain-mlogloss:0.447863\tTest-mlogloss:0.531502\n",
            "[158]\tTrain-mlogloss:0.446921\tTest-mlogloss:0.530986\n",
            "[159]\tTrain-mlogloss:0.446064\tTest-mlogloss:0.530543\n",
            "[160]\tTrain-mlogloss:0.445333\tTest-mlogloss:0.530103\n",
            "[161]\tTrain-mlogloss:0.44462\tTest-mlogloss:0.529715\n",
            "[162]\tTrain-mlogloss:0.443999\tTest-mlogloss:0.529366\n",
            "[163]\tTrain-mlogloss:0.443359\tTest-mlogloss:0.529031\n",
            "[164]\tTrain-mlogloss:0.442541\tTest-mlogloss:0.528464\n",
            "[165]\tTrain-mlogloss:0.441584\tTest-mlogloss:0.527922\n",
            "[166]\tTrain-mlogloss:0.440688\tTest-mlogloss:0.527412\n",
            "[167]\tTrain-mlogloss:0.439862\tTest-mlogloss:0.526908\n",
            "[168]\tTrain-mlogloss:0.43916\tTest-mlogloss:0.526537\n",
            "[169]\tTrain-mlogloss:0.438293\tTest-mlogloss:0.526068\n",
            "[170]\tTrain-mlogloss:0.437418\tTest-mlogloss:0.525518\n",
            "[171]\tTrain-mlogloss:0.436831\tTest-mlogloss:0.525237\n",
            "[172]\tTrain-mlogloss:0.436082\tTest-mlogloss:0.524843\n",
            "[173]\tTrain-mlogloss:0.435438\tTest-mlogloss:0.524573\n",
            "[174]\tTrain-mlogloss:0.434844\tTest-mlogloss:0.524316\n",
            "[175]\tTrain-mlogloss:0.43425\tTest-mlogloss:0.523997\n",
            "[176]\tTrain-mlogloss:0.433402\tTest-mlogloss:0.523553\n",
            "[177]\tTrain-mlogloss:0.432873\tTest-mlogloss:0.523394\n",
            "[178]\tTrain-mlogloss:0.432331\tTest-mlogloss:0.523171\n",
            "[179]\tTrain-mlogloss:0.431565\tTest-mlogloss:0.522862\n",
            "[180]\tTrain-mlogloss:0.430754\tTest-mlogloss:0.522412\n",
            "[181]\tTrain-mlogloss:0.430042\tTest-mlogloss:0.522052\n",
            "[182]\tTrain-mlogloss:0.429357\tTest-mlogloss:0.521773\n",
            "[183]\tTrain-mlogloss:0.428693\tTest-mlogloss:0.521357\n",
            "[184]\tTrain-mlogloss:0.427986\tTest-mlogloss:0.520998\n",
            "[185]\tTrain-mlogloss:0.42739\tTest-mlogloss:0.520645\n",
            "[186]\tTrain-mlogloss:0.426508\tTest-mlogloss:0.520238\n",
            "[187]\tTrain-mlogloss:0.42584\tTest-mlogloss:0.519901\n",
            "[188]\tTrain-mlogloss:0.425146\tTest-mlogloss:0.519599\n",
            "[189]\tTrain-mlogloss:0.424398\tTest-mlogloss:0.519298\n",
            "[190]\tTrain-mlogloss:0.423785\tTest-mlogloss:0.519036\n",
            "[191]\tTrain-mlogloss:0.423092\tTest-mlogloss:0.518769\n",
            "[192]\tTrain-mlogloss:0.422399\tTest-mlogloss:0.518391\n",
            "[193]\tTrain-mlogloss:0.421756\tTest-mlogloss:0.518107\n",
            "[194]\tTrain-mlogloss:0.421244\tTest-mlogloss:0.517913\n",
            "[195]\tTrain-mlogloss:0.420505\tTest-mlogloss:0.517526\n",
            "[196]\tTrain-mlogloss:0.420031\tTest-mlogloss:0.517397\n",
            "[197]\tTrain-mlogloss:0.419269\tTest-mlogloss:0.517034\n",
            "[198]\tTrain-mlogloss:0.41853\tTest-mlogloss:0.516858\n",
            "[199]\tTrain-mlogloss:0.41787\tTest-mlogloss:0.516609\n",
            "[200]\tTrain-mlogloss:0.417312\tTest-mlogloss:0.5164\n",
            "[201]\tTrain-mlogloss:0.416575\tTest-mlogloss:0.515938\n",
            "[202]\tTrain-mlogloss:0.416128\tTest-mlogloss:0.515692\n",
            "[203]\tTrain-mlogloss:0.4155\tTest-mlogloss:0.515329\n",
            "[204]\tTrain-mlogloss:0.414942\tTest-mlogloss:0.515055\n",
            "[205]\tTrain-mlogloss:0.414403\tTest-mlogloss:0.514889\n",
            "[206]\tTrain-mlogloss:0.414048\tTest-mlogloss:0.514747\n",
            "[207]\tTrain-mlogloss:0.413519\tTest-mlogloss:0.514457\n",
            "[208]\tTrain-mlogloss:0.412853\tTest-mlogloss:0.514127\n",
            "[209]\tTrain-mlogloss:0.41232\tTest-mlogloss:0.513998\n",
            "[210]\tTrain-mlogloss:0.411597\tTest-mlogloss:0.513684\n",
            "[211]\tTrain-mlogloss:0.41112\tTest-mlogloss:0.513506\n",
            "[212]\tTrain-mlogloss:0.410463\tTest-mlogloss:0.513243\n",
            "[213]\tTrain-mlogloss:0.409793\tTest-mlogloss:0.512886\n",
            "[214]\tTrain-mlogloss:0.409303\tTest-mlogloss:0.512735\n",
            "[215]\tTrain-mlogloss:0.408634\tTest-mlogloss:0.512444\n",
            "[216]\tTrain-mlogloss:0.407918\tTest-mlogloss:0.512112\n",
            "[217]\tTrain-mlogloss:0.407399\tTest-mlogloss:0.511895\n",
            "[218]\tTrain-mlogloss:0.406701\tTest-mlogloss:0.511575\n",
            "[219]\tTrain-mlogloss:0.406279\tTest-mlogloss:0.511467\n",
            "[220]\tTrain-mlogloss:0.405743\tTest-mlogloss:0.511287\n",
            "[221]\tTrain-mlogloss:0.405151\tTest-mlogloss:0.51102\n",
            "[222]\tTrain-mlogloss:0.40462\tTest-mlogloss:0.5108\n",
            "[223]\tTrain-mlogloss:0.404084\tTest-mlogloss:0.510595\n",
            "[224]\tTrain-mlogloss:0.403574\tTest-mlogloss:0.510368\n",
            "[225]\tTrain-mlogloss:0.403018\tTest-mlogloss:0.510089\n",
            "[226]\tTrain-mlogloss:0.402488\tTest-mlogloss:0.509887\n",
            "[227]\tTrain-mlogloss:0.402081\tTest-mlogloss:0.509816\n",
            "[228]\tTrain-mlogloss:0.401383\tTest-mlogloss:0.509582\n",
            "[229]\tTrain-mlogloss:0.400816\tTest-mlogloss:0.509284\n",
            "[230]\tTrain-mlogloss:0.40035\tTest-mlogloss:0.509012\n",
            "[231]\tTrain-mlogloss:0.399707\tTest-mlogloss:0.5086\n",
            "[232]\tTrain-mlogloss:0.399228\tTest-mlogloss:0.508345\n",
            "[233]\tTrain-mlogloss:0.398591\tTest-mlogloss:0.508035\n",
            "[234]\tTrain-mlogloss:0.398029\tTest-mlogloss:0.507863\n",
            "[235]\tTrain-mlogloss:0.397418\tTest-mlogloss:0.507603\n",
            "[236]\tTrain-mlogloss:0.396767\tTest-mlogloss:0.50725\n",
            "[237]\tTrain-mlogloss:0.396122\tTest-mlogloss:0.507135\n",
            "[238]\tTrain-mlogloss:0.395647\tTest-mlogloss:0.506868\n",
            "[239]\tTrain-mlogloss:0.395192\tTest-mlogloss:0.506579\n",
            "[240]\tTrain-mlogloss:0.394771\tTest-mlogloss:0.506446\n",
            "[241]\tTrain-mlogloss:0.394386\tTest-mlogloss:0.506231\n",
            "[242]\tTrain-mlogloss:0.393794\tTest-mlogloss:0.505941\n",
            "[243]\tTrain-mlogloss:0.393433\tTest-mlogloss:0.505765\n",
            "[244]\tTrain-mlogloss:0.393067\tTest-mlogloss:0.505557\n",
            "[245]\tTrain-mlogloss:0.392415\tTest-mlogloss:0.505271\n",
            "[246]\tTrain-mlogloss:0.391699\tTest-mlogloss:0.505049\n",
            "[247]\tTrain-mlogloss:0.39132\tTest-mlogloss:0.504865\n",
            "[248]\tTrain-mlogloss:0.390934\tTest-mlogloss:0.504649\n",
            "[249]\tTrain-mlogloss:0.390298\tTest-mlogloss:0.504331\n",
            "[250]\tTrain-mlogloss:0.389871\tTest-mlogloss:0.504201\n",
            "[251]\tTrain-mlogloss:0.389239\tTest-mlogloss:0.504004\n",
            "[252]\tTrain-mlogloss:0.388719\tTest-mlogloss:0.503808\n",
            "[253]\tTrain-mlogloss:0.388165\tTest-mlogloss:0.503525\n",
            "[254]\tTrain-mlogloss:0.387702\tTest-mlogloss:0.503367\n",
            "[255]\tTrain-mlogloss:0.387192\tTest-mlogloss:0.503243\n",
            "[256]\tTrain-mlogloss:0.386708\tTest-mlogloss:0.503044\n",
            "[257]\tTrain-mlogloss:0.385904\tTest-mlogloss:0.50268\n",
            "[258]\tTrain-mlogloss:0.385305\tTest-mlogloss:0.502456\n",
            "[259]\tTrain-mlogloss:0.384813\tTest-mlogloss:0.50228\n",
            "[260]\tTrain-mlogloss:0.38421\tTest-mlogloss:0.501975\n",
            "[261]\tTrain-mlogloss:0.383711\tTest-mlogloss:0.501846\n",
            "[262]\tTrain-mlogloss:0.383087\tTest-mlogloss:0.501521\n",
            "[263]\tTrain-mlogloss:0.382646\tTest-mlogloss:0.501348\n",
            "[264]\tTrain-mlogloss:0.38231\tTest-mlogloss:0.501187\n",
            "[265]\tTrain-mlogloss:0.38173\tTest-mlogloss:0.50085\n",
            "[266]\tTrain-mlogloss:0.381352\tTest-mlogloss:0.500723\n",
            "[267]\tTrain-mlogloss:0.380964\tTest-mlogloss:0.500594\n",
            "[268]\tTrain-mlogloss:0.380565\tTest-mlogloss:0.500477\n",
            "[269]\tTrain-mlogloss:0.379868\tTest-mlogloss:0.500234\n",
            "[270]\tTrain-mlogloss:0.379556\tTest-mlogloss:0.500128\n",
            "[271]\tTrain-mlogloss:0.37896\tTest-mlogloss:0.499929\n",
            "[272]\tTrain-mlogloss:0.378497\tTest-mlogloss:0.4998\n",
            "[273]\tTrain-mlogloss:0.378034\tTest-mlogloss:0.499663\n",
            "[274]\tTrain-mlogloss:0.377559\tTest-mlogloss:0.499386\n",
            "[275]\tTrain-mlogloss:0.377111\tTest-mlogloss:0.499217\n",
            "[276]\tTrain-mlogloss:0.376707\tTest-mlogloss:0.499009\n",
            "[277]\tTrain-mlogloss:0.37618\tTest-mlogloss:0.498782\n",
            "[278]\tTrain-mlogloss:0.375754\tTest-mlogloss:0.498795\n",
            "[279]\tTrain-mlogloss:0.375273\tTest-mlogloss:0.498579\n",
            "[280]\tTrain-mlogloss:0.374751\tTest-mlogloss:0.498482\n",
            "[281]\tTrain-mlogloss:0.374307\tTest-mlogloss:0.498334\n",
            "[282]\tTrain-mlogloss:0.373988\tTest-mlogloss:0.498203\n",
            "[283]\tTrain-mlogloss:0.373595\tTest-mlogloss:0.498066\n",
            "[284]\tTrain-mlogloss:0.373225\tTest-mlogloss:0.498019\n",
            "[285]\tTrain-mlogloss:0.372741\tTest-mlogloss:0.49774\n",
            "[286]\tTrain-mlogloss:0.372337\tTest-mlogloss:0.497666\n",
            "[287]\tTrain-mlogloss:0.371865\tTest-mlogloss:0.497496\n",
            "[288]\tTrain-mlogloss:0.371404\tTest-mlogloss:0.497273\n",
            "[289]\tTrain-mlogloss:0.371031\tTest-mlogloss:0.497107\n",
            "[290]\tTrain-mlogloss:0.370697\tTest-mlogloss:0.496954\n",
            "[291]\tTrain-mlogloss:0.370318\tTest-mlogloss:0.496771\n",
            "[292]\tTrain-mlogloss:0.370013\tTest-mlogloss:0.496707\n",
            "[293]\tTrain-mlogloss:0.369725\tTest-mlogloss:0.496633\n",
            "[294]\tTrain-mlogloss:0.369214\tTest-mlogloss:0.496475\n",
            "[295]\tTrain-mlogloss:0.368825\tTest-mlogloss:0.496395\n",
            "[296]\tTrain-mlogloss:0.368518\tTest-mlogloss:0.496273\n",
            "[297]\tTrain-mlogloss:0.368076\tTest-mlogloss:0.496093\n",
            "[298]\tTrain-mlogloss:0.367712\tTest-mlogloss:0.495964\n",
            "[299]\tTrain-mlogloss:0.367291\tTest-mlogloss:0.495911\n",
            "[300]\tTrain-mlogloss:0.366806\tTest-mlogloss:0.495733\n",
            "[301]\tTrain-mlogloss:0.366436\tTest-mlogloss:0.495628\n",
            "[302]\tTrain-mlogloss:0.366073\tTest-mlogloss:0.495323\n",
            "[303]\tTrain-mlogloss:0.365716\tTest-mlogloss:0.495201\n",
            "[304]\tTrain-mlogloss:0.365263\tTest-mlogloss:0.49497\n",
            "[305]\tTrain-mlogloss:0.364851\tTest-mlogloss:0.494793\n",
            "[306]\tTrain-mlogloss:0.36452\tTest-mlogloss:0.494783\n",
            "[307]\tTrain-mlogloss:0.364168\tTest-mlogloss:0.49468\n",
            "[308]\tTrain-mlogloss:0.363803\tTest-mlogloss:0.494499\n",
            "[309]\tTrain-mlogloss:0.363399\tTest-mlogloss:0.494304\n",
            "[310]\tTrain-mlogloss:0.362891\tTest-mlogloss:0.494047\n",
            "[311]\tTrain-mlogloss:0.362518\tTest-mlogloss:0.493967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[312]\tTrain-mlogloss:0.362056\tTest-mlogloss:0.493846\n",
            "[313]\tTrain-mlogloss:0.361528\tTest-mlogloss:0.493676\n",
            "[314]\tTrain-mlogloss:0.361198\tTest-mlogloss:0.493551\n",
            "[315]\tTrain-mlogloss:0.36073\tTest-mlogloss:0.493539\n",
            "[316]\tTrain-mlogloss:0.360391\tTest-mlogloss:0.49346\n",
            "[317]\tTrain-mlogloss:0.359993\tTest-mlogloss:0.493399\n",
            "[318]\tTrain-mlogloss:0.359589\tTest-mlogloss:0.493224\n",
            "[319]\tTrain-mlogloss:0.359266\tTest-mlogloss:0.493123\n",
            "[320]\tTrain-mlogloss:0.358917\tTest-mlogloss:0.493068\n",
            "[321]\tTrain-mlogloss:0.358575\tTest-mlogloss:0.492889\n",
            "[322]\tTrain-mlogloss:0.358249\tTest-mlogloss:0.492747\n",
            "[323]\tTrain-mlogloss:0.358002\tTest-mlogloss:0.492677\n",
            "[324]\tTrain-mlogloss:0.357686\tTest-mlogloss:0.492613\n",
            "[325]\tTrain-mlogloss:0.35734\tTest-mlogloss:0.492611\n",
            "[326]\tTrain-mlogloss:0.356874\tTest-mlogloss:0.492479\n",
            "[327]\tTrain-mlogloss:0.35644\tTest-mlogloss:0.492316\n",
            "[328]\tTrain-mlogloss:0.355719\tTest-mlogloss:0.492014\n",
            "[329]\tTrain-mlogloss:0.355281\tTest-mlogloss:0.491941\n",
            "[330]\tTrain-mlogloss:0.354869\tTest-mlogloss:0.491724\n",
            "[331]\tTrain-mlogloss:0.354421\tTest-mlogloss:0.491549\n",
            "[332]\tTrain-mlogloss:0.353963\tTest-mlogloss:0.491369\n",
            "[333]\tTrain-mlogloss:0.35349\tTest-mlogloss:0.491191\n",
            "[334]\tTrain-mlogloss:0.35307\tTest-mlogloss:0.49103\n",
            "[335]\tTrain-mlogloss:0.352645\tTest-mlogloss:0.490854\n",
            "[336]\tTrain-mlogloss:0.352317\tTest-mlogloss:0.490761\n",
            "[337]\tTrain-mlogloss:0.35196\tTest-mlogloss:0.49065\n",
            "[338]\tTrain-mlogloss:0.351669\tTest-mlogloss:0.490523\n",
            "[339]\tTrain-mlogloss:0.351367\tTest-mlogloss:0.490465\n",
            "[340]\tTrain-mlogloss:0.351046\tTest-mlogloss:0.490375\n",
            "[341]\tTrain-mlogloss:0.350557\tTest-mlogloss:0.490246\n",
            "[342]\tTrain-mlogloss:0.350349\tTest-mlogloss:0.490229\n",
            "[343]\tTrain-mlogloss:0.349957\tTest-mlogloss:0.490095\n",
            "[344]\tTrain-mlogloss:0.349518\tTest-mlogloss:0.490002\n",
            "[345]\tTrain-mlogloss:0.349186\tTest-mlogloss:0.489938\n",
            "[346]\tTrain-mlogloss:0.348837\tTest-mlogloss:0.489854\n",
            "[347]\tTrain-mlogloss:0.348417\tTest-mlogloss:0.489729\n",
            "[348]\tTrain-mlogloss:0.348065\tTest-mlogloss:0.489638\n",
            "[349]\tTrain-mlogloss:0.347796\tTest-mlogloss:0.489618\n",
            "[350]\tTrain-mlogloss:0.347564\tTest-mlogloss:0.489571\n",
            "[351]\tTrain-mlogloss:0.347154\tTest-mlogloss:0.489442\n",
            "[352]\tTrain-mlogloss:0.346855\tTest-mlogloss:0.489323\n",
            "[353]\tTrain-mlogloss:0.346645\tTest-mlogloss:0.48927\n",
            "[354]\tTrain-mlogloss:0.346321\tTest-mlogloss:0.489199\n",
            "[355]\tTrain-mlogloss:0.345961\tTest-mlogloss:0.489019\n",
            "[356]\tTrain-mlogloss:0.345524\tTest-mlogloss:0.488833\n",
            "[357]\tTrain-mlogloss:0.345014\tTest-mlogloss:0.488688\n",
            "[358]\tTrain-mlogloss:0.344768\tTest-mlogloss:0.48867\n",
            "[359]\tTrain-mlogloss:0.344398\tTest-mlogloss:0.488568\n",
            "[360]\tTrain-mlogloss:0.344059\tTest-mlogloss:0.488355\n",
            "[361]\tTrain-mlogloss:0.343837\tTest-mlogloss:0.488309\n",
            "[362]\tTrain-mlogloss:0.343587\tTest-mlogloss:0.488189\n",
            "[363]\tTrain-mlogloss:0.343207\tTest-mlogloss:0.488086\n",
            "[364]\tTrain-mlogloss:0.342906\tTest-mlogloss:0.487988\n",
            "[365]\tTrain-mlogloss:0.342541\tTest-mlogloss:0.487848\n",
            "[366]\tTrain-mlogloss:0.342132\tTest-mlogloss:0.487775\n",
            "[367]\tTrain-mlogloss:0.341618\tTest-mlogloss:0.487594\n",
            "[368]\tTrain-mlogloss:0.341235\tTest-mlogloss:0.487476\n",
            "[369]\tTrain-mlogloss:0.340881\tTest-mlogloss:0.487359\n",
            "[370]\tTrain-mlogloss:0.340548\tTest-mlogloss:0.487303\n",
            "[371]\tTrain-mlogloss:0.340098\tTest-mlogloss:0.487127\n",
            "[372]\tTrain-mlogloss:0.339723\tTest-mlogloss:0.487019\n",
            "[373]\tTrain-mlogloss:0.339265\tTest-mlogloss:0.486873\n",
            "[374]\tTrain-mlogloss:0.338944\tTest-mlogloss:0.486762\n",
            "[375]\tTrain-mlogloss:0.338656\tTest-mlogloss:0.486636\n",
            "[376]\tTrain-mlogloss:0.338359\tTest-mlogloss:0.486536\n",
            "[377]\tTrain-mlogloss:0.337837\tTest-mlogloss:0.486457\n",
            "[378]\tTrain-mlogloss:0.337448\tTest-mlogloss:0.486406\n",
            "[379]\tTrain-mlogloss:0.336974\tTest-mlogloss:0.486257\n",
            "[380]\tTrain-mlogloss:0.336648\tTest-mlogloss:0.48611\n",
            "[381]\tTrain-mlogloss:0.336288\tTest-mlogloss:0.48603\n",
            "[382]\tTrain-mlogloss:0.335978\tTest-mlogloss:0.48594\n",
            "[383]\tTrain-mlogloss:0.335601\tTest-mlogloss:0.485754\n",
            "[384]\tTrain-mlogloss:0.335262\tTest-mlogloss:0.485633\n",
            "[385]\tTrain-mlogloss:0.334942\tTest-mlogloss:0.485561\n",
            "[386]\tTrain-mlogloss:0.33456\tTest-mlogloss:0.485498\n",
            "[387]\tTrain-mlogloss:0.334137\tTest-mlogloss:0.485411\n",
            "[388]\tTrain-mlogloss:0.333725\tTest-mlogloss:0.485286\n",
            "[389]\tTrain-mlogloss:0.333295\tTest-mlogloss:0.48504\n",
            "[390]\tTrain-mlogloss:0.332963\tTest-mlogloss:0.485053\n",
            "[391]\tTrain-mlogloss:0.332664\tTest-mlogloss:0.484953\n",
            "[392]\tTrain-mlogloss:0.332341\tTest-mlogloss:0.484863\n",
            "[393]\tTrain-mlogloss:0.332124\tTest-mlogloss:0.484784\n",
            "[394]\tTrain-mlogloss:0.331808\tTest-mlogloss:0.484661\n",
            "[395]\tTrain-mlogloss:0.331405\tTest-mlogloss:0.484634\n",
            "[396]\tTrain-mlogloss:0.331097\tTest-mlogloss:0.484561\n",
            "[397]\tTrain-mlogloss:0.330816\tTest-mlogloss:0.484445\n",
            "[398]\tTrain-mlogloss:0.330418\tTest-mlogloss:0.484251\n",
            "[399]\tTrain-mlogloss:0.329942\tTest-mlogloss:0.48414\n",
            "[400]\tTrain-mlogloss:0.329559\tTest-mlogloss:0.484085\n",
            "[401]\tTrain-mlogloss:0.32922\tTest-mlogloss:0.483923\n",
            "[402]\tTrain-mlogloss:0.328807\tTest-mlogloss:0.483824\n",
            "[403]\tTrain-mlogloss:0.328512\tTest-mlogloss:0.483781\n",
            "[404]\tTrain-mlogloss:0.328147\tTest-mlogloss:0.483645\n",
            "[405]\tTrain-mlogloss:0.327799\tTest-mlogloss:0.483597\n",
            "[406]\tTrain-mlogloss:0.32746\tTest-mlogloss:0.483489\n",
            "[407]\tTrain-mlogloss:0.327077\tTest-mlogloss:0.483387\n",
            "[408]\tTrain-mlogloss:0.326633\tTest-mlogloss:0.48328\n",
            "[409]\tTrain-mlogloss:0.326324\tTest-mlogloss:0.483254\n",
            "[410]\tTrain-mlogloss:0.326118\tTest-mlogloss:0.483246\n",
            "[411]\tTrain-mlogloss:0.325832\tTest-mlogloss:0.483169\n",
            "[412]\tTrain-mlogloss:0.325431\tTest-mlogloss:0.483124\n",
            "[413]\tTrain-mlogloss:0.325084\tTest-mlogloss:0.482985\n",
            "[414]\tTrain-mlogloss:0.324757\tTest-mlogloss:0.48288\n",
            "[415]\tTrain-mlogloss:0.324494\tTest-mlogloss:0.482824\n",
            "[416]\tTrain-mlogloss:0.324108\tTest-mlogloss:0.482694\n",
            "[417]\tTrain-mlogloss:0.323799\tTest-mlogloss:0.482592\n",
            "[418]\tTrain-mlogloss:0.323355\tTest-mlogloss:0.482487\n",
            "[419]\tTrain-mlogloss:0.322945\tTest-mlogloss:0.482392\n",
            "[420]\tTrain-mlogloss:0.322687\tTest-mlogloss:0.482401\n",
            "[421]\tTrain-mlogloss:0.322356\tTest-mlogloss:0.482312\n",
            "[422]\tTrain-mlogloss:0.321936\tTest-mlogloss:0.482268\n",
            "[423]\tTrain-mlogloss:0.321665\tTest-mlogloss:0.482168\n",
            "[424]\tTrain-mlogloss:0.321354\tTest-mlogloss:0.482118\n",
            "[425]\tTrain-mlogloss:0.321009\tTest-mlogloss:0.482079\n",
            "[426]\tTrain-mlogloss:0.320764\tTest-mlogloss:0.481991\n",
            "[427]\tTrain-mlogloss:0.320279\tTest-mlogloss:0.481804\n",
            "[428]\tTrain-mlogloss:0.320007\tTest-mlogloss:0.481724\n",
            "[429]\tTrain-mlogloss:0.319695\tTest-mlogloss:0.481694\n",
            "[430]\tTrain-mlogloss:0.319343\tTest-mlogloss:0.481653\n",
            "[431]\tTrain-mlogloss:0.31904\tTest-mlogloss:0.481641\n",
            "[432]\tTrain-mlogloss:0.318762\tTest-mlogloss:0.481602\n",
            "[433]\tTrain-mlogloss:0.318526\tTest-mlogloss:0.481547\n",
            "[434]\tTrain-mlogloss:0.31827\tTest-mlogloss:0.481528\n",
            "[435]\tTrain-mlogloss:0.318033\tTest-mlogloss:0.481472\n",
            "[436]\tTrain-mlogloss:0.317689\tTest-mlogloss:0.481387\n",
            "[437]\tTrain-mlogloss:0.317317\tTest-mlogloss:0.481365\n",
            "[438]\tTrain-mlogloss:0.317017\tTest-mlogloss:0.481389\n",
            "[439]\tTrain-mlogloss:0.316723\tTest-mlogloss:0.481342\n",
            "[440]\tTrain-mlogloss:0.316428\tTest-mlogloss:0.48127\n",
            "[441]\tTrain-mlogloss:0.316173\tTest-mlogloss:0.481232\n",
            "[442]\tTrain-mlogloss:0.315928\tTest-mlogloss:0.481154\n",
            "[443]\tTrain-mlogloss:0.315485\tTest-mlogloss:0.481106\n",
            "[444]\tTrain-mlogloss:0.315182\tTest-mlogloss:0.481015\n",
            "[445]\tTrain-mlogloss:0.314771\tTest-mlogloss:0.480998\n",
            "[446]\tTrain-mlogloss:0.314496\tTest-mlogloss:0.480881\n",
            "[447]\tTrain-mlogloss:0.314095\tTest-mlogloss:0.480798\n",
            "[448]\tTrain-mlogloss:0.313649\tTest-mlogloss:0.480589\n",
            "[449]\tTrain-mlogloss:0.313429\tTest-mlogloss:0.480569\n",
            "[450]\tTrain-mlogloss:0.313202\tTest-mlogloss:0.480437\n",
            "[451]\tTrain-mlogloss:0.312913\tTest-mlogloss:0.480341\n",
            "[452]\tTrain-mlogloss:0.312607\tTest-mlogloss:0.480275\n",
            "[453]\tTrain-mlogloss:0.312336\tTest-mlogloss:0.480192\n",
            "[454]\tTrain-mlogloss:0.312041\tTest-mlogloss:0.480093\n",
            "[455]\tTrain-mlogloss:0.311612\tTest-mlogloss:0.480129\n",
            "[456]\tTrain-mlogloss:0.311199\tTest-mlogloss:0.48004\n",
            "[457]\tTrain-mlogloss:0.310923\tTest-mlogloss:0.479998\n",
            "[458]\tTrain-mlogloss:0.31058\tTest-mlogloss:0.479846\n",
            "[459]\tTrain-mlogloss:0.310362\tTest-mlogloss:0.479837\n",
            "[460]\tTrain-mlogloss:0.31003\tTest-mlogloss:0.479751\n",
            "[461]\tTrain-mlogloss:0.309723\tTest-mlogloss:0.4796\n",
            "[462]\tTrain-mlogloss:0.309482\tTest-mlogloss:0.47958\n",
            "[463]\tTrain-mlogloss:0.309359\tTest-mlogloss:0.479564\n",
            "[464]\tTrain-mlogloss:0.309123\tTest-mlogloss:0.479491\n",
            "[465]\tTrain-mlogloss:0.30887\tTest-mlogloss:0.479464\n",
            "[466]\tTrain-mlogloss:0.308567\tTest-mlogloss:0.479363\n",
            "[467]\tTrain-mlogloss:0.308281\tTest-mlogloss:0.479241\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[468]\tTrain-mlogloss:0.307941\tTest-mlogloss:0.479126\n",
            "[469]\tTrain-mlogloss:0.307654\tTest-mlogloss:0.479027\n",
            "[470]\tTrain-mlogloss:0.307348\tTest-mlogloss:0.478912\n",
            "[471]\tTrain-mlogloss:0.306996\tTest-mlogloss:0.478816\n",
            "[472]\tTrain-mlogloss:0.306749\tTest-mlogloss:0.478836\n",
            "[473]\tTrain-mlogloss:0.306362\tTest-mlogloss:0.478743\n",
            "[474]\tTrain-mlogloss:0.306\tTest-mlogloss:0.478577\n",
            "[475]\tTrain-mlogloss:0.305733\tTest-mlogloss:0.478507\n",
            "[476]\tTrain-mlogloss:0.305325\tTest-mlogloss:0.478405\n",
            "[477]\tTrain-mlogloss:0.305106\tTest-mlogloss:0.478426\n",
            "[478]\tTrain-mlogloss:0.304836\tTest-mlogloss:0.478383\n",
            "[479]\tTrain-mlogloss:0.304543\tTest-mlogloss:0.478333\n",
            "[480]\tTrain-mlogloss:0.304219\tTest-mlogloss:0.478272\n",
            "[481]\tTrain-mlogloss:0.303807\tTest-mlogloss:0.47818\n",
            "[482]\tTrain-mlogloss:0.303503\tTest-mlogloss:0.478161\n",
            "[483]\tTrain-mlogloss:0.303237\tTest-mlogloss:0.478043\n",
            "[484]\tTrain-mlogloss:0.302789\tTest-mlogloss:0.477906\n",
            "[485]\tTrain-mlogloss:0.30246\tTest-mlogloss:0.477874\n",
            "[486]\tTrain-mlogloss:0.302064\tTest-mlogloss:0.477713\n",
            "[487]\tTrain-mlogloss:0.301799\tTest-mlogloss:0.477619\n",
            "[488]\tTrain-mlogloss:0.301485\tTest-mlogloss:0.477582\n",
            "[489]\tTrain-mlogloss:0.301185\tTest-mlogloss:0.477556\n",
            "[490]\tTrain-mlogloss:0.300953\tTest-mlogloss:0.477555\n",
            "[491]\tTrain-mlogloss:0.300563\tTest-mlogloss:0.477474\n",
            "[492]\tTrain-mlogloss:0.300262\tTest-mlogloss:0.477363\n",
            "[493]\tTrain-mlogloss:0.29998\tTest-mlogloss:0.477283\n",
            "[494]\tTrain-mlogloss:0.299705\tTest-mlogloss:0.477226\n",
            "[495]\tTrain-mlogloss:0.299374\tTest-mlogloss:0.477147\n",
            "[496]\tTrain-mlogloss:0.299055\tTest-mlogloss:0.47702\n",
            "[497]\tTrain-mlogloss:0.298683\tTest-mlogloss:0.47697\n",
            "[498]\tTrain-mlogloss:0.298431\tTest-mlogloss:0.476911\n",
            "[499]\tTrain-mlogloss:0.298065\tTest-mlogloss:0.476809\n",
            "[500]\tTrain-mlogloss:0.297847\tTest-mlogloss:0.476793\n",
            "[501]\tTrain-mlogloss:0.297617\tTest-mlogloss:0.476755\n",
            "[502]\tTrain-mlogloss:0.29724\tTest-mlogloss:0.476651\n",
            "[503]\tTrain-mlogloss:0.296962\tTest-mlogloss:0.476607\n",
            "[504]\tTrain-mlogloss:0.296531\tTest-mlogloss:0.476461\n",
            "[505]\tTrain-mlogloss:0.296282\tTest-mlogloss:0.476406\n",
            "[506]\tTrain-mlogloss:0.296002\tTest-mlogloss:0.476384\n",
            "[507]\tTrain-mlogloss:0.295734\tTest-mlogloss:0.476387\n",
            "[508]\tTrain-mlogloss:0.295452\tTest-mlogloss:0.476386\n",
            "[509]\tTrain-mlogloss:0.295247\tTest-mlogloss:0.476324\n",
            "[510]\tTrain-mlogloss:0.294877\tTest-mlogloss:0.476182\n",
            "[511]\tTrain-mlogloss:0.294641\tTest-mlogloss:0.476162\n",
            "[512]\tTrain-mlogloss:0.294342\tTest-mlogloss:0.476085\n",
            "[513]\tTrain-mlogloss:0.293918\tTest-mlogloss:0.476071\n",
            "[514]\tTrain-mlogloss:0.293629\tTest-mlogloss:0.476053\n",
            "[515]\tTrain-mlogloss:0.293277\tTest-mlogloss:0.476013\n",
            "[516]\tTrain-mlogloss:0.292996\tTest-mlogloss:0.476009\n",
            "[517]\tTrain-mlogloss:0.292674\tTest-mlogloss:0.475924\n",
            "[518]\tTrain-mlogloss:0.292426\tTest-mlogloss:0.475902\n",
            "[519]\tTrain-mlogloss:0.292162\tTest-mlogloss:0.475824\n",
            "[520]\tTrain-mlogloss:0.291865\tTest-mlogloss:0.475716\n",
            "[521]\tTrain-mlogloss:0.291646\tTest-mlogloss:0.475697\n",
            "[522]\tTrain-mlogloss:0.291275\tTest-mlogloss:0.475635\n",
            "[523]\tTrain-mlogloss:0.291052\tTest-mlogloss:0.475562\n",
            "[524]\tTrain-mlogloss:0.290723\tTest-mlogloss:0.475518\n",
            "[525]\tTrain-mlogloss:0.290407\tTest-mlogloss:0.475365\n",
            "[526]\tTrain-mlogloss:0.290121\tTest-mlogloss:0.475341\n",
            "[527]\tTrain-mlogloss:0.289887\tTest-mlogloss:0.475319\n",
            "[528]\tTrain-mlogloss:0.289711\tTest-mlogloss:0.475299\n",
            "[529]\tTrain-mlogloss:0.289426\tTest-mlogloss:0.475227\n",
            "[530]\tTrain-mlogloss:0.289211\tTest-mlogloss:0.475218\n",
            "[531]\tTrain-mlogloss:0.289012\tTest-mlogloss:0.475182\n",
            "[532]\tTrain-mlogloss:0.288737\tTest-mlogloss:0.475014\n",
            "[533]\tTrain-mlogloss:0.288433\tTest-mlogloss:0.474978\n",
            "[534]\tTrain-mlogloss:0.288062\tTest-mlogloss:0.474908\n",
            "[535]\tTrain-mlogloss:0.287761\tTest-mlogloss:0.474821\n",
            "[536]\tTrain-mlogloss:0.287478\tTest-mlogloss:0.474791\n",
            "[537]\tTrain-mlogloss:0.287223\tTest-mlogloss:0.474699\n",
            "[538]\tTrain-mlogloss:0.286936\tTest-mlogloss:0.47459\n",
            "[539]\tTrain-mlogloss:0.286691\tTest-mlogloss:0.47461\n",
            "[540]\tTrain-mlogloss:0.286488\tTest-mlogloss:0.474579\n",
            "[541]\tTrain-mlogloss:0.286232\tTest-mlogloss:0.474481\n",
            "[542]\tTrain-mlogloss:0.285894\tTest-mlogloss:0.474329\n",
            "[543]\tTrain-mlogloss:0.285611\tTest-mlogloss:0.474252\n",
            "[544]\tTrain-mlogloss:0.285329\tTest-mlogloss:0.474264\n",
            "[545]\tTrain-mlogloss:0.285103\tTest-mlogloss:0.474247\n",
            "[546]\tTrain-mlogloss:0.284791\tTest-mlogloss:0.474193\n",
            "[547]\tTrain-mlogloss:0.284522\tTest-mlogloss:0.47416\n",
            "[548]\tTrain-mlogloss:0.284265\tTest-mlogloss:0.474109\n",
            "[549]\tTrain-mlogloss:0.284011\tTest-mlogloss:0.474033\n",
            "[550]\tTrain-mlogloss:0.283779\tTest-mlogloss:0.474015\n",
            "[551]\tTrain-mlogloss:0.283546\tTest-mlogloss:0.47395\n",
            "[552]\tTrain-mlogloss:0.28316\tTest-mlogloss:0.473952\n",
            "[553]\tTrain-mlogloss:0.282954\tTest-mlogloss:0.474008\n",
            "[554]\tTrain-mlogloss:0.282711\tTest-mlogloss:0.474026\n",
            "[555]\tTrain-mlogloss:0.282483\tTest-mlogloss:0.473959\n",
            "[556]\tTrain-mlogloss:0.282264\tTest-mlogloss:0.473924\n",
            "[557]\tTrain-mlogloss:0.282024\tTest-mlogloss:0.473948\n",
            "[558]\tTrain-mlogloss:0.281769\tTest-mlogloss:0.473929\n",
            "[559]\tTrain-mlogloss:0.281555\tTest-mlogloss:0.473816\n",
            "[560]\tTrain-mlogloss:0.281257\tTest-mlogloss:0.473803\n",
            "[561]\tTrain-mlogloss:0.280996\tTest-mlogloss:0.473689\n",
            "[562]\tTrain-mlogloss:0.280722\tTest-mlogloss:0.473607\n",
            "[563]\tTrain-mlogloss:0.280471\tTest-mlogloss:0.473561\n",
            "[564]\tTrain-mlogloss:0.280238\tTest-mlogloss:0.473527\n",
            "[565]\tTrain-mlogloss:0.279954\tTest-mlogloss:0.473484\n",
            "[566]\tTrain-mlogloss:0.279571\tTest-mlogloss:0.473447\n",
            "[567]\tTrain-mlogloss:0.279302\tTest-mlogloss:0.47343\n",
            "[568]\tTrain-mlogloss:0.279111\tTest-mlogloss:0.473425\n",
            "[569]\tTrain-mlogloss:0.278871\tTest-mlogloss:0.473415\n",
            "[570]\tTrain-mlogloss:0.278588\tTest-mlogloss:0.473356\n",
            "[571]\tTrain-mlogloss:0.278344\tTest-mlogloss:0.473327\n",
            "[572]\tTrain-mlogloss:0.278079\tTest-mlogloss:0.473295\n",
            "[573]\tTrain-mlogloss:0.277837\tTest-mlogloss:0.473262\n",
            "[574]\tTrain-mlogloss:0.277526\tTest-mlogloss:0.473148\n",
            "[575]\tTrain-mlogloss:0.277195\tTest-mlogloss:0.473067\n",
            "[576]\tTrain-mlogloss:0.276936\tTest-mlogloss:0.472958\n",
            "[577]\tTrain-mlogloss:0.276777\tTest-mlogloss:0.47299\n",
            "[578]\tTrain-mlogloss:0.276509\tTest-mlogloss:0.472926\n",
            "[579]\tTrain-mlogloss:0.276209\tTest-mlogloss:0.472854\n",
            "[580]\tTrain-mlogloss:0.275979\tTest-mlogloss:0.472845\n",
            "[581]\tTrain-mlogloss:0.275779\tTest-mlogloss:0.472847\n",
            "[582]\tTrain-mlogloss:0.275494\tTest-mlogloss:0.472709\n",
            "[583]\tTrain-mlogloss:0.275255\tTest-mlogloss:0.472705\n",
            "[584]\tTrain-mlogloss:0.27502\tTest-mlogloss:0.472746\n",
            "[585]\tTrain-mlogloss:0.274795\tTest-mlogloss:0.472698\n",
            "[586]\tTrain-mlogloss:0.27454\tTest-mlogloss:0.472641\n",
            "[587]\tTrain-mlogloss:0.274269\tTest-mlogloss:0.472647\n",
            "[588]\tTrain-mlogloss:0.274021\tTest-mlogloss:0.472604\n",
            "[589]\tTrain-mlogloss:0.273736\tTest-mlogloss:0.472627\n",
            "[590]\tTrain-mlogloss:0.273457\tTest-mlogloss:0.472603\n",
            "[591]\tTrain-mlogloss:0.273163\tTest-mlogloss:0.472547\n",
            "[592]\tTrain-mlogloss:0.27293\tTest-mlogloss:0.472509\n",
            "[593]\tTrain-mlogloss:0.272712\tTest-mlogloss:0.472444\n",
            "[594]\tTrain-mlogloss:0.272512\tTest-mlogloss:0.472529\n",
            "[595]\tTrain-mlogloss:0.272305\tTest-mlogloss:0.472502\n",
            "[596]\tTrain-mlogloss:0.272116\tTest-mlogloss:0.472478\n",
            "[597]\tTrain-mlogloss:0.271921\tTest-mlogloss:0.472476\n",
            "[598]\tTrain-mlogloss:0.271643\tTest-mlogloss:0.472422\n",
            "[599]\tTrain-mlogloss:0.271347\tTest-mlogloss:0.472373\n",
            "[600]\tTrain-mlogloss:0.27109\tTest-mlogloss:0.472318\n",
            "[601]\tTrain-mlogloss:0.270853\tTest-mlogloss:0.47238\n",
            "[602]\tTrain-mlogloss:0.270578\tTest-mlogloss:0.472341\n",
            "[603]\tTrain-mlogloss:0.270337\tTest-mlogloss:0.472247\n",
            "[604]\tTrain-mlogloss:0.269982\tTest-mlogloss:0.472189\n",
            "[605]\tTrain-mlogloss:0.269773\tTest-mlogloss:0.472189\n",
            "[606]\tTrain-mlogloss:0.269466\tTest-mlogloss:0.472171\n",
            "[607]\tTrain-mlogloss:0.269302\tTest-mlogloss:0.472156\n",
            "[608]\tTrain-mlogloss:0.269062\tTest-mlogloss:0.472084\n",
            "[609]\tTrain-mlogloss:0.268721\tTest-mlogloss:0.472035\n",
            "[610]\tTrain-mlogloss:0.2685\tTest-mlogloss:0.471942\n",
            "[611]\tTrain-mlogloss:0.268219\tTest-mlogloss:0.471901\n",
            "[612]\tTrain-mlogloss:0.267982\tTest-mlogloss:0.471868\n",
            "[613]\tTrain-mlogloss:0.267764\tTest-mlogloss:0.471892\n",
            "[614]\tTrain-mlogloss:0.267574\tTest-mlogloss:0.471913\n",
            "[615]\tTrain-mlogloss:0.267405\tTest-mlogloss:0.471895\n",
            "[616]\tTrain-mlogloss:0.267181\tTest-mlogloss:0.471878\n",
            "[617]\tTrain-mlogloss:0.266938\tTest-mlogloss:0.47186\n",
            "[618]\tTrain-mlogloss:0.266665\tTest-mlogloss:0.471755\n",
            "[619]\tTrain-mlogloss:0.266419\tTest-mlogloss:0.471711\n",
            "[620]\tTrain-mlogloss:0.266046\tTest-mlogloss:0.471606\n",
            "[621]\tTrain-mlogloss:0.265855\tTest-mlogloss:0.471589\n",
            "[622]\tTrain-mlogloss:0.265664\tTest-mlogloss:0.471602\n",
            "[623]\tTrain-mlogloss:0.265412\tTest-mlogloss:0.471523\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[624]\tTrain-mlogloss:0.265207\tTest-mlogloss:0.471529\n",
            "[625]\tTrain-mlogloss:0.264936\tTest-mlogloss:0.471577\n",
            "[626]\tTrain-mlogloss:0.264696\tTest-mlogloss:0.471605\n",
            "[627]\tTrain-mlogloss:0.264336\tTest-mlogloss:0.471589\n",
            "[628]\tTrain-mlogloss:0.264178\tTest-mlogloss:0.471604\n",
            "[629]\tTrain-mlogloss:0.263917\tTest-mlogloss:0.47164\n",
            "[630]\tTrain-mlogloss:0.263681\tTest-mlogloss:0.471522\n",
            "[631]\tTrain-mlogloss:0.263468\tTest-mlogloss:0.471445\n",
            "[632]\tTrain-mlogloss:0.263259\tTest-mlogloss:0.471445\n",
            "[633]\tTrain-mlogloss:0.262982\tTest-mlogloss:0.471445\n",
            "[634]\tTrain-mlogloss:0.262733\tTest-mlogloss:0.471401\n",
            "[635]\tTrain-mlogloss:0.262502\tTest-mlogloss:0.47137\n",
            "[636]\tTrain-mlogloss:0.262257\tTest-mlogloss:0.471389\n",
            "[637]\tTrain-mlogloss:0.261994\tTest-mlogloss:0.471347\n",
            "[638]\tTrain-mlogloss:0.261803\tTest-mlogloss:0.47133\n",
            "[639]\tTrain-mlogloss:0.261591\tTest-mlogloss:0.471285\n",
            "[640]\tTrain-mlogloss:0.261323\tTest-mlogloss:0.471162\n",
            "[641]\tTrain-mlogloss:0.261145\tTest-mlogloss:0.471171\n",
            "[642]\tTrain-mlogloss:0.260907\tTest-mlogloss:0.471143\n",
            "[643]\tTrain-mlogloss:0.260674\tTest-mlogloss:0.471072\n",
            "[644]\tTrain-mlogloss:0.260352\tTest-mlogloss:0.471021\n",
            "[645]\tTrain-mlogloss:0.260169\tTest-mlogloss:0.471038\n",
            "[646]\tTrain-mlogloss:0.259951\tTest-mlogloss:0.471049\n",
            "[647]\tTrain-mlogloss:0.259778\tTest-mlogloss:0.471012\n",
            "[648]\tTrain-mlogloss:0.259617\tTest-mlogloss:0.470972\n",
            "[649]\tTrain-mlogloss:0.259328\tTest-mlogloss:0.470884\n",
            "[650]\tTrain-mlogloss:0.259131\tTest-mlogloss:0.470943\n",
            "[651]\tTrain-mlogloss:0.258925\tTest-mlogloss:0.47096\n",
            "[652]\tTrain-mlogloss:0.258734\tTest-mlogloss:0.470929\n",
            "[653]\tTrain-mlogloss:0.258442\tTest-mlogloss:0.470907\n",
            "[654]\tTrain-mlogloss:0.25819\tTest-mlogloss:0.470899\n",
            "[655]\tTrain-mlogloss:0.257949\tTest-mlogloss:0.47093\n",
            "[656]\tTrain-mlogloss:0.257711\tTest-mlogloss:0.470875\n",
            "[657]\tTrain-mlogloss:0.25748\tTest-mlogloss:0.470834\n",
            "[658]\tTrain-mlogloss:0.257195\tTest-mlogloss:0.470792\n",
            "[659]\tTrain-mlogloss:0.256976\tTest-mlogloss:0.470716\n",
            "[660]\tTrain-mlogloss:0.256708\tTest-mlogloss:0.470562\n",
            "[661]\tTrain-mlogloss:0.256445\tTest-mlogloss:0.4706\n",
            "[662]\tTrain-mlogloss:0.256252\tTest-mlogloss:0.470611\n",
            "[663]\tTrain-mlogloss:0.255992\tTest-mlogloss:0.470448\n",
            "[664]\tTrain-mlogloss:0.255723\tTest-mlogloss:0.470385\n",
            "[665]\tTrain-mlogloss:0.25555\tTest-mlogloss:0.470369\n",
            "[666]\tTrain-mlogloss:0.255303\tTest-mlogloss:0.47033\n",
            "[667]\tTrain-mlogloss:0.255117\tTest-mlogloss:0.470287\n",
            "[668]\tTrain-mlogloss:0.254836\tTest-mlogloss:0.470167\n",
            "[669]\tTrain-mlogloss:0.25456\tTest-mlogloss:0.470084\n",
            "[670]\tTrain-mlogloss:0.254275\tTest-mlogloss:0.469979\n",
            "[671]\tTrain-mlogloss:0.254052\tTest-mlogloss:0.469984\n",
            "[672]\tTrain-mlogloss:0.253763\tTest-mlogloss:0.469962\n",
            "[673]\tTrain-mlogloss:0.253547\tTest-mlogloss:0.469893\n",
            "[674]\tTrain-mlogloss:0.253318\tTest-mlogloss:0.469824\n",
            "[675]\tTrain-mlogloss:0.253145\tTest-mlogloss:0.469808\n",
            "[676]\tTrain-mlogloss:0.252921\tTest-mlogloss:0.469763\n",
            "[677]\tTrain-mlogloss:0.252727\tTest-mlogloss:0.469789\n",
            "[678]\tTrain-mlogloss:0.252527\tTest-mlogloss:0.469777\n",
            "[679]\tTrain-mlogloss:0.25229\tTest-mlogloss:0.469706\n",
            "[680]\tTrain-mlogloss:0.25209\tTest-mlogloss:0.469663\n",
            "[681]\tTrain-mlogloss:0.251873\tTest-mlogloss:0.46961\n",
            "[682]\tTrain-mlogloss:0.251645\tTest-mlogloss:0.469663\n",
            "[683]\tTrain-mlogloss:0.251444\tTest-mlogloss:0.469656\n",
            "[684]\tTrain-mlogloss:0.251309\tTest-mlogloss:0.469668\n",
            "[685]\tTrain-mlogloss:0.251144\tTest-mlogloss:0.469665\n",
            "[686]\tTrain-mlogloss:0.250964\tTest-mlogloss:0.469637\n",
            "[687]\tTrain-mlogloss:0.250754\tTest-mlogloss:0.469621\n",
            "[688]\tTrain-mlogloss:0.250564\tTest-mlogloss:0.46962\n",
            "[689]\tTrain-mlogloss:0.250425\tTest-mlogloss:0.469589\n",
            "[690]\tTrain-mlogloss:0.250262\tTest-mlogloss:0.469611\n",
            "[691]\tTrain-mlogloss:0.250037\tTest-mlogloss:0.469582\n",
            "[692]\tTrain-mlogloss:0.249824\tTest-mlogloss:0.469513\n",
            "[693]\tTrain-mlogloss:0.249586\tTest-mlogloss:0.469503\n",
            "[694]\tTrain-mlogloss:0.24932\tTest-mlogloss:0.469437\n",
            "[695]\tTrain-mlogloss:0.249106\tTest-mlogloss:0.469418\n",
            "[696]\tTrain-mlogloss:0.24897\tTest-mlogloss:0.469427\n",
            "[697]\tTrain-mlogloss:0.248829\tTest-mlogloss:0.469462\n",
            "[698]\tTrain-mlogloss:0.248629\tTest-mlogloss:0.469441\n",
            "[699]\tTrain-mlogloss:0.248453\tTest-mlogloss:0.469413\n",
            "[700]\tTrain-mlogloss:0.248322\tTest-mlogloss:0.469416\n",
            "[701]\tTrain-mlogloss:0.248117\tTest-mlogloss:0.469453\n",
            "[702]\tTrain-mlogloss:0.248016\tTest-mlogloss:0.469454\n",
            "[703]\tTrain-mlogloss:0.24778\tTest-mlogloss:0.469438\n",
            "[704]\tTrain-mlogloss:0.247554\tTest-mlogloss:0.469342\n",
            "[705]\tTrain-mlogloss:0.247409\tTest-mlogloss:0.469284\n",
            "[706]\tTrain-mlogloss:0.247179\tTest-mlogloss:0.469242\n",
            "[707]\tTrain-mlogloss:0.247015\tTest-mlogloss:0.469246\n",
            "[708]\tTrain-mlogloss:0.246791\tTest-mlogloss:0.469257\n",
            "[709]\tTrain-mlogloss:0.246647\tTest-mlogloss:0.469302\n",
            "[710]\tTrain-mlogloss:0.246373\tTest-mlogloss:0.469305\n",
            "[711]\tTrain-mlogloss:0.246105\tTest-mlogloss:0.469299\n",
            "[712]\tTrain-mlogloss:0.245856\tTest-mlogloss:0.469246\n",
            "[713]\tTrain-mlogloss:0.245653\tTest-mlogloss:0.469293\n",
            "[714]\tTrain-mlogloss:0.245479\tTest-mlogloss:0.469291\n",
            "[715]\tTrain-mlogloss:0.245251\tTest-mlogloss:0.469135\n",
            "[716]\tTrain-mlogloss:0.245026\tTest-mlogloss:0.469055\n",
            "[717]\tTrain-mlogloss:0.244771\tTest-mlogloss:0.469075\n",
            "[718]\tTrain-mlogloss:0.244635\tTest-mlogloss:0.469052\n",
            "[719]\tTrain-mlogloss:0.244519\tTest-mlogloss:0.469034\n",
            "[720]\tTrain-mlogloss:0.244317\tTest-mlogloss:0.469049\n",
            "[721]\tTrain-mlogloss:0.244024\tTest-mlogloss:0.469035\n",
            "[722]\tTrain-mlogloss:0.24379\tTest-mlogloss:0.469016\n",
            "[723]\tTrain-mlogloss:0.243585\tTest-mlogloss:0.469027\n",
            "[724]\tTrain-mlogloss:0.243391\tTest-mlogloss:0.468988\n",
            "[725]\tTrain-mlogloss:0.243227\tTest-mlogloss:0.469051\n",
            "[726]\tTrain-mlogloss:0.243023\tTest-mlogloss:0.468957\n",
            "[727]\tTrain-mlogloss:0.24284\tTest-mlogloss:0.468993\n",
            "[728]\tTrain-mlogloss:0.242628\tTest-mlogloss:0.469018\n",
            "[729]\tTrain-mlogloss:0.242452\tTest-mlogloss:0.468996\n",
            "[730]\tTrain-mlogloss:0.242232\tTest-mlogloss:0.469005\n",
            "[731]\tTrain-mlogloss:0.241994\tTest-mlogloss:0.468959\n",
            "[732]\tTrain-mlogloss:0.241775\tTest-mlogloss:0.468881\n",
            "[733]\tTrain-mlogloss:0.241526\tTest-mlogloss:0.46888\n",
            "[734]\tTrain-mlogloss:0.24134\tTest-mlogloss:0.468898\n",
            "[735]\tTrain-mlogloss:0.241162\tTest-mlogloss:0.468856\n",
            "[736]\tTrain-mlogloss:0.240987\tTest-mlogloss:0.46881\n",
            "[737]\tTrain-mlogloss:0.240733\tTest-mlogloss:0.468804\n",
            "[738]\tTrain-mlogloss:0.240551\tTest-mlogloss:0.468804\n",
            "[739]\tTrain-mlogloss:0.240352\tTest-mlogloss:0.468769\n",
            "[740]\tTrain-mlogloss:0.240111\tTest-mlogloss:0.468828\n",
            "[741]\tTrain-mlogloss:0.239908\tTest-mlogloss:0.468771\n",
            "[742]\tTrain-mlogloss:0.239741\tTest-mlogloss:0.468788\n",
            "[743]\tTrain-mlogloss:0.239569\tTest-mlogloss:0.468869\n",
            "[744]\tTrain-mlogloss:0.239256\tTest-mlogloss:0.468889\n",
            "[745]\tTrain-mlogloss:0.239034\tTest-mlogloss:0.468824\n",
            "[746]\tTrain-mlogloss:0.238883\tTest-mlogloss:0.468797\n",
            "[747]\tTrain-mlogloss:0.238638\tTest-mlogloss:0.468811\n",
            "[748]\tTrain-mlogloss:0.238379\tTest-mlogloss:0.468782\n",
            "[749]\tTrain-mlogloss:0.23818\tTest-mlogloss:0.46882\n",
            "[750]\tTrain-mlogloss:0.237964\tTest-mlogloss:0.468804\n",
            "[751]\tTrain-mlogloss:0.237694\tTest-mlogloss:0.468804\n",
            "[752]\tTrain-mlogloss:0.237589\tTest-mlogloss:0.468847\n",
            "[753]\tTrain-mlogloss:0.237374\tTest-mlogloss:0.468822\n",
            "[754]\tTrain-mlogloss:0.23722\tTest-mlogloss:0.468825\n",
            "[755]\tTrain-mlogloss:0.237073\tTest-mlogloss:0.468783\n",
            "[756]\tTrain-mlogloss:0.236847\tTest-mlogloss:0.468794\n",
            "[757]\tTrain-mlogloss:0.236586\tTest-mlogloss:0.468756\n",
            "[758]\tTrain-mlogloss:0.236313\tTest-mlogloss:0.468678\n",
            "[759]\tTrain-mlogloss:0.23607\tTest-mlogloss:0.468721\n",
            "[760]\tTrain-mlogloss:0.235851\tTest-mlogloss:0.468723\n",
            "[761]\tTrain-mlogloss:0.235642\tTest-mlogloss:0.468709\n",
            "[762]\tTrain-mlogloss:0.235446\tTest-mlogloss:0.468733\n",
            "[763]\tTrain-mlogloss:0.235215\tTest-mlogloss:0.468674\n",
            "[764]\tTrain-mlogloss:0.235059\tTest-mlogloss:0.468665\n",
            "[765]\tTrain-mlogloss:0.234948\tTest-mlogloss:0.46867\n",
            "[766]\tTrain-mlogloss:0.234791\tTest-mlogloss:0.468712\n",
            "[767]\tTrain-mlogloss:0.234631\tTest-mlogloss:0.468735\n",
            "[768]\tTrain-mlogloss:0.234394\tTest-mlogloss:0.468672\n",
            "[769]\tTrain-mlogloss:0.234179\tTest-mlogloss:0.46864\n",
            "[770]\tTrain-mlogloss:0.233976\tTest-mlogloss:0.468695\n",
            "[771]\tTrain-mlogloss:0.233835\tTest-mlogloss:0.468771\n",
            "[772]\tTrain-mlogloss:0.233618\tTest-mlogloss:0.468724\n",
            "[773]\tTrain-mlogloss:0.233395\tTest-mlogloss:0.46877\n",
            "[774]\tTrain-mlogloss:0.233286\tTest-mlogloss:0.468807\n",
            "[775]\tTrain-mlogloss:0.233124\tTest-mlogloss:0.468853\n",
            "[776]\tTrain-mlogloss:0.232914\tTest-mlogloss:0.468872\n",
            "[777]\tTrain-mlogloss:0.232697\tTest-mlogloss:0.468837\n",
            "[778]\tTrain-mlogloss:0.23255\tTest-mlogloss:0.468809\n",
            "[779]\tTrain-mlogloss:0.232368\tTest-mlogloss:0.468819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[780]\tTrain-mlogloss:0.23223\tTest-mlogloss:0.468753\n",
            "[781]\tTrain-mlogloss:0.232037\tTest-mlogloss:0.468723\n",
            "[782]\tTrain-mlogloss:0.231793\tTest-mlogloss:0.468556\n",
            "[783]\tTrain-mlogloss:0.231661\tTest-mlogloss:0.468581\n",
            "[784]\tTrain-mlogloss:0.231504\tTest-mlogloss:0.468542\n",
            "[785]\tTrain-mlogloss:0.231348\tTest-mlogloss:0.46856\n",
            "[786]\tTrain-mlogloss:0.231168\tTest-mlogloss:0.468621\n",
            "[787]\tTrain-mlogloss:0.231\tTest-mlogloss:0.46857\n",
            "[788]\tTrain-mlogloss:0.2308\tTest-mlogloss:0.468538\n",
            "[789]\tTrain-mlogloss:0.230583\tTest-mlogloss:0.468521\n",
            "[790]\tTrain-mlogloss:0.230373\tTest-mlogloss:0.468436\n",
            "[791]\tTrain-mlogloss:0.230175\tTest-mlogloss:0.468418\n",
            "[792]\tTrain-mlogloss:0.230024\tTest-mlogloss:0.468417\n",
            "[793]\tTrain-mlogloss:0.229809\tTest-mlogloss:0.468455\n",
            "[794]\tTrain-mlogloss:0.229608\tTest-mlogloss:0.468417\n",
            "[795]\tTrain-mlogloss:0.229412\tTest-mlogloss:0.468406\n",
            "[796]\tTrain-mlogloss:0.229226\tTest-mlogloss:0.468417\n",
            "[797]\tTrain-mlogloss:0.22911\tTest-mlogloss:0.468393\n",
            "[798]\tTrain-mlogloss:0.228894\tTest-mlogloss:0.468344\n",
            "[799]\tTrain-mlogloss:0.228741\tTest-mlogloss:0.468354\n",
            "[800]\tTrain-mlogloss:0.228555\tTest-mlogloss:0.468264\n",
            "[801]\tTrain-mlogloss:0.228371\tTest-mlogloss:0.468288\n",
            "[802]\tTrain-mlogloss:0.228213\tTest-mlogloss:0.468304\n",
            "[803]\tTrain-mlogloss:0.228044\tTest-mlogloss:0.468286\n",
            "[804]\tTrain-mlogloss:0.227801\tTest-mlogloss:0.468231\n",
            "[805]\tTrain-mlogloss:0.227605\tTest-mlogloss:0.468167\n",
            "[806]\tTrain-mlogloss:0.227457\tTest-mlogloss:0.468131\n",
            "[807]\tTrain-mlogloss:0.22724\tTest-mlogloss:0.468134\n",
            "[808]\tTrain-mlogloss:0.227089\tTest-mlogloss:0.468121\n",
            "[809]\tTrain-mlogloss:0.226979\tTest-mlogloss:0.468136\n",
            "[810]\tTrain-mlogloss:0.226835\tTest-mlogloss:0.468101\n",
            "[811]\tTrain-mlogloss:0.226636\tTest-mlogloss:0.468082\n",
            "[812]\tTrain-mlogloss:0.226432\tTest-mlogloss:0.468069\n",
            "[813]\tTrain-mlogloss:0.226271\tTest-mlogloss:0.468039\n",
            "[814]\tTrain-mlogloss:0.226109\tTest-mlogloss:0.468047\n",
            "[815]\tTrain-mlogloss:0.225808\tTest-mlogloss:0.468065\n",
            "[816]\tTrain-mlogloss:0.225631\tTest-mlogloss:0.468097\n",
            "[817]\tTrain-mlogloss:0.225384\tTest-mlogloss:0.468054\n",
            "[818]\tTrain-mlogloss:0.225164\tTest-mlogloss:0.467979\n",
            "[819]\tTrain-mlogloss:0.22497\tTest-mlogloss:0.467937\n",
            "[820]\tTrain-mlogloss:0.224776\tTest-mlogloss:0.468002\n",
            "[821]\tTrain-mlogloss:0.224605\tTest-mlogloss:0.468009\n",
            "[822]\tTrain-mlogloss:0.224437\tTest-mlogloss:0.468084\n",
            "[823]\tTrain-mlogloss:0.224285\tTest-mlogloss:0.468087\n",
            "[824]\tTrain-mlogloss:0.224055\tTest-mlogloss:0.468054\n",
            "[825]\tTrain-mlogloss:0.223851\tTest-mlogloss:0.468125\n",
            "[826]\tTrain-mlogloss:0.223608\tTest-mlogloss:0.468157\n",
            "[827]\tTrain-mlogloss:0.223439\tTest-mlogloss:0.468131\n",
            "[828]\tTrain-mlogloss:0.22321\tTest-mlogloss:0.468174\n",
            "[829]\tTrain-mlogloss:0.222954\tTest-mlogloss:0.468129\n",
            "[830]\tTrain-mlogloss:0.22271\tTest-mlogloss:0.468135\n",
            "[831]\tTrain-mlogloss:0.222563\tTest-mlogloss:0.468121\n",
            "[832]\tTrain-mlogloss:0.222405\tTest-mlogloss:0.468036\n",
            "[833]\tTrain-mlogloss:0.222205\tTest-mlogloss:0.468037\n",
            "[834]\tTrain-mlogloss:0.22209\tTest-mlogloss:0.468062\n",
            "[835]\tTrain-mlogloss:0.221901\tTest-mlogloss:0.468098\n",
            "[836]\tTrain-mlogloss:0.221666\tTest-mlogloss:0.468029\n",
            "[837]\tTrain-mlogloss:0.221477\tTest-mlogloss:0.46808\n",
            "[838]\tTrain-mlogloss:0.221293\tTest-mlogloss:0.468064\n",
            "[839]\tTrain-mlogloss:0.221064\tTest-mlogloss:0.468035\n",
            "Stopping. Best iteration:\n",
            "[819]\tTrain-mlogloss:0.22497\tTest-mlogloss:0.467937\n",
            "\n",
            "Training Class Probabilities for First 5 Instances:\n",
            " [[4.9844184e-05 6.9372177e-02 8.5674280e-01 8.6777434e-03 1.0010292e-06\n",
            "  2.8729174e-04 6.4836718e-02 2.3504343e-05 8.8882243e-06]\n",
            " [3.1389707e-05 8.8631797e-01 1.0836417e-01 4.9758391e-03 3.6361993e-07\n",
            "  4.7017700e-05 2.1747377e-04 3.7665020e-05 8.1120725e-06]\n",
            " [4.9069218e-05 7.3243850e-01 2.6268178e-01 1.8789043e-03 4.1920401e-04\n",
            "  1.4515994e-04 1.2019466e-04 2.0674316e-03 1.9975394e-04]\n",
            " [6.7047938e-03 5.4456745e-03 4.0798704e-03 3.5606921e-03 1.9896209e-04\n",
            "  9.3619955e-01 4.2241827e-02 9.5457648e-04 6.1407266e-04]\n",
            " [1.0348459e-03 7.4048882e-07 8.2638854e-07 3.1749260e-05 1.8552701e-08\n",
            "  9.9859911e-01 6.6823115e-05 1.7528633e-04 9.0634836e-05]]\n",
            "Validation Class Probabilities for First 5 Instances:\n",
            " [[7.2848506e-04 8.9173287e-01 9.3639968e-03 8.6400948e-02 1.1589239e-05\n",
            "  3.2631191e-03 9.0438809e-04 1.2422038e-03 6.3523571e-03]\n",
            " [1.0593581e-06 2.7923760e-04 1.8005558e-05 6.5093848e-04 9.9903476e-01\n",
            "  2.5785930e-06 3.8228964e-06 7.2202788e-06 2.3744601e-06]\n",
            " [6.8782327e-05 3.9982647e-01 2.3272060e-01 3.6253268e-01 1.2162398e-06\n",
            "  4.3401698e-04 3.3715577e-03 5.3852604e-04 5.0618843e-04]\n",
            " [9.6280667e-07 9.8677617e-01 9.1692135e-03 4.0273303e-03 1.0159204e-06\n",
            "  1.0095715e-05 1.3714976e-05 8.2723409e-07 6.3683956e-07]\n",
            " [3.0424712e-02 5.7978858e-03 2.4987208e-03 3.3988126e-04 2.0563092e-04\n",
            "  6.6049960e-03 2.9065943e-04 3.9346125e-03 9.4990295e-01]]\n",
            "Best Predictions for Train:\n",
            " [3 2 2 ... 2 6 6]\n",
            "Best Predictions for Validation:\n",
            " [2 5 2 ... 2 2 2]\n",
            "Precision Score of the Training Set=  0.947052452046258\n",
            "Precision Score of the Validation Set=  0.8098757825236489\n",
            "Recall Score of the Training Set=  0.9184636160533136\n",
            "Recall Score of the Validation Set=  0.7686112919660354\n",
            "F1 Score of the Training Set=  0.9311314351938879\n",
            "F1 Score of the Validation Set=  0.7852967471088035\n",
            "Accuracy Score the Training Set=  0.9329320027473638\n",
            "Accuracy Score of the Validation Set=  0.8209437621202327\n",
            "Logloss Score Training Set=  0.22106428668160558\n",
            "Logloss Score of the Validation Set=  0.46803515527699024\n",
            "Confusion Matrix of the Training Set: \n",
            "\n",
            "[[ 1414    13     2     0     0    18    13    38    45]\n",
            " [    3 12230   577    39     3     5    26     8     7]\n",
            " [    2  1409  4924    28     0     3    30     6     1]\n",
            " [    2   323   127  1672     2    16     9     2     0]\n",
            " [    1     1     0     0  2188     0     1     0     0]\n",
            " [   21    24     5     3     0 11167    22    48    18]\n",
            " [   10    98    44     7     2    20  2074    13     3]\n",
            " [   12    28    12     0     0    29    20  6651    19]\n",
            " [   15    31     2     1     0    12     8    33  3862]]\n",
            "Confusion Matrix of the Validation Set: \n",
            "\n",
            "[[ 214   10    1    3    1   28   10   56   63]\n",
            " [   3 2733  390   42    5   11   24    4   12]\n",
            " [   0  663  857   26    0    6   40    5    4]\n",
            " [   0  146   69  301    1   10   10    0    1]\n",
            " [   1    6    0    0  539    1    1    0    0]\n",
            " [  15   15    5    5    0 2685   28   41   33]\n",
            " [  10   50   40    9    3   33  387   30    6]\n",
            " [  24   16    3    0    0   38   11 1578   23]\n",
            " [  37   10    0    1    2   22    5   48  866]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siYmGrQaybHv",
        "colab_type": "code",
        "colab": {},
        "outputId": "1d816e8d-d070-4c36-931f-c5fa30a0f240"
      },
      "source": [
        "dtest = xgb.DMatrix(test_trans.values,feature_names=dtrain.feature_names)#, label=y_train)\n",
        "preds = model.predict(dtest)\n",
        "#print(preds)\n",
        "#print(\"3\",Xtest_trans)\n",
        "print(\"Class Probabilities for First 5 Instances:\\n\",preds[:5])\n",
        "best_preds = np.asarray([np.argmax(line) for line in preds])\n",
        "print(\"Best Predictions:\\n\", best_preds+1) #add 1 to to transform back to 1-9 range"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class Probabilities for First 5 Instances:\n",
            " [[1.35319224e-05 4.82110530e-02 1.51201308e-01 7.99503267e-01\n",
            "  2.56055068e-06 1.66108039e-05 9.75354109e-04 7.38393501e-05\n",
            "  2.45435945e-06]\n",
            " [2.73837388e-04 3.68902870e-02 2.06024828e-03 2.16818109e-04\n",
            "  2.30928799e-05 6.48815095e-01 1.39847654e-03 3.09689105e-01\n",
            "  6.33068790e-04]\n",
            " [3.81909285e-06 2.46431046e-06 6.79720733e-06 3.45100460e-07\n",
            "  4.42192588e-10 9.99786913e-01 1.74428601e-06 1.89917715e-04\n",
            "  7.96869517e-06]\n",
            " [3.40573024e-04 8.45560431e-01 1.52021080e-01 1.34100544e-03\n",
            "  1.03178104e-06 1.57047580e-05 3.81460959e-05 3.11309996e-05\n",
            "  6.50859380e-04]\n",
            " [1.23687096e-01 6.70902009e-05 1.39287498e-04 2.88706051e-05\n",
            "  8.61013723e-06 2.77960650e-03 6.34676835e-04 1.08688109e-01\n",
            "  7.63966620e-01]]\n",
            "Best Predictions:\n",
            " [4 6 6 ... 2 4 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnduDvgoybH0",
        "colab_type": "code",
        "colab": {},
        "outputId": "71773e1d-5183-4bc8-9593-f177f32bd6e9"
      },
      "source": [
        "# Class counts\n",
        "best_preds = pd.DataFrame(best_preds, columns =[\"class\"])\n",
        "best_preds[\"class\"].value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    42206\n",
              "5    33128\n",
              "7    20445\n",
              "2    16264\n",
              "8    11563\n",
              "6     6418\n",
              "4     6366\n",
              "3     4424\n",
              "0     3554\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1n4TnP1ybH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Write output\n",
        "import csv\n",
        "headers = \"c1,c2,c3,c4,c5,c6,c7,c8,c9\"\n",
        "#with open('OUT.csv', 'wt', newline ='') as file:\n",
        "#    writer = csv.writer(file, delimiter=',')\n",
        "#    writer.writerow(i for i in header)\n",
        "np.savetxt(\"OUT.csv\", preds, header=headers , delimiter=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHtouQ4CybIB",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "+ [xgboost/demo/kaggle-higgs/higgs-cv.py](https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-cv.py)\n",
        "+ [Hyperparameter tuning in XGBoost](https://cambridgespark.com/content/tutorials/hyperparameter-tuning-in-xgboost/index.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbZ6W8fcybIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "km_Z8AmJybIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}